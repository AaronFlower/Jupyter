{
 "cells": [
  {
   "attachments": {
    "regress-algo.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAK4AAAEgCAYAAAAtwUjyAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAADsMAAA7DAcdvqGQAAA7LSURBVHhe7Z0JcqswEAVzLh/I58lpfJl/mHy0DGiHxEhoTHcVVbGNMUszehLY+foBUAjigkoQF1SCuAmv59fP15eZHj/f//yTMB2JuK+fpz1o5en58rN9JP9+vh9mOxFWA4WK6+V9fC+HckMq0afK++/7cYOT83PIxf33/fNYDuAjLTv++S/NR/b1XOR8Lqdmjjsxy6/BfOTi2oNbqDzqxS23JALi6iIT1zWZ+QHciwpbp8ZNYcWWZjjMj+H8f1nmRprLl3U3J98qqGTXfMrXkXyrhURcf5Ar+bYojlTi4D1ufie/+du+z8/3fLnPcLKKVMmJsrPMFd86hK2AnCTZutZaEmHvdZiKRNzKqEKleV3nL4pekTGpam7e8Lmjy6w0/RUBay3Jil+/clWH2YjFzQ66l6NShkrNqxOsJk7+fCru0WXWlld6//JssSUJyU8MmJlI3Lo0qQiGSnasyGGXk72WRoWjy6xEjIXy57gTsFxNK5UbpiYQt1yVapVtPeD5CwX8svPy6OLD+vzRZVZky5bnOZBfy5UaZiUQtyKNP+h5tapJZiRNKmElP+bV/OgyS+JKFT4aHxLIuKpYxZXKmktTqW4LeS5082YH38sfLrtWyY8tUySV+czjRczvoLIaEf06p59lH+cfXFwfmBMnroglU3T0tkqWV63wtdLrDsmd31bK9rxHl2nFzObxJ5mZohMteH6ZSlU1r/4wM1HnrA9exMlLWV7pYWYGiFuJD5NRiy4wJ/3FVZMdJU5QdTXQXdy106dC3laeh5kYEBUAzgdxQSWICypBXFAJ4oJKEBdUgrjQYP9SeU78ntI9LmeAuFDGXjgKLsb4C0lNeZM77MwYfq8rpogLBcyFmPQCjL84U62ge6+fC+LCYZo3IsndeoMujyIuHKYl7uiblBAXKiSdrMXIorjRfdHB1NlgxIUc+WJBIN96s1Qlw46+nxlxIcFX2lTQZob17xmVExYQFyKqWTUZ6opovdYJxIUAuR85b/Kd0OV7lFuv9QJxIaASE0ToSfKtAXEhoCJo8+tXNdn7grgQkTX7excWLsi3BsSFDNf0b1NzsOCiL8MiLryFk3z8F0sRF97gmnxrQFz4M1cMgwmIC3/AV9oLsq2AuKASneIeuRsfPpou4m7DKeHVFLmc+H4mkuvpPTsFvbfhfLbm207Shl80ztqb08U1B9zus2iHmQN+4sGOKq7IdHzay2VDtuFMsosEbp+Yh+4EHHs5dgQdo4IXatl7qwjq0LAN5SEp0yo9nk8rtM5936ZrxnVnuzvwWumxDWvUOTi1mnlZVrp662c01jv8jNHTu3QVdznqy0q2m9foIF4wkL3L1NsgMSmPAp8aEYSO4h4Y67NSyM718x+pbJ0z7kbHbTiFckxYzqQk834e3cR9PU2V2jJijnstbAZd5dqvEmuF61zdem7DOZTE3U7kVsTQThdxzcGTnRY2WeHzstOjnXugWbZEFbcP3bfhFERSOVHc4+crXC/z96gTaRyniuuqTXIgpdnKRHM7NypkQw96GXXbEKxb+Llrp/JX0rrtce9Lt3Uu+nbOmoRVwWMPuqbq8Anb4EnX2z6eV94Lxc2zo612M44sVPmEbTCY7UhbCb9tk27LheL6g7ye5W5Hzdw8lfiEbagx85DapeIatiwWVy5NfMI2lEBcUEDcMTMnYE3c7UQNX/PRYlDHFHHBmLjKKsjoSppxjbR2Nj+a4WJRKSP3BXFvj6+0aSds9+qbr7C+Mldn6wTi3hyprJl4UUUts0aG0dYuIO6tkVya51gn9E7zbyNGe541cpjpxKE1xL01lZggQjdF2zpz1YIbXdTw859UnRH31lQE9Z21lmNX34CEuDcniwS7nTL3HhEyHDILn5cKG2XkA9HiKIgLXr5tqjkreTWSMbjJJ3reixstC3FhfmoVl6gAU5PnX1uxTxpZQFzohosWUmHzzto7IC50JcrPjQ7fb0FcUAnigkoQF1SCuKASxAWVIC6oBHFBJYgLKkFcUAnigkoQF1SCuKASxAWVIC6oBHFBJYgLKkFcUAnigkoQF1SCuKCSLuLmPxzhv6oc/UCE/PZU/Xv2sy3HcOay4O8g7i+WY0DcOSAqgEoQF1SCuKASxAWVIC6oBHFBJYgLKkFcUAnigkoQF1SCuKASxAWVIC6oBHFBJYgLKkFcUAnigkoQF1SCuKASxAWVIC6oBHFvjXwb2U3bN5dbxO8567+h/xbEvSuvZ/z1eft4R95/3z+PYB7zVf1jsp8P4t4S81sQj5/YOf/7ENUKuvf6WBAXVty/6K/8iImvtmf+6/53QFxYaYkrv+AzibeIe1+STtZiZFFcqbTpdLHBiHtHfEcslE8qai3DNmPEBSDu7fCVNhW0mWH9e2bJCQuIezOqWTUZ6opovXYRiHsr5CdR8ybfCZ0OkTlar10F4t6KSkwQoZXkWwPi3oqKoL6zVo6wNdmvBXFvRtbs711YmDDfGhD3hrimf5uagwXNanwdiAtNnORzdcwMiAsN5sy3BsSFKjMOgwmICwV8pZ0w2wqICyrRI+6RO/ThNnQRdxtuCa+2yOXGv2Umucbes6PQY737sDXldpL2fNIx1x6cLq45+HY/RjvRHPw3D3xUcUWm49NeVuu23meTXTBw+8I8dCfeXJdme9ExKni5lj26SqGCmde7PDxlWqPH82mF1rOf36NrxnUVwEmgibPWe403B6e9Jl6Wl67W+jmV9Q0/Y8Q0gr6fYpv3dlNrd/psA9xTrrfEozwK3CkiCB3FPXkssHPG3Zh1DLMcE5YzKMm896CbuK+nqVhbXsz5nSBrc9i5yp293udREnc7gfdixqfRRVwjmezIsBkLn7fYanGw1z5gHLfLep9GGhXc4+fLCe3Wz/x9j8hwqrhSFfODXKkKRsYJ8q2a9Q7WKczga2fysLRbq1HcPgX07ZztYIVRmM20rrfFtlyB4ANash5cKK5r6vSd7VrX22DWPY04PoJM0PL9hgvFdc2VvsKldb3raBxOu05c20SN7uCcgNb1boC4v8DmRGXNk0Hrem/EHTOT1Wviljt9PlpcfPJeJK7befpyotb19viOWNixlBGV9GQ00trZ/EiG2+ZSRr6GweLK2RrvvPnRut4hvtKmrYUMsVW3y2/78voq8wRc2DmDkUhlzcSLKmqZNTJMdNIi7i2QFiPPsU7oneb/QId0dPZH3FtQiQkidFO4rTM3UcFF3HtQEdR31lpCznrTEeLehCwS7HbK3Hsk+4ZDZuHzFrussaMNiHsjtnHZdoV0kicdNhE9fd5gKvfAfGtAXHgbK/rInLCAuPAmLv+2htN6gLjwJq5zNrjgIi68yUU3HSEuvIXNt4M7ZgbEhTdwMWF0vjUgLvwBuYS8TKPDrQdxQSWICypBXFAJ4oJKEBdUgrigEsQFlSAuqARxQSWICypBXFAJ4oJKEBdUgrigEsQFlSAuqARxQSWICypBXFAJ4oJKEBdUgrigEsQFlSAuqARxQSWICypBXFAJ4oJKEBdUgrigEsQFlSAuqKSLuPn/ydp+CHj7HWD5j4T5/5cVZluO4cxlwd9B3F8sx4C4c0BUAJUgLqgEcUEliAsqQVxQCeKCShAXVIK4oBLEBZUgLqgEceEgchnbTdsl7xbxe74e3z9H3nUExIV9Xs/4vgv7eEfef98/j2Aec4/HMdmPgbiwg7mJ6PETO+dvLKpW0L3X3wdx4U+8nqb5r9z95qvt13a73OkgLvyJlrhy62dHbxEXjpB0shYji+JKpU2nDgYjLrTxHbFQPqmotQzbjBEngbjQwFfaVNBmhvXv6ZkTFhAXqlSzajLUFdF67UQQFyrId+nyJt8JnQ6ROVqvnQniQoVKTBChL8y3BsSFChVBfWetHGFrsp8P4kKVrNnfu7AwKN8aEBeauKZ/m5qDBc1qfC6IC6fhJO/fMTMgLpzEuHxrQFw4hVHDYALiwpv4Sjso2wqICyrRK+6Ru/DhY+ki7jaEEl5BkUuI5+QguY7eszMwYjv6sDXfdpI2fOA4a29OF9ccbLufop1kDvbJBzqquCLT8Wkvjw3bjrPJLhK4fWMeuhOx/+XYEXSMCl6mZY+tEqhE03aUh6RM6/R4Pq3Qeo9DTNeM685wd9A102s71rhzcNpr4mV56Wqun9NY//Bzrph+S1dxlyO+rNTkTesRVGyHxKU8CnxSRBA6itt5fK9zxt24Zpzy95RjQp55P4Nu4r6epkJt+TBnk82+LDv4oCAjRhUMvbfjPEribuu2FzO00UVc2xnwOypspuLnXdNrXrcdB7/D7fxHjviAcdwh23EaIqlEAvf4+XJCu/U1f39GZDhVXKmCkUxBBcol8zt7rRLucT7fWNRuR7COYSZ3J52ZfiPtFpHK23wtfTtnu7idsxUm83jb4Xr4lO3w2NYskHxA6/ZbrhW3uIMUNmWfsh0W01qkJ13aolzPpeLaJjnYGdvjpWKN7dm8xadsR4sw48/ApeJmHRjfJGmrVp+yHS0QFxTgMrt0zMxJWRLXPZc+72NF54yPuBAjrUXQgsgoSxiHjLR2Fj+S4TpupXzcB8SFAF9pA0EtMswWxqEVX2F9VS7O0gHEhRWprJl8UVXNWSPDKGsXEBc8kk3zDpgTuhEBbLwYO26NuOCpxAQROnte2DpyAwsu4oJQEdR31mpSXnUTEuLCShYJmp0yN7/k3nC4LH7eLc+8fuZNSIgLEWtHq1EVpRMXddaCSpp34tJq7h7XOntHQFwYgMvB20lgHr/XmUNc6I/NycFoRfr4DyAudMdGi6DTtz1eKu8fcy7iQneyjphcVn6j6iIuqARxQSWICypBXFAJ4oJKEBdUgrigEsQFlThx1wHh9258ABhFXHFF4DduNwMYQRYV3C1rY7+GAfBb8ozrqy6RAWYmF3fnG50AM1CtuMRcmBkyLqgkEzf80hvArNA5A5Xk4lpqPw4BMAd0zkAlZFxQSSYuowqgAaICqCQXlytnoIBKVKDiwtzE4soPlzEMBpPjxPW51k6UWlBAnnEBFIC4oBLEBZUgLqgEcUEliAsqQVxQyM/Pf2W9H2H24gBdAAAAAElFTkSuQmCC"
    }
   },
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 回归的相关数学知识点\n",
    "\n",
    "## 1. 梯度(Gradient)\n",
    "\n",
    "### 1. 1 定义\n",
    "\n",
    "梯度是连续函数中的一个概念。以二元函数为例，假设 $f(x, y)$ 在平面区域 $D$ 内有一阶连续偏导数，则对于 $ \\forall  P(x_0, y_0) \\in D $, 都可以给出一个向量：\n",
    "\n",
    "\\begin{equation*}\n",
    "f_x(x_0, y_0) \\mathbf{i} + f_y(x_0, y_0) \\mathbf{j}\n",
    "\\end{equation*}\n",
    "\n",
    "则这个向量称为函数在 $ P(x_0, y_0)$ 的**梯度**，记作 $\\mathbf{grad} f(x_0, y_0)$ 或 $\\triangledown f(x_0, y_0)$, 即：\n",
    "\n",
    "\\begin{equation*}\n",
    "\\mathbf{grad} f(x_0, y_0) = \\triangledown f(x_0, y_0) = f_x(x_0, y_0) \\mathbf{i} + f_y(x_0, y_0) \\mathbf{j}\n",
    "\\end{equation*}\n",
    "\n",
    "### 1. 2 梯度与方向导数\n",
    "\n",
    "方向导数有一个定理是： 如果函数 $f(x, y)$ 在点 $P(x_0, y_0)$ 可微分，那么函数在该点沿任一方向 $l$ 的方向导数存在，且\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial{f}}{\\partial{l}} \\Biggr\\rvert _{(x_0, y_0)} = f_x(x_0, y_0) \\cos{\\alpha} + f_y(x_0, y_0) \\cos{\\beta}\n",
    "\\end{equation*}\n",
    "\n",
    "其中，$\\cos{\\alpha}, \\cos{\\beta}$ 是方向 $l$ 的方向余弦。\n",
    "\n",
    "如果函数 $f(x, y)$ 在点 $P(x_0, y_0)$ 可微分, $\\mathbf{e}_{l} = (\\cos{\\alpha}, \\cos{\\beta})$ 是与方向 $l$ 同向的单位向量，\n",
    "则有**方向导数与梯度的关系**为：\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial{f}}{\\partial{l}} \\Biggr\\rvert _{(x_0, y_0)} &= f_x(x_0, y_0) \\cos{\\alpha} + f_y(x_0, y_0) \\cos{\\beta}\n",
    "\\\\ &= \\mathbf{grad}f(x_0, y_0) \\cdot \\mathbf{e}_{l}\n",
    "\\\\ &= \\bigr\\rvert \\mathbf{grad}f(x_0, y_0) \\bigr\\rvert \\cdot \\cos{\\theta}\n",
    "\\end{align}\n",
    "\n",
    "其中 $\\theta $ 是 $\\mathbf{grad}f(x_0, y_0)$ 与 $ \\mathbf{e}_{l} $ 夹角。上式利用的是内积公式：$ \\vec{a} \\cdot \\vec{b} = \\rvert\\vec{a}\\rvert\\rvert\\vec{b}\\rvert\\cos{<\\widehat{\\vec{a},\\vec{b}}>}$。\n",
    "\n",
    "这一关系式表明了函数在一点的梯度与函数在这一点的方向导数间的关系。\n",
    "特别地，\n",
    "1. 当 $\\theta = 0$ 时，$ \\cos{\\theta} = 1$, 梯度向量与方向导数向量**同向**，函数可以在这个方向的方向导数增加最快，可以达到函数的最大值。\n",
    "2. 当 $\\theta = 180$ 时，$ \\cos{\\theta} = -1$, 梯度向量与方向导数向量**反向**，函数可以在这个方向的方向导数减少最快，可以达到函数的最小值。\n",
    "\n",
    "所在工程上在求函数的最大值时，用梯度上升算法；而求函数最小值时，用梯度下降算法。\n",
    "\n",
    "### 1.3 梯度下降算法\n",
    "\n",
    "其中 $\\alpha$ 是学习因子。\n",
    "\n",
    "![regress-algo.png](attachment:regress-algo.png)\n",
    "\n",
    "当是全部更新时就是指 Batch, 随机沿一个轴更新时就是随机。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 总体，样本，极大似然估计\n",
    "\n",
    "### 2. 1 总体与样本\n",
    "\n",
    "**总体** $X$ , 即是我们研究问题所涉及到的对象的全体。\n",
    "\n",
    "**样本** $X_1, X_2, ... X_n, ...$ , 是从总体随机抽取的一些样品。\n",
    "\n",
    "**样本的二重性：** , 样本即是数，又是随机变量。\n",
    "\n",
    "当样本  $X_1, X_2, ... X_n, ...$ 看成随机变量时，是相互独立且同分布的 （Independent and Indentically Distribued, IID） 并且与 总体 $X$ 的分布相同。\n",
    "\n",
    "假设  $X$ 的概率密度是 $f(x)$, 则 $X_1, X_2, ... X_n$ 与 $ X $ IID, 有其联合概率密度函数：\n",
    "\n",
    "\\begin{equation*}\n",
    "g(x_1, x_2, ..., x_n) = \\prod_{i=1}^{n} f(x_i)\n",
    "\\end{equation*}\n",
    "\n",
    "### 2.2 极大似然估计\n",
    "\n",
    "设总体分布为 $f(x,\\space \\theta_1, ..., \\theta_k)$,  $X_1, X_2, ... X_n$ 为从该总体抽出的样本， 因为  $X_1, X_2, ... X_n $ IID, 所以它们的联合概率密度函数为：\n",
    "\n",
    "\\begin{equation*}\n",
    "L(x_1, x_2, ..., x_n;\\space \\theta_1, ..., \\theta_k) = \\prod_{i=1}^{n} f(x_i,\\space \\theta_1, ..., \\theta_k)\n",
    "\\end{equation*}\n",
    "\n",
    "我们把 $x_1, x_2, ..., x_n$ 看成固定的， 则 $L(x_1, x_2, ..., x_n;\\space \\theta_1, ..., \\theta_k)$ 就是 $\\theta_1, ..., \\theta_k $ 的函数，这时我们称之为似然函数。\n",
    "\n",
    "对 $\\theta$ 的极大似然估计就归结为示 $L(\\theta)$ 的最大值点。 一般极大依然函数都 log 来处理一下方便计算。\n",
    "\n",
    "在 Logistic 回归中用到这个方法，求最大值点，当然要用梯度上升算法了。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
