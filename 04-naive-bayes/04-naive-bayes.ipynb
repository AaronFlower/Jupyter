{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 朴素贝叶斯模型 （Naive Bayes Model）\n",
    "\n",
    "### 1. 全概率公式与贝叶斯公式\n",
    "\n",
    "机器学习中的贝叶斯模型是从贝叶斯公式得到，而贝叶斯又是从全概率公式得到：\n",
    "\n",
    "- 全概率公式\n",
    "\n",
    "假设事件 A, 与一个划分 $B_1, B_2, ..., B_n$, 则事件 A 的概率为：\n",
    "\n",
    "\\begin{align}\n",
    "P(A) &= P[ A( B_1 \\cup B_2 \\cup ... \\cup B_n ) ]\n",
    "\\\\   &= P(AB_1) + P(AB_2) + ... + P(AB_n)\n",
    "\\\\   &= \\sum_{i=1}^n P(AB_i)\n",
    "\\\\   &= \\sum_{i=1}^n P(B_i)P(A \\mid B_i)\n",
    "\\end{align}\n",
    "\n",
    "- 贝叶斯公式\n",
    "\n",
    "从当事件 A 发生时反推 $B_i$ 发生的概率，则有：\n",
    "\n",
    "\\begin{align}\n",
    "P(B_i \\mid A) &= \\frac{P(B_i)P(A \\mid B_i)}{P(A)}\n",
    "\\\\ &= \\frac{P(B_i)P(A \\mid B_i)}{\\sum_{k=1}^n P(B_k)P(A \\mid B_k)}\n",
    "\\end{align}\n",
    "\n",
    "### 2. 文本分类\n",
    "\n",
    "应用到文本分类，可以将文本的所有类别看成是一个划分，如邮件分类时的 spam 与 ham。 利用贝叶斯公式给定一个 txt 时，则其为 ham 与 spam 的概率分别为：\n",
    "\\begin{align}\n",
    "P(ham \\mid txt) &= \\frac {P(txt \\cdot ham)} {P(txt)} = \\frac {P(ham) P(txt \\mid ham)} {P(txt)}\n",
    "\\\\P(spam \\mid txt) &= \\frac {P(txt \\cdot spam)} {P(txt)} = \\frac {P(spam) P(txt \\mid spam)} {P(txt)}\n",
    "\\end{align}\n",
    "\n",
    "两者那个概率大，则 txt 就属于那种分类。\n",
    "\n",
    "用形式化的语言来描述：假设有分类 $c_1, c_2$，则 $P(c_1 \\mid txt), P(c_1 \\mid txt)$ 分别表示为 txt  来自类别  $c_1， c_2$ 的概率分别是多少？ 具体地，应用贝叶斯准则得到：\n",
    "\n",
    "\\begin{align}\n",
    "P(c_i \\mid txt) = \\frac {p(c_i) p(txt \\mid c_i)} {p(txt)}\n",
    "\\end{align}\n",
    "\n",
    "而贝叶斯决策理论的核心思想是：选择具有最高概率的决策。\n",
    "\n",
    "### 3. 算法实现\n",
    "\n",
    "在算法实现上， txt 可以用向量 $\\mathbf{w}$ 来表示 $\\mathbf{w} = (0, 1, 1, ...)$, 0, 1 表示在字典中是否出现。而字典的构造则由所有训练样本中的单词组成。所以用：\n",
    "\n",
    "\\begin{align}\n",
    "P(c_i \\mid \\mathbf{w}) = \\frac {p(c_i) p(\\mathbf{w} \\mid c_i)} {p(\\mathbf{w})}\n",
    "\\end{align}\n",
    "\n",
    "**公式计算说明**：\n",
    "1. 因为在选择最高概率的决策时，只需要对分子进行比较即可，分母不需要计算。\n",
    "2. $P(c_i)$ 的计算方法只需要统计下分类 $c_i$ 在全部训练样本中的比重即可。\n",
    "3. 而对于 $P(\\mathbf{w} \\mid c_i)$ 则就要用到我们的朴素贝叶斯假设了，即假设 $\\mathbf{w}$ 由 n 个单词组成，每个单词 $w_i$ 相互独立，\n",
    "即有：$P(\\mathbf{w} \\mid c_i)$ = $p(w_0, w_1, ..., w_n \\mid c_i) = p(w_0 \\mid c_i)p(w_1 \\mid c_i) ... p(w_n \\mid c_i)$ \n",
    "4. $p(w_k|c_i)$ 的计算最简单有两个方式：\n",
    "    - 词集模型(set-of-words), 只考虑 word 是否在词典中出现。出现为 1，不出现为 0。\n",
    "    - 词包模型(bag-of-words), 不仅考虑 word 是否出现, 而且还记录出现的次数。\n",
    "\n",
    "### 4. 简单的例子\n",
    "训练样本为一些是否表示喜欢的文本\n",
    "\n",
    "Order| text | Like/Dislike|\n",
    "---|---|---|\n",
    " 1| \t\t'I love you' \t\t\t\t\t\t\t\t\t| 0\n",
    " 2| \t\t'Glad glad glad glad see you' | 0\n",
    " 3| \t\t'happy happy with you' \t\t\t\t| 0\n",
    " 4| \t\t'Sad talk with you sad' \t\t\t| 1\n",
    " 5| \t\t'I hate hate hate you' \t\t\t\t| 1\n",
    " 6| \t\t'I dislike you' \t\t\t\t\t\t\t| 1\n",
    " \n",
    "#### a. 创建字典\n",
    "\n",
    "字典为：\n",
    "```\n",
    "['love', 'i', 'with', 'sad', 'see', 'talk', 'you', 'hate', 'dislike', 'glad', 'happy']\n",
    "```\n",
    "#### b. 根据词集模型表示每个训练样本\n",
    "\n",
    "第个训练样本可以用向量 $\\mathbf{w}$ 来表示。\n",
    "\n",
    "```\n",
    "\t[\t\n",
    "\t\t[1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0], # like 0, num of word, 3\n",
    "\t\t[0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0], # like 0, num of word, 3\n",
    "\t\t[0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1], # like 0, num of word, 3\n",
    "\n",
    "\t\t[0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0], # dislike 1, num of word, 4\n",
    "\t\t[0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0], # dislike 1, num of word, 3\n",
    "\t\t[0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0], # dislike 1, num of word, 3\n",
    "\t]\n",
    "```\n",
    "\n",
    "- 计算所有单词在 like 分类的 $p(w_i \\mid like)$ 概率向量\n",
    "\n",
    "    在 like 分类中，单词的出现的总次数为 3 + 3 + 3 =  9， 每个单词出现的概率组成的概率向量为：\n",
    "\n",
    "\\begin{align}\n",
    "[p(w_0 \\mid like), p(w_1 \\mid like), ..., p(w_8 \\mid like)]  \n",
    "\\\\ &= [p(love \\mid like), p(i \\mid like), ..., p(happy \\mid like)] \n",
    "\\\\ &= [ 0.1111,  0.1111,  0.1111,  0. ,   0.1111 , 0. , 0.3333,  0. ,  0.  , 0.1111 , 0.1111]\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "- 计算所有单词在 dislike 分类的 $p(w_i \\mid dislike)$ 概率向量\n",
    "\n",
    "    在 dislike 分类中，单词出现的总次数 4 + 3 + 3 = 10， 每个单词出现的概率组成的概率向量为：\n",
    "\n",
    "\\begin{align}\n",
    "[p(w_0 \\mid dislike), p(w_1 \\mid dislike), ..., p(w_8 \\mid dislike)]  \n",
    "\\\\ &= [p(love \\mid dislike), p(i \\mid dislike), ..., p(happy \\mid dislike)] \n",
    "\\\\ &= [ 0. ,  0.2 , 0.1 , 0.1 , 0.  ,  0.1 ,  0.3  , 0.1  , 0.1 ,  0.  ,  0. ] \n",
    "\\end{align}\n",
    "\n",
    "\n",
    "- 计算 $p(like), p(dislike)$\n",
    "\n",
    "```\n",
    "    p(like) = 3/6 = 0.5\n",
    "    p(dislike) = 3/6 = 0.5\n",
    "```\n",
    "\n",
    "#### c. 判定文本\n",
    "\n",
    "计算出了  $p(like), p(dislike), p(w_i \\mid like), p(w_i \\mid dislike)$ 就训练出来模型了。就可以对输入文本进行判断了。\n",
    "\n",
    "假设输入广本为：msg = 'I love like you', 则其对应的词集向量为$\\mathbf{w_{msg}}$ : [1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
    "\n",
    "则有： \n",
    "\n",
    "\\begin{align}\n",
    "p(like \\mid \\mathbf{w_{msg}}) &= \\frac {p(like) * p(\\mathbf{w_{msg}} \\mid like)}{p(\\mathbf{w_{msg}})}\n",
    "\\\\ &= \\frac { [ 0.1111,  0.1111,  0.1111,  0. ,   0.1111 , 0., 0.3333,  0. ,  0.  , 0.1111 , 0.1111] * [1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0]}{p(\\mathbf{w_{msg}})}\n",
    "\\\\\n",
    "p(dislike \\mid \\mathbf{w_{msg}}) &= \\frac {p(dislike) * p(\\mathbf{w_{msg}} \\mid dislike)}{p(\\mathbf{w_{msg}})}\n",
    "\\\\ &= \\frac { [ 0. ,  0.2 , 0.1 , 0.1 , 0.  ,  0.1 ,  0.3  , 0.1  , 0.1 ,  0.  ,  0. ]  * [1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0]}{p(\\mathbf{w_{msg}})}\n",
    "\\end{align}\n",
    "\n",
    "选择概率最大的就是文本的分类。\n",
    "\n",
    "**注意：** \n",
    "\n",
    "1. 上面的概率$P(\\mathbf{w} \\mid c_i)$ = $p(w_0, w_1, ..., w_n \\mid c_i) = p(w_0 \\mid c_i)p(w_1 \\mid c_i) ... p(w_n \\mid c_i)$ 相乘都为 0. 因为有些单词未出现概率就是为0，解决方法是: **可以将所有词出现的次数初始化为1， 并将单词总数的分母初始化为2.**\n",
    "\n",
    "2. $P(\\mathbf{w} \\mid c_i)$ = $p(w_0, w_1, ..., w_n \\mid c_i) = p(w_0 \\mid c_i)p(w_1 \\mid c_i) ... p(w_n \\mid c_i)$ 计算时，由于大部分因子都非常小，所以程序会下溢或者得不到正确答案，**我们可以用 `ln(a/b)` 函数来替代 `a/b`计算概率**。\n",
    "\n",
    "\n",
    "\n",
    "#### d. 说明\n",
    "\n",
    "上面的模型计算我们使用了词集模型，我们也可使用词包模型，对于词包模型对应的向量及 $p(like), p(dislike)$的结果为：\n",
    "\n",
    "```\n",
    "[\n",
    "\t[1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "\t[0, 0, 0, 0, 1, 0, 1, 0, 0, 4, 0],\n",
    "\t[0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 2],\n",
    "\n",
    "\t[0, 0, 1, 2, 0, 1, 1, 0, 0, 0, 0],\n",
    "\t[0, 1, 0, 0, 0, 0, 1, 3, 0, 0, 0],\n",
    "\t[0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0],\n",
    "]\n",
    "\n",
    "[ 0.0769, 0.0769, 0.0769, 0., 0.0769, 0. , 0.2307, 0. , 0. , 0.3076, 0.1538]\n",
    "[ 0. , 0.1538, 0.0769, 0.1538, 0.  , 0.0769, 0.2307, 0.2307, 0.0769, 0.  , 0. ]\n",
    "```\n",
    "\n",
    "是使用词包模型还是词集模型，可以通过测试看一下效果来决定。\n",
    "\n",
    "**注意：**\n",
    "\n",
    "每个分为 $c_i$ 对应的分类字典是不一样的, 出现的总次数也不一样，也可能一样。\n",
    "\n",
    "\n",
    "### 5. 代码实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load dataset:\n",
      "{'dog', 'how', 'my', 'food', 'buying', 'him', 'i', 'take', 'please', 'worthless', 'love', 'posting', 'help', 'quit', 'is', 'has', 'steak', 'flea', 'mr', 'to', 'so', 'ate', 'problems', 'park', 'licks', 'maybe', 'cute', 'stop', 'dalmation', 'stupid', 'garbage', 'not'}\n",
      "[0, 1, 0, 1, 0, 1]\n",
      "\n",
      "Test words to vector:\n",
      "[ 0.  0.  1.  0.  0.  0.  1.  0.  0.  0.  1.  0.  0.  0.  0.  0.  1.  0.\n",
      "  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "[ 0.  0.  1.  0.  0.  0.  1.  0.  0.  0.  2.  0.  0.  0.  0.  0.  1.  0.\n",
      "  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "\n",
      "Training Naive Bayes Model:\n",
      "[-0.69314718 -0.69314718]\n",
      "[[-2.56494936 -2.56494936 -1.87180218 -3.25809654 -3.25809654 -2.15948425\n",
      "  -2.56494936 -3.25809654 -2.56494936 -3.25809654 -2.56494936 -3.25809654\n",
      "  -2.56494936 -3.25809654 -2.56494936 -2.56494936 -2.56494936 -2.56494936\n",
      "  -2.56494936 -2.56494936 -2.56494936 -2.56494936 -2.56494936 -3.25809654\n",
      "  -2.56494936 -3.25809654 -2.56494936 -2.56494936 -2.56494936 -3.25809654\n",
      "  -3.25809654 -3.25809654]\n",
      " [-1.94591015 -3.04452244 -3.04452244 -2.35137526 -2.35137526 -2.35137526\n",
      "  -3.04452244 -2.35137526 -3.04452244 -1.94591015 -3.04452244 -2.35137526\n",
      "  -3.04452244 -2.35137526 -3.04452244 -3.04452244 -3.04452244 -3.04452244\n",
      "  -3.04452244 -2.35137526 -3.04452244 -3.04452244 -3.04452244 -2.35137526\n",
      "  -3.04452244 -2.35137526 -3.04452244 -2.35137526 -3.04452244 -1.65822808\n",
      "  -2.35137526 -2.35137526]]\n",
      "\n",
      " Test Naive Bayes Model:\n",
      "I love love to see my cat eat steak\n",
      "[-12.82474679 -15.22261219] [0, 1]\n",
      "The class is: 0\n",
      "\n",
      "You hate my  dog, stop do it, stupid\n",
      "[-8.38799525 -7.74727295] [1, 0]\n",
      "The class is: 1\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "encoding: utf8\n",
    "朴素贝叶斯分类模型\n",
    "'''\n",
    "import numpy as np\n",
    "from math import log\n",
    "\n",
    "# 加载样本数据，下面是收集到的 6 条评论，并且有相应的分类标签。\n",
    "def loadDataset():\n",
    "    \n",
    "  postingList = [\n",
    "    ['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
    "    ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "    ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "    ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "    ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "    ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']\n",
    "  ]\n",
    "  classVec = [0, 1, 0, 1, 0, 1]\n",
    "    \n",
    "  return postingList, classVec\n",
    "\n",
    "# 创建朴素贝叶斯模型的字典\n",
    "def createVocabulary(postList):\n",
    "    \n",
    "    vocabSet = set([])\n",
    "    for post in postList:\n",
    "        vocabSet = vocabSet | set([word.lower() for word in post]) # 注意这里的 “|“ 不在是位运算的“或“而是集合运算中的并集。\n",
    "    \n",
    "    return vocabSet\n",
    "\n",
    "\n",
    "'''\n",
    "将文本样本 words 转换成与字典对应的向量, 可以选择是词包模型(bag-of-words)，还是词集模型(set-of-words)。\n",
    "'''\n",
    "def wordsToVector(vocabSet, words, type='set'):\n",
    "    numWords = len(vocabSet)\n",
    "    wordsVec = np.zeros(numWords)\n",
    "    words = [word.lower() for word in words]\n",
    "    index = 0\n",
    "    for word in vocabSet:\n",
    "        if word in words:\n",
    "            # set-of-words\n",
    "            if type == 'set':\n",
    "                wordsVec[index] = 1\n",
    "            # bag-of-words\n",
    "            else:\n",
    "                wordsVec[index] = words.count(word)\n",
    "        index += 1\n",
    "    \n",
    "    return wordsVec\n",
    "\n",
    "'''\n",
    "训练朴素贝叶斯模型，计算所有分类的概率向量。不仅支持两分的模型，也支持多于两个文本分类的模型。\n",
    "为了防止数据溢出，我们使用 log 函数来处理下实际的概率。\n",
    "'''\n",
    "def calcClassProbability(vocabSet, postList, classVec):\n",
    "    classSet = set(classVec)\n",
    "    numExamples = len(classVec)\n",
    "    numClasses = len(classSet)\n",
    "    numWords = len(vocabSet)\n",
    "    classProbs = np.zeros(numClasses)\n",
    "    # 计算 p(c_i)\n",
    "    for ci in classSet:\n",
    "        classProbs[ci] = log(classVec.count(ci) / numExamples)\n",
    "        \n",
    "    \n",
    "    classProbVecs = np.ones((numClasses, numWords))\n",
    "    classDenoms = np.full(numClasses, 2)\n",
    "    \n",
    "    index = 0\n",
    "    for post in postList:\n",
    "        postVec = wordsToVector(vocabSet, post)\n",
    "        postClass = classVec[index]\n",
    "        classProbVecs[postClass] += postVec\n",
    "        classDenoms[postClass] += sum(postVec) # 用 sum 函数对词包模型也有效。\n",
    "        index += 1\n",
    "    \n",
    "    # 计算 p(w_i | c_i)\n",
    "    index = 0\n",
    "    for probVec in classProbVecs:\n",
    "        #probVec /= classDenoms[index]\n",
    "        for i in range(numWords):\n",
    "            probVec[i] = log(probVec[i] / classDenoms[index])\n",
    "        index += 1\n",
    "        \n",
    "    return classProbs, classProbVecs\n",
    "\n",
    "# 分类函数\n",
    "def classifyNaiveBayes(classProbs, classProbVecs, verifyExampleVec):\n",
    "    numClasses = len(classProbs)\n",
    "    probs = np.zeros(numClasses)\n",
    "    for i in range(numClasses):\n",
    "        probs[i] = classProbs[i]  +  sum(classProbVecs[i] * verifyExampleVec) # log(a) + log(b) = log(a * b)\n",
    "    indices = np.argsort(probs)\n",
    "    reversedIndices = list(indices)\n",
    "    reversedIndices.reverse()\n",
    "    \n",
    "    print(probs, reversedIndices)\n",
    "    return reversedIndices[0]\n",
    "    \n",
    "\n",
    "print('Load dataset:')\n",
    "postList, classVec = loadDataset()\n",
    "vocabSet = createVocabulary(postList)\n",
    "print(vocabSet)\n",
    "print(classVec)\n",
    "\n",
    "print('\\nTest words to vector:')\n",
    "comment = 'I love love to see my cat eat steak'\n",
    "commentSetVec = wordsToVector(vocabSet, comment.split())\n",
    "print(commentSetVec)\n",
    "commentBagVec = wordsToVector(vocabSet, comment.split(), type = 'bag')\n",
    "print(commentBagVec)\n",
    "\n",
    "print('\\nTraining Naive Bayes Model:')\n",
    "classProbs, classProbVecs = calcClassProbability(vocabSet, postList, classVec)\n",
    "print(classProbs)\n",
    "print(classProbVecs)\n",
    "\n",
    "print('\\n Test Naive Bayes Model:')\n",
    "comment = 'I love love to see my cat eat steak'\n",
    "commentSetVec = wordsToVector(vocabSet, comment.split())\n",
    "print(comment)\n",
    "commentClass = classifyNaiveBayes(classProbs, classProbVecs, commentSetVec)\n",
    "print('The class is:', commentClass)\n",
    "\n",
    "print()\n",
    "comment = 'You hate my  dog, stop do it, stupid'\n",
    "commentSetVec = wordsToVector(vocabSet, comment.split())\n",
    "print(comment)\n",
    "commentClass = classifyNaiveBayes(classProbs, classProbVecs, commentSetVec)\n",
    "print('The class is:', commentClass)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
