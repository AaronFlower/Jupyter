{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 朴素贝叶斯模型 （Naive Bayes Model）\n",
    "\n",
    "### 1. 全概率公式与贝叶斯公式\n",
    "\n",
    "机器学习中的贝叶斯模型是从贝叶斯公式得到，而贝叶斯又是从全概率公式得到：\n",
    "\n",
    "- 全概率公式\n",
    "\n",
    "假设事件 A, 与一个划分 $B_1, B_2, ..., B_n$, 则事件 A 的概率为：\n",
    "\n",
    "\\begin{align}\n",
    "P(A) &= P[ A( B_1 \\cup B_2 \\cup ... \\cup B_n ) ]\n",
    "\\\\   &= P(AB_1) + P(AB_2) + ... + P(AB_n)\n",
    "\\\\   &= \\sum_{i=1}^n P(AB_i)\n",
    "\\\\   &= \\sum_{i=1}^n P(B_i)P(A \\mid B_i)\n",
    "\\end{align}\n",
    "\n",
    "- 贝叶斯公式\n",
    "\n",
    "从当事件 A 发生时反推 $B_i$ 发生的概率，则有：\n",
    "\n",
    "\\begin{align}\n",
    "P(B_i \\mid A) &= \\frac{P(B_i)P(A \\mid B_i)}{P(A)}\n",
    "\\\\ &= \\frac{P(B_i)P(A \\mid B_i)}{\\sum_{k=1}^n P(B_k)P(A \\mid B_k)}\n",
    "\\end{align}\n",
    "\n",
    "### 2. 文本分类\n",
    "\n",
    "应用到文本分类，可以将文本的所有类别看成是一个划分，如邮件分类时的 spam 与 ham。 利用贝叶斯公式给定一个 txt 时，则其为 ham 与 spam 的概率分别为：\n",
    "\\begin{align}\n",
    "P(ham \\mid txt) &= \\frac {P(txt \\cdot ham)} {P(txt)} = \\frac {P(ham) P(txt \\mid ham)} {P(txt)}\n",
    "\\\\P(spam \\mid txt) &= \\frac {P(txt \\cdot spam)} {P(txt)} = \\frac {P(spam) P(txt \\mid spam)} {P(txt)}\n",
    "\\end{align}\n",
    "\n",
    "两者那个概率大，则 txt 就属于那种分类。\n",
    "\n",
    "用形式化的语言来描述：假设有分类 $c_1, c_2$，则 $P(c_1 \\mid txt), P(c_1 \\mid txt)$ 分别表示为 txt  来自类别  $c_1， c_2$ 的概率分别是多少？ 具体地，应用贝叶斯准则得到：\n",
    "\n",
    "\\begin{align}\n",
    "P(c_i \\mid txt) = \\frac {p(c_i) p(txt \\mid c_i)} {p(txt)}\n",
    "\\end{align}\n",
    "\n",
    "而贝叶斯决策理论的核心思想是：选择具有最高概率的决策。\n",
    "\n",
    "### 3. 算法实现\n",
    "\n",
    "在算法实现上， txt 可以用向量 $\\mathbf{w}$ 来表示 $\\mathbf{w} = (0, 1, 1, ...)$, 0, 1 表示在字典中是否出现。而字典的构造则由所有训练样本中的单词组成。所以用：\n",
    "\n",
    "\\begin{align}\n",
    "P(c_i \\mid \\mathbf{w}) = \\frac {p(c_i) p(\\mathbf{w} \\mid c_i)} {p(\\mathbf{w})}\n",
    "\\end{align}\n",
    "\n",
    "**公式计算说明**：\n",
    "1. 因为在选择最高概率的决策时，只需要对分子进行比较即可，分母不需要计算。\n",
    "2. $P(c_i)$ 的计算方法只需要统计下分类 $c_i$ 在全部训练样本中的比重即可。\n",
    "3. 而对于 $P(\\mathbf{w} \\mid c_i)$ 则就要用到我们的朴素贝叶斯假设了，即假设 $\\mathbf{w}$ 由 n 个单词组成，每个单词 $w_i$ 相互独立，\n",
    "即有：$P(\\mathbf{w} \\mid c_i)$ = $p(w_0, w_1, ..., w_n \\mid c_i) = p(w_0 \\mid c_i)p(w_1 \\mid c_i) ... p(w_n \\mid c_i)$ \n",
    "4. $p(w_k|c_i)$ 的计算最简单有两个方式：\n",
    "    - 词集模型(set-of-words), 只考虑 word 是否在词典中出现。出现为 1，不出现为 0。\n",
    "    - 词包模型(bag-of-words), 不仅考虑 word 是否出现, 而且还记录出现的次数。\n",
    "\n",
    "### 4. 简单的例子\n",
    "训练样本为一些是否表示喜欢的文本\n",
    "\n",
    "Order| text | Like/Dislike|\n",
    "---|---|---|\n",
    " 1| \t\t'I love you' \t\t\t\t\t\t\t\t\t| 0\n",
    " 2| \t\t'Glad glad glad glad see you' | 0\n",
    " 3| \t\t'happy happy with you' \t\t\t\t| 0\n",
    " 4| \t\t'Sad talk with you sad' \t\t\t| 1\n",
    " 5| \t\t'I hate hate hate you' \t\t\t\t| 1\n",
    " 6| \t\t'I dislike you' \t\t\t\t\t\t\t| 1\n",
    " \n",
    "#### a. 创建字典\n",
    "\n",
    "字典为：\n",
    "```\n",
    "['love', 'i', 'with', 'sad', 'see', 'talk', 'you', 'hate', 'dislike', 'glad', 'happy']\n",
    "```\n",
    "#### b. 根据词集模型表示每个训练样本\n",
    "\n",
    "第个训练样本可以用向量 $\\mathbf{w}$ 来表示。\n",
    "\n",
    "```\n",
    "\t[\t\n",
    "\t\t[1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0], # like 0, num of word, 3\n",
    "\t\t[0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0], # like 0, num of word, 3\n",
    "\t\t[0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1], # like 0, num of word, 3\n",
    "\n",
    "\t\t[0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0], # diklike 1, num of word, 4\n",
    "\t\t[0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0], # diklike 1, num of word, 3\n",
    "\t\t[0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0], # diklike 1, num of word, 3\n",
    "\t]\n",
    "```\n",
    "\n",
    "- 计算 $p(w_i \\mid like)$\n",
    "\n",
    "    则可以看来 like 分类的单词出现的总次数 3 + 3 + 3 =  9， 当为 like 分类时，每个单词出现的概率分别为：\n",
    "\n",
    "\\begin{align}\n",
    "[p(w_0 \\mid like), p(w_1 \\mid like), ..., p(w_8 \\mid like)]  \n",
    "\\\\ &= [p(love \\mid like), p(i \\mid like), ..., p(happy \\mid like)] \n",
    "\\\\ &= [ 0.1111,  0.1111,  0.1111,  0. ,   0.1111 , 0. , 0.3333,  0. ,  0.  , 0.1111 , 0.1111]\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "- 计算 $p(w_i \\mid dislike)$\n",
    "\n",
    "    而 dislike 分类的单词出现的总次数 4 + 3 + 3 = 10, 当为 dislike 分类时，每个单词出现的概率分别为：\n",
    "\n",
    "\\begin{align}\n",
    "[p(w_0 \\mid dislike), p(w_1 \\mid dislike), ..., p(w_8 \\mid dislike)]  \n",
    "\\\\ &= [p(love \\mid dislike), p(i \\mid dislike), ..., p(happy \\mid dislike)] \n",
    "\\\\ &= [ 0. ,  0.2 , 0.1 , 0.1 , 0.  ,  0.1 ,  0.3  , 0.1  , 0.1 ,  0.  ,  0. ] \n",
    "\\end{align}\n",
    "\n",
    "\n",
    "- 计算 $p(like), p(dislike)$\n",
    "\n",
    "```\n",
    "    p(like) = 3/6 = 0.5\n",
    "    p(dislike) = 3/6 = 0.5\n",
    "```\n",
    "\n",
    "#### c. 判定文本\n",
    "\n",
    "计算出了  $p(like), p(dislike), p(w_i \\mid like), p(w_i \\mid dislike)$ 就训练出来模型了。就可以对输入文本进行判断了。\n",
    "\n",
    "假设输入广本为：msg = 'I love like you', 则其对应的词集向量为$\\mathbf{w_{msg}}$ : [1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
    "\n",
    "则有： \n",
    "\n",
    "\\begin{align}\n",
    "p(like \\mid \\mathbf{w_{msg}}) &= \\frac {p(like) * p(\\mathbf{w_{msg}} \\mid like)}{p(\\mathbf{w_{msg}})}\n",
    "\\\\ &= \\frac { [ 0.1111,  0.1111,  0.1111,  0. ,   0.1111 , 0., 0.3333,  0. ,  0.  , 0.1111 , 0.1111] * [1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0]}{p(\\mathbf{w_{msg}})}\n",
    "\\\\\n",
    "p(dislike \\mid \\mathbf{w_{msg}}) &= \\frac {p(dislike) * p(\\mathbf{w_{msg}} \\mid dislike)}{p(\\mathbf{w_{msg}})}\n",
    "\\\\ &= \\frac { [ 0. ,  0.2 , 0.1 , 0.1 , 0.  ,  0.1 ,  0.3  , 0.1  , 0.1 ,  0.  ,  0. ]  * [1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0]}{p(\\mathbf{w_{msg}})}\n",
    "\\end{align}\n",
    "\n",
    "选择概率最大的就是文本的分类。\n",
    "\n",
    "**注意：** \n",
    "\n",
    "1. 上面的概率$P(\\mathbf{w} \\mid c_i)$ = $p(w_0, w_1, ..., w_n \\mid c_i) = p(w_0 \\mid c_i)p(w_1 \\mid c_i) ... p(w_n \\mid c_i)$ 相乘都为 0. 因为有些单词未出现概率就是为0，解决方法是: **可以将所有词出现的次数初始化为1， 并将单词总数的分母初始化为2.**\n",
    "\n",
    "2. $P(\\mathbf{w} \\mid c_i)$ = $p(w_0, w_1, ..., w_n \\mid c_i) = p(w_0 \\mid c_i)p(w_1 \\mid c_i) ... p(w_n \\mid c_i)$ 计算时，由于大部分因子都非常小，所以程序会下溢或者得不到正确答案，**我们可以用 `ln(a/b)` 函数来替代 `a/b`计算概率**。\n",
    "\n",
    "\n",
    "\n",
    "#### d. 说明\n",
    "\n",
    "上面的模型计算我们使用了词集模型，我们也可使用词包模型，对于词包模型对应的向量及 $p(like), p(dislike)$的结果为：\n",
    "\n",
    "```\n",
    "[\n",
    "\t[1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "\t[0, 0, 0, 0, 1, 0, 1, 0, 0, 4, 0],\n",
    "\t[0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 2],\n",
    "\n",
    "\t[0, 0, 1, 2, 0, 1, 1, 0, 0, 0, 0],\n",
    "\t[0, 1, 0, 0, 0, 0, 1, 3, 0, 0, 0],\n",
    "\t[0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0],\n",
    "]\n",
    "\n",
    "[ 0.0769, 0.0769, 0.0769, 0., 0.0769, 0. , 0.2307, 0. , 0. , 0.3076, 0.1538]\n",
    "[ 0. , 0.1538, 0.0769, 0.1538, 0.  , 0.0769, 0.2307, 0.2307, 0.0769, 0.  , 0. ]\n",
    "```\n",
    "\n",
    "是使用词包模型还是词集模型，可以通过测试看一下效果来决定。\n",
    "\n",
    "**注意：**\n",
    "\n",
    "每个分为 $c_i$ 对应的分类字典是不一样的, 出现的总次数也不一样，也可能一样。\n",
    "\n",
    "\n",
    "### 5. 代码实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 简单的例子\n",
    "训练样本为一些是否表示喜欢的文本\n",
    "\n",
    "Order| text | Like/Dislike|\n",
    "---|---|---|\n",
    " 1| \t\t'I love you' \t\t\t\t\t\t\t\t\t| 0\n",
    " 2| \t\t'Glad glad glad glad see you' | 0\n",
    " 3| \t\t'happy happy with you' \t\t\t\t| 0\n",
    " 4| \t\t'Sad talk with you sad' \t\t\t| 1\n",
    " 5| \t\t'I hate hate hate you' \t\t\t\t| 1\n",
    " 6| \t\t'I dislike you' \t\t\t\t\t\t\t| 1\n",
    " \n",
    "#### a. 创建字典\n",
    "\n",
    "字典为：\n",
    "```\n",
    "['love', 'i', 'with', 'sad', 'see', 'talk', 'you', 'hate', 'dislike', 'glad', 'happy']\n",
    "```\n",
    "#### b. 根据词集模型表示每个训练样本\n",
    "\n",
    "第个训练样本可以用向量 $\\mathbf{w}$ 来表示。\n",
    "\n",
    "```\n",
    "\t[\t\n",
    "\t\t[1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0], # like 0, num of word, 3\n",
    "\t\t[0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0], # like 0, num of word, 3\n",
    "\t\t[0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1], # like 0, num of word, 3\n",
    "\n",
    "\t\t[0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0], # diklike 1, num of word, 4\n",
    "\t\t[0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0], # diklike 1, num of word, 3\n",
    "\t\t[0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0], # diklike 1, num of word, 3\n",
    "\t]\n",
    "```\n",
    "\n",
    "- 计算 $p(w_i \\mid like)$\n",
    "\n",
    "    则可以看来 like 分类的字典的单词个数一共为 3 + 3 + 3 =  9， 当为 like 分类时，每个单词出现的概率分别为：\n",
    "\n",
    "\\begin{align}\n",
    "[p(w_0 \\mid like), p(w_1 \\mid like), ..., p(w_8 \\mid like)]  \n",
    "\\\\ &= [p(love \\mid like), p(i \\mid like), ..., p(happy \\mid like)] \n",
    "\\\\ &= [ 0.1111,  0.1111,  0.1111,  0. ,   0.1111 , 0. , 0.3333,  0. ,  0.  , 0.1111 , 0.1111]\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "- 计算 $p(w_i \\mid dislike)$\n",
    "\n",
    "    而 dislike 分类的字典的单词个数一共为 4 + 3 + 3 = 10, 当为 dislike 分类时，每个单词出现的概率分别为：\n",
    "\n",
    "\\begin{align}\n",
    "[p(w_0 \\mid dislike), p(w_1 \\mid dislike), ..., p(w_8 \\mid dislike)]  \n",
    "\\\\ &= [p(love \\mid dislike), p(i \\mid dislike), ..., p(happy \\mid dislike)] \n",
    "\\\\ &= [ 0. ,  0.2 , 0.1 , 0.1 , 0.  ,  0.1 ,  0.3  , 0.1  , 0.1 ,  0.  ,  0. ] \n",
    "\\end{align}\n",
    "\n",
    "\n",
    "- 计算 $p(like), p(dislike)$\n",
    "\n",
    "```\n",
    "    p(like) = 3/6 = 0.5\n",
    "    p(dislike) = 3/6 = 0.5\n",
    "```\n",
    "\n",
    "#### c. 判定文本\n",
    "\n",
    "计算出了  $p(like), p(dislike), p(w_i \\mid like), p(w_i \\mid dislike)$ 就训练出来模型了。就可以对输入文本进行判断了。\n",
    "\n",
    "假设输入广本为：msg = 'I love like you', 则其对应的词集向量为$\\mathbf{w_{msg}}$ : [1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
    "\n",
    "则有： \n",
    "\n",
    "\\begin{align}\n",
    "p(like \\mid \\mathbf{w_{msg}}) &= \\frac {p(like) * p(\\mathbf{w_{msg}} \\mid like)}{p(\\mathbf{w_{msg}})}\n",
    "\\\\ &= \\frac { [ 0.1111,  0.1111,  0.1111,  0. ,   0.1111 , 0., 0.3333,  0. ,  0.  , 0.1111 , 0.1111] * [1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0]}{p(\\mathbf{w_{msg}})}\n",
    "\\\\\n",
    "p(dislike \\mid \\mathbf{w_{msg}}) &= \\frac {p(dislike) * p(\\mathbf{w_{msg}} \\mid dislike)}{p(\\mathbf{w_{msg}})}\n",
    "\\\\ &= \\frac { [ 0. ,  0.2 , 0.1 , 0.1 , 0.  ,  0.1 ,  0.3  , 0.1  , 0.1 ,  0.  ,  0. ]  * [1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0]}{p(\\mathbf{w_{msg}})}\n",
    "\\end{align}\n",
    "\n",
    "选择概率最大的就是文本的分类。\n",
    "\n",
    "**注意：** \n",
    "\n",
    "1. 上面的概率$P(\\mathbf{w} \\mid c_i)$ = $p(w_0, w_1, ..., w_n \\mid c_i) = p(w_0 \\mid c_i)p(w_1 \\mid c_i) ... p(w_n \\mid c_i)$ 相乘都为 0. 因为有些单词未出现概率就是为0，解决方法是: **可以将所有词出现的次数初始化为1， 并将单词总数的分母初始化为2.**\n",
    "\n",
    "2. $P(\\mathbf{w} \\mid c_i)$ = $p(w_0, w_1, ..., w_n \\mid c_i) = p(w_0 \\mid c_i)p(w_1 \\mid c_i) ... p(w_n \\mid c_i)$ 计算时，由于大部分因子都非常小，所以程序会下溢或者得不到正确答案，**我们可以用 `ln(a/b)` 函数来替代 `a/b`计算概率**。\n",
    "\n",
    "\n",
    "\n",
    "#### d. 说明\n",
    "\n",
    "上面的模型计算我们使用了词集模型，我们也可使用词包模型，对于词包模型对应的向量及 $p(like), p(dislike)$的结果为：\n",
    "\n",
    "```\n",
    "[\n",
    "\t[1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "\t[0, 0, 0, 0, 1, 0, 1, 0, 0, 4, 0],\n",
    "\t[0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 2],\n",
    "\n",
    "\t[0, 0, 1, 2, 0, 1, 1, 0, 0, 0, 0],\n",
    "\t[0, 1, 0, 0, 0, 0, 1, 3, 0, 0, 0],\n",
    "\t[0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0],\n",
    "]\n",
    "\n",
    "[ 0.0769, 0.0769, 0.0769, 0., 0.0769, 0. , 0.2307, 0. , 0. , 0.3076, 0.1538]\n",
    "[ 0. , 0.1538, 0.0769, 0.1538, 0.  , 0.0769, 0.2307, 0.2307, 0.0769, 0.  , 0. ]\n",
    "```\n",
    "\n",
    "是使用词包模型还是词集模型，可以通过测试看一下效果来决定。\n",
    "\n",
    "**注意：**\n",
    "\n",
    "每个分为 $c_i$ 对应的分类字典是不一样的, 字典个数也不一样，但是如果足够大的话，各分类的字典也可能一样。\n",
    "\n",
    "\n",
    "### 5. 代码实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
