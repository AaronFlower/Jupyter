{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "classified-script",
   "metadata": {},
   "source": [
    "## Text Processing and Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interstate-framework",
   "metadata": {},
   "source": [
    "对 IMDB 电影评论的数据进行分析。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "level-chicken",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import re\n",
    "import string\n",
    "import pathlib\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Flatten\n",
    "from tensorflow.keras.layers import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "retired-arcade",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_path = pathlib.Path('./../../../dataset/imdb/aclImdb')\n",
    "train_path = imdb_path / 'train'\n",
    "test_path = imdb_path / 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "descending-carnival",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('../../../dataset/imdb/aclImdb/train/.DS_Store'),\n",
       " PosixPath('../../../dataset/imdb/aclImdb/train/neg'),\n",
       " PosixPath('../../../dataset/imdb/aclImdb/train/urls_pos.txt'),\n",
       " PosixPath('../../../dataset/imdb/aclImdb/train/urls_neg.txt'),\n",
       " PosixPath('../../../dataset/imdb/aclImdb/train/pos')]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(train_path.iterdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "raising-piece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Using 20000 files for training.\n",
      "Using 5000 files for validation.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 512\n",
    "seed = 33\n",
    "train_ds, val_ds = tf.keras.utils.text_dataset_from_directory(train_path,\n",
    "                                          shuffle=True,\n",
    "                                          seed=33,\n",
    "                                          validation_split=0.2,\n",
    "                                          subset='both')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "skilled-laugh",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 b'This is one military drama I like a lot! Tom Berenger playing military assassin Thomas Beckett. This Marine is no-nonsense, in your face, and no questions asked kind of person who gets the job done. There you have Billy Zane(\"The Phantom\" and others) who plays Richard Miller, a former SWAT form D.C., works for the government and takes orders only from them. Who needs a bureaucrat? I don\\'t! When these two are paired, sparks should be flying. And how. However, Beckett teaches the young bureaucrat on how it works. When the other sniper hits, it\\'s wits vs. wits, cat vs. mouse, gunman vs. gunman. And when the seasoned sniper is caught, it\\'s up to Miller to put politics aside and save him. Who needs politics when you a pro like Beckett, he took orders from no one but himself, plays by the rules and not the book, and mutual respect is brought out despite the politics. The movie was a direct hit. Watch it. Rating 4 out of 5 stars.'\n",
      "0 b\"A truly frightening film. Feels as if it were made in the early '90s by a straight person who wanted to show that gays are good, normal, mainstream-aspiring people. Retrograde to the point of being offensive, LTR suggests that monogamy and marriage are the preferred path to salvation for sad, lonely, sex-crazed gays. Wow! Who knew? The supporting characters are caricatures of gay stereotypes (the effeminate buffoon, the bitter, lonely queen, the fag hag, etc.) and the main characters are milquetoast, middle-class, middlebrow clones, of little interest.<br /><br />As far as the romantic & ideological struggles of the main couple are concerned, there's not much to say: we've seen it all before, and done much better.\"\n",
      "1 b'\"The Garden of Allah\" was one of the first feature length, 3-strip Technicolor films. To correct a previous poster the first Technicolor feature (after Disney\\'s 5-year exclusivity deal) was 1935\\'s \"Becky Sharp\" which was a costume drama that used the color for it\\'s garish color costumes.<br /><br />\"The Garden of Allah\" looks as if it could have been shot years later as the cinematography uses not only the color but also the use of shadows. It must have been amazing for an audiences at the time to see a color feature after seeing basically only black and white films for their whole life. Unfortunately, the film does not stand up to the cinematography. That being said, the film is worth seeing just as a visual treat.'\n"
     ]
    }
   ],
   "source": [
    "for text_batch, label_batch in train_ds.take(1):\n",
    "    for i in range(3):\n",
    "        print(label_batch[i].numpy(), text_batch.numpy()[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominican-legend",
   "metadata": {},
   "source": [
    "#### [Text Preprocessing](https://www.tensorflow.org/text/guide/word_embeddings#text_preprocessing)\n",
    "\n",
    "我们需要对文本数据进行一下预处理，然后将文本进行向量化。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "floating-eclipse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom standardization function to strip HTML '<br/>'.\n",
    "def custom_standardization(input_data):\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    stripped = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
    "    return tf.strings.regex_replace(stripped,\n",
    "                                    '[%s]' % re.escape(string.punctuation), '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "sporting-religion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary size and number of words in a sequence.\n",
    "vocab_size = 10000\n",
    "sequence_length = 100\n",
    "\n",
    "# Use the text vectorization layer to normalize, split, and map strings\n",
    "# to integer. Set maximum_sequence length as all samples are not of the\n",
    "# same length.\n",
    "vectorization_layer = tf.keras.layers.TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "defensive-mills",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a text-only dataset (no labels) and call adapt to build the vocab\n",
    "text_ds = train_ds.map(lambda x, y: x)\n",
    "vectorization_layer.adapt(text_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confirmed-reporter",
   "metadata": {},
   "source": [
    "### Create A Classification Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "radical-louisville",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 16\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(vectorization_layer)\n",
    "model.add(Embedding(vocab_size, embedding_dim, name=\"embedding\"))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "employed-contract",
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")\n",
    "model.compile(optimizer=\"RMSprop\",\n",
    "             loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "every-andrew",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - 4s 5ms/step - loss: 0.5458 - accuracy: 0.6615 - val_loss: 0.3969 - val_accuracy: 0.8120\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.3261 - accuracy: 0.8518 - val_loss: 0.3851 - val_accuracy: 0.8334\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.2325 - accuracy: 0.9032 - val_loss: 0.4256 - val_accuracy: 0.8128\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.1389 - accuracy: 0.9498 - val_loss: 0.4987 - val_accuracy: 0.8088\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.0625 - accuracy: 0.9808 - val_loss: 0.6188 - val_accuracy: 0.8024\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.0207 - accuracy: 0.9949 - val_loss: 0.7697 - val_accuracy: 0.7998\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.0051 - accuracy: 0.9987 - val_loss: 0.9285 - val_accuracy: 0.7962\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 1.0452 - val_accuracy: 0.7962\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 5.6480e-04 - accuracy: 0.9999 - val_loss: 1.1185 - val_accuracy: 0.7950\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 3.4834e-04 - accuracy: 0.9999 - val_loss: 1.1492 - val_accuracy: 0.7948\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x161736f70>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_ds,\n",
    "         validation_data=test_ds,\n",
    "         epochs=10,\n",
    "         callbacks=[tb_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "parental-phoenix",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization_2 (TextV  (None, 100)              0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 100, 16)           160000    \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 1600)              0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 16)                25616     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 185,633\n",
      "Trainable params: 185,633\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "large-evening",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-7b9b8af117d05cbc\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-7b9b8af117d05cbc\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#docs_infra: no_execute\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "historical-nepal",
   "metadata": {},
   "source": [
    "### Retrieve the trained word embedding and save them to disk\n",
    "\n",
    "我们可以把词典对就的 Embedding 下载保存的本地。保存后还可以上传到 Embedding Projector 上进行观察。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "casual-video",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.get_layer('embedding').get_weights()[0]\n",
    "vocab = vectorization_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "nominated-discount",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_v = io.open('vectors.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('metadata.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for index, word in enumerate(vocab):\n",
    "    if index == 0:\n",
    "        continue # skip 0, it's padding\n",
    "    vec = weights[index]\n",
    "    out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "    out_m.write(word + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "asian-panama",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_ds = tf.keras.utils.text_dataset_from_directory(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "short-mistress",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 3s 3ms/step - loss: 1.2282 - accuracy: 0.7752\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.2282118797302246, 0.7752000093460083]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_ds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
