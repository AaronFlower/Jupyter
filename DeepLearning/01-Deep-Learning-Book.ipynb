{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "\n",
    "不做研究的话，只看 PART 1 和 PART 2 就行了。PART 3 目前不用关心。\n",
    "\n",
    "## Part 1. 应用数学与机器学习基础\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 线性代数 \n",
    "\n",
    "### 2.5 范数\n",
    "\n",
    "**范数（Norm）**: 用来衡量向量大小。形式上，**$L^p$ 范数** 定义如下：\n",
    "\n",
    "$$\n",
    "\\| x \\|_p = \\Big(\\sum_{i}|x_i|^p\\Big)^{\\frac{1}{p}}\n",
    "$$\n",
    "\n",
    "基中, $p \\in \\mathbb{R}, p \\ge 1$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其实更一般的范数意义，不仅 $L^p$ 范数, 是将向量映射到非负值的函数。\n",
    "\n",
    "直观上，向量的 $\\mathbf{x}$ 的范数衡量从原点到点 $\\mathbf{x}$ 的距离。更严格地话，范数是满足下列性质的任意函数：\n",
    "\n",
    "1. $f(\\mathbf{x}) - 0 \\Rightarrow \\mathbf{0}$\n",
    "2. $f(\\mathbf{x} + \\mathbf{y}) \\le f(\\mathbf{x}) + f(\\mathbf{y})$ 三角不等式（triangle inequality）\n",
    "3. $\\forall \\alpha \\in \\mathbb{R}, f(\\alpha \\mathbf{x}) = |\\alpha|f(\\mathbf{x})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "下面讨论一下 $L^p$ 范数。\n",
    "\n",
    "#### 1. **$L^2$ 范数**与**平方$L^2$ 范数** \n",
    "\n",
    "当 $p = 2$ 时，$L^2$ 范数称之为欧几里得范数(Euclidean Norm), 确定的是欧几里得距离。 $L^2$ 范数经常简化为 $\\| x \\|$，略去下标 2.\n",
    "\n",
    "$L^2$ 范数在机器学习中出现的十分频繁。而其平方 -- **平方$L^2$ 范数**也经常用来衡量向量的大小，可以简单地通过点积计算 $\\mathbf{x}^T\\mathbf{x}$。**平方$L^2$ 范数**在使用时更方便。例如：**平方$L^2$ 范数** 对 $\\mathbf{x}$ 中每个元素的导数只取决于对应的元素，而 **$L^2$ 范数** 则和整个向量相关。\n",
    "\n",
    "#### 2. **$L^1 范数$**\n",
    "\n",
    "在某些情况下，**平方$L^2$ 范数** 可能并不受欢迎，因为它在原点附近增长的十分缓慢。在某些机器学习应用中，区分零元素和非零但很小的元素是十分重要的。在这种情况下，我们可以 $L^1$ 范数，其各个位置斜率相同，同时保持简单的数学形式。$L^1$ 范数可以简化如下：\n",
    "\n",
    "$$\n",
    "\\| x \\|_1 = \\sum_{i}|x_i|\n",
    "$$\n",
    "\n",
    "当机器学习问题中，零和非零元素之间的差异非常重要时，通常使用 $L^1$ 范数。每当 $\\mathbf{x}$ 中某个元素从 0 增加 $\\epsilon$ 时，对应的范数也增加 $\\epsilon$。而 **平方$L^2$ 范数** 在 0 附近增加就很小了。\n",
    "\n",
    "机器学习中也有很多 Truncated 算法用来获取近似为 0.\n",
    "\n",
    "#### 3. $L^0$ 范数\n",
    "\n",
    "有时候我们会统计向量中非零元素的个数来衡量向量的大小。有些作者称这种函数为『 $L^0$ 范数』，但是这个术语在数学意义上并不正确。因为不满足第三条性质。 因此，$L^1$ 范数经常作为表示非零元素数目的替代函数。\n",
    "\n",
    "#### 4. $L^{\\infty}$ 最大范数 (max norm)\n",
    "\n",
    "$L^{\\infty}$ 最大范数 (max norm) 表示向量中具有最大幅值的元素的组对值。\n",
    "\n",
    "$$\n",
    "\\| x \\|_{\\infty} = \\max_{i}|x_i|\n",
    "$$\n",
    "\n",
    "#### 5. Frobenius 范数\n",
    "\n",
    "在深度学习中用 Frobenius 范数来衡量矩阵的大小。\n",
    "\n",
    "$$\n",
    "\\| A \\|_F = \\sqrt{\\sum_{i,j} A_{i,j}^2}\n",
    "$$\n",
    "\n",
    "类似于向量的 $L^2$ 范数。\n",
    "\n",
    "\n",
    "两个向量的**点积**可以用范数来表示，即: $\\mathbf{x}^{T}\\mathbf{y} = \\| x \\|_2\\| y \\|_2\\cos{\\theta}$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 特征分解 （eigendecomposition)\n",
    "\n",
    "方阵 $A$ 的特征向量 （eigenvector) 是指与 $A$ 相乘后，相当于对该向量进行缩放的非零向量 $\\mathbf{v}$.\n",
    "\n",
    "$$\n",
    "A\\mathbf{v} = \\lambda \\mathbf{v}\n",
    "$$\n",
    "\n",
    "其中标量 $\\lambda$ 称为特征向量对应的特征值。\n",
    "\n",
    "### 2.8 奇异值分解 (Singular Value Decomposition)\n",
    "\n",
    "每一个实数矩阵都奇异值分解，但不一定有特征分解。非方阵就没有特征分解，只有奇异值分解。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 数值计算\n",
    "\n",
    "### 4.1 上溢和下溢\n",
    "\n",
    "Ref. P52 在实现基础库的时候需要处理这部分信息。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
