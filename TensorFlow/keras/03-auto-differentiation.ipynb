{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic differentiation and gradient tape, 自动微分\n",
    "\n",
    "Automatic Differentiation(AD) 自动微分。\n",
    "\n",
    "1. [Reverse-mode automatic differentiation: a tutorial](https://rufflewind.com/2016-12-30/reverse-mode-automatic-differentiation) 反向自动微分计算。\n",
    "2. [Step-by-step example of reverse-mode automatic differentiation](https://stats.stackexchange.com/questions/224140/step-by-step-example-of-reverse-mode-automatic-differentiation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们以下面的式子为例：\n",
    "\n",
    "$$\n",
    "z = xy + sin(x) \\tag{1}\n",
    "$$\n",
    "\n",
    "计算上式很简单，但是如果我想计算上式的微分该怎么计算那？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "手动计算呀！但是如果是一个计算机程序，那我们应该怎么计算那？这个时候就需要考虑用到 AD 了。\n",
    "\n",
    "AD 是基于以下实事的，即所有的程序操作都是基于一组基础的操作来实现的，如 +, *, 三角函数等。而再加上链式求导的法则，则可以让我们更好的利用这个性质。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward-Mode AD\n",
    "\n",
    "#### 表达式求值的程序计算\n",
    "\n",
    "> Program A\n",
    "\n",
    "```c\n",
    "x = ? \n",
    "y = ?\n",
    "a = x * y \n",
    "b = sin(x)\n",
    "z = a + b \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对 (A) 中所有表达式都对 $t$ 进行求导。则有:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial{x}}{\\partial{t}} = ? \\\\\n",
    "\\frac{\\partial{y}}{\\partial{t}} = ? \\\\\n",
    "\\frac{\\partial{a}}{\\partial{t}} = y \\frac{\\partial{x}}{\\partial{t}} + x  \\frac{\\partial{y}}{\\partial{t}}\\\\\n",
    "\\frac{\\partial{b}}{\\partial{t}} = cos(x)\\frac{\\partial{x}}{\\partial{t}} \\\\\n",
    "\\frac{\\partial{z}}{\\partial{t}} = \\frac{\\partial{a}}{\\partial{t}} + \\frac{\\partial{b}}{\\partial{t}}\n",
    "\\tag{F1}\n",
    "$$\n",
    "\n",
    "现在我们将 (F1) 写成计算机程序，用 ${dx, dy, \\dots}$ 分别代替 ${\\frac{\\partial{x}}{\\partial{t}}, \\frac{\\partial{y}}{\\partial{t}}, \\dots}$。 则有"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Program B\n",
    "\n",
    "```c\n",
    "dx = ?\n",
    "dy = ?\n",
    "da = ydx + xdy\n",
    "db = cos(x)dx\n",
    "dz = da + db\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在 (F1) 中，\n",
    "- 当我们用 $x = t$ 替换时就可以得到 $dx = 1, dy = 0$, 则最终我们就可以计算出： $\\frac{\\partial{z}}{\\partial{x}}$\n",
    "\n",
    "- 当我们用 $y = t$ 替换时就可以得到 $dx = 0, dy = 1$, 则最终我们就可以计算出： $\\frac{\\partial{z}}{\\partial{y}}$\n",
    "\n",
    "\n",
    "这样就计算出了程序 A 的导数了。\n",
    "\n",
    "在实现进，我们只需要实现一个翻译程序，能够将程序 A 根据相应的规则翻译 成程序 B 即可。这些规则可以是：\n",
    "\n",
    "```\n",
    "c = a + b     =>    dc = da + db\n",
    "c = a + b     =>    dc = da + db\n",
    "c = a * b     =>    dc = b * da + a * db\n",
    "c = sin(a)    =>    dc = cos(a) * da\n",
    "c = a - b     =>    dc = da - db\n",
    "c = a / b     =>    dc = da / b - a * db / b ** 2\n",
    "```\n",
    "等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在翻译时，我只需要根据对应的规则进行替换即可。并且程序的顺序并不会改变，即：如果 K 在 L 之前求得，那么在求导的时候，K 也是在 L 之前的。所以这种也被称为 Forward-mode automatic differentiation. 前向自动微分。\n",
    "\n",
    "FAD 的优点：\n",
    "\n",
    "- 中间变量可以省略，节省内存。\n",
    "- （Dual Number 二元数）有时间可以参考学习。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reverse-Mode Auto Diff\n",
    "\n",
    "FAD 有一个缺点，就是当输入变量有 n 个时需要计算 n 次，即算法复杂度为  O(n)。哪上例有 x, y 两个自变量那我们就需要遍历两次来计算。\n",
    "\n",
    "所以我们可用 RAD （逆序自动微分）。\n",
    "\n",
    "RAD 也是复用链式求导的规则，即：\n",
    "\n",
    "$$\n",
    "\\frac{dt}{dv}= \\sum{i}\\frac{dt}{du_i} \\frac{du_i}{dv}\n",
    "$$\n",
    "\n",
    "给定表达式，我们可生成一个计算图。假设我们的最终表达式值为 $z$, 其输入变量为 $w_i$, 从 $w_i$ 到 z 经过了 $w_p$ 节点，(即 $ z = g(w_p), w_p = f(w_i)$ ), 那我们可以求得输出变量对输入变量的导数为：\n",
    "\n",
    "$$\n",
    "\\frac{dz}{dw_i} = \\sum{p \\in parents(i)} \\frac{dz}{dw_p} \\frac{dw_p}{dw_i}\n",
    "$$\n",
    "\n",
    "这样，对于任意的中间节点，或者输入节点求导，我们只需要计算其父函数$w_p = f(w_i)$的基础求导公式即可。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们就通过下的例子来说明逆向求导的过程。\n",
    "\n",
    "假设我们表达式程序为：\n",
    "\n",
    "$$\n",
    "𝑤_1=𝑥_1\\\\\n",
    "𝑤_2=𝑥_2\\\\\n",
    "𝑤_3=𝑤_1𝑤_2\\\\\n",
    "𝑤_4=sin(𝑤_1)\\\\\n",
    "𝑤_5=𝑤_3+𝑤_4\\\\\n",
    "𝑧=𝑤_5\n",
    "$$\n",
    "\n",
    "假设 $x_1 = 2, x_2 = 3$, 则有：\n",
    "\n",
    "$$\n",
    "𝑤_1=𝑥_1=2\\\\\n",
    "𝑤_2=𝑥_2=3\\\\\n",
    "𝑤_3=𝑤_1𝑤_2=6\\\\\n",
    "𝑤_4=sin(𝑤_1) =0.9\\\\\n",
    "𝑤_5=𝑤_3+𝑤_4=6.9\\\\\n",
    "𝑧=𝑤_5=6.9\\\\\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "首先有，\n",
    "\n",
    "$$\n",
    "\\frac{dz}{dz} = 1\n",
    "$$\n",
    "\n",
    "又因为 $z = w_5$, 所以有 $\\frac{dz}{dw_5} = 1$。又因为 $\\frac{dw_5}{dw_3} = 1, \\frac{dw_5}{dw_4} = 1$。所以中间的节点的 $w_3, w_4$ 的导数为：\n",
    "\n",
    "$$\n",
    "\\frac{dz}{dw_3} = \\frac{dz}{dw_5}\\frac{dw_5}{dw_3} = 1 \\times 1 \\\\\n",
    "\\frac{dz}{dw_4} = \\frac{dz}{dw_5}\\frac{dw_5}{dw_4} = 1 \\times 1\n",
    "$$\n",
    "\n",
    "双因为 $w_3 = w_1 w_2$, 所以有 $\\frac{dw_3}{dw_2} = w_1$, 所有：\n",
    "\n",
    "$$\n",
    "\\frac{dz}{dw_2} = \\frac{dz}{dw_3}\\frac{dw_3}{dw_2} = 1 \\times w_1 = w_1 \n",
    "$$\n",
    "\n",
    "根据表达式前向计算时，可以知道 \n",
    "\n",
    "$$\n",
    "\\frac{dz}{dw_2} = w_1 = 2\n",
    "$$\n",
    "\n",
    "而对于输入变量 $w_1$, 其父函数有 $w_3, w_4$, 并且 $\\frac{dw_3}{dw_1} = w_2, \\frac{dw_4}{dw_1} = cos(w_1)$。所以们可以得到：\n",
    "\n",
    "$$\n",
    "\\frac{dz}{dw_1} = \\frac{dz}{dw_3}\\frac{dw_3}{dw_1} + \\frac{dz}{dw_4}\\frac{dw_4}{dw_1} = w_2 + cos(w_1) \n",
    "$$\n",
    "\n",
    "输入已知时，我们可以得到：\n",
    "\n",
    "$$\n",
    "\\frac{dz}{dw_1} = w_2 + cos(w_1) = 3 + cos(2) = 2.58\n",
    "$$\n",
    "\n",
    "所以最终我们得到了:\n",
    "\n",
    "$$\n",
    "\\frac{dz}{dx_1} = 2.58 \\\\\n",
    "\\frac{dz}{dx_2} = 2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAD 的过程就搞定了。\n",
    "\n",
    "在上描述中，我们公考虑了标量，但是其实也是可以应用到向量和矩阵的。\n",
    "\n",
    "[Step-by-step example of reverse-mode automatic differentiation](https://stats.stackexchange.com/questions/224140/step-by-step-example-of-reverse-mode-automatic-differentiation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RAD 的算法实现\n",
    "\n",
    "1. Naive 逆序树实现。\n",
    "2. Gradient Tape 算法。\n",
    "\n",
    "Naive 的方法是使用树的来实现，输入变量作为根结点。具体的参考 1."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
