{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic differentiation and gradient tape\n",
    "\n",
    "### Gradient tapes\n",
    "\n",
    "Tensorflow 使用的是逆序自动微分方法来自动求得梯度的。在实现使用的是 Gradient Tape 算法，而不是 Naive 的树。\n",
    "\n",
    "TF 的 `tf.GradientTape` API 提供梯度的自动计算。TF 会将所有操作记录到 `tf.GradientTape` 上，然后通过逆序自动微分方法来求得导数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(4.0, shape=(), dtype=float32)\n",
      "tf.Tensor(16.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.ones((2, 2))\n",
    "\n",
    "with tf.GradientTape() as t:\n",
    "    t.watch(x) # 输入变量作为根结点\n",
    "    y = tf.reduce_sum(x)\n",
    "    z = tf.multiply(y, y)\n",
    "    print(y)\n",
    "    print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(8.0, shape=(), dtype=float32)\n",
      "tf.Tensor(8.0, shape=(), dtype=float32)\n",
      "tf.Tensor(8.0, shape=(), dtype=float32)\n",
      "tf.Tensor(8.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "dz_dx = t.gradient(z, x)\n",
    "\n",
    "for i in [0, 1]:\n",
    "    for j in [0, 1]:\n",
    "        print(dz_dx[i][j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `tf.GradientTape`  类\n",
    "\n",
    "`tf.GradientTape` 用于记录操作实现自动微分。\n",
    "\n",
    "在实现自动微分时，输入变量是被当作根节点的，我们需要对输入变量进行 `watch` 创建一个上下文环境。\n",
    "\n",
    "当我们使用 `tf.Varaiable` 创建一个变量时，如果指定了第二个参数  `trainable=True` 那么该变量会自动被 watched.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(6.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(3.0)\n",
    "\n",
    "with tf.GradientTape() as t:\n",
    "    t.watch(x)\n",
    "    y = x * x\n",
    "\n",
    "dy_dx = t.gradient(y, x)\n",
    "\n",
    "# y = x^2, dy_dx = 2x\n",
    "print(dy_dx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 高阶积分\n",
    "\n",
    "通过对 GradientTapes 的嵌套，我们可以计算高阶的导数。如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(6.0, shape=(), dtype=float32)\n",
      "tf.Tensor(2.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant(3.0)\n",
    "with tf.GradientTape() as g:\n",
    "    g.watch(x)\n",
    "    with tf.GradientTape() as gg:\n",
    "        gg.watch(x)\n",
    "        y = x * x\n",
    "\n",
    "    dy_dx = gg.gradient(y, x) \n",
    "dy_dx2 = g.gradient(dy_dx, x)\n",
    "\n",
    "# y = x^2, y' = 2x, y'' = 2\n",
    "\n",
    "print(dy_dx)\n",
    "print(dy_dx2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "默认情况下，当调用了 `gradient()` 方法后，GradientType 对象就会被释放，为了多次计算，我们可以对该对象进行持久化，当计算完成后，删除该对象即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(108.0, shape=(), dtype=float32)\n",
      "tf.Tensor(6.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant(3.0)\n",
    "with tf.GradientTape(persistent=True) as g:\n",
    "    g.watch(x)\n",
    "    y = x * x\n",
    "    z = y * y\n",
    "dz_dx = g.gradient(z, x)\n",
    "dy_dx = g.gradient(y, x)\n",
    "del g\n",
    "\n",
    "# z = x^4, z' = 4*x^3\n",
    "print(dz_dx)\n",
    "# y = x^2, y' = 2*x\n",
    "print(dy_dx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在再用一个 $z = xy + sin(x)$ 的例子来说明下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(4.0205746, shape=(), dtype=float32)\n",
      "tf.Tensor(0.5, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(0.5)\n",
    "y = tf.Variable(4.5)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as t:\n",
    "    t.watch(x)\n",
    "    z = x * y + tf.math.cos(x) #  为什么要用 tf.math.cos, 因为 tf 的操作才会记录操作。\n",
    "\n",
    "dz_dx = t.gradient(z, x)\n",
    "dz_dy = t.gradient(z, y)\n",
    "print(dz_dx)\n",
    "print(dz_dy)\n",
    "\n",
    "del t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当在参与的运算中有一个变量被 watch， 那么参与运算的所有变量都会被 watch。我们可以通过 `watch_accessed_variables=False` 来取消这个配置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.Variable(3.0)\n",
    "b = tf.Variable(4.0)\n",
    "with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "  tape.watch(a)\n",
    "  y = a ** 2  # Gradients will be available for `variable_a`.\n",
    "  z = b ** 3  # No gradients will be available since `variable_b` is\n",
    "                       # not being watched."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注意**，在模型中对层的参数进行 watch 的时候，要保证层已经调用了 build。否则了进行 watch 时并且应用了  `watch_accessed_variables=False`, 那很可能得不到任何梯度。所以如果不是性能问题，一般不要置成 `False`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, None]\n"
     ]
    }
   ],
   "source": [
    "inputs = tf.ones((32,1))\n",
    "a = tf.keras.layers.Dense(32, input_shape=(32,1))\n",
    "b = tf.keras.layers.Dense(32)\n",
    "\n",
    "with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "  tape.watch(a.variables)  # Since `a.build` has not been called at this point\n",
    "                           # `a.variables` will return an empty list and the\n",
    "                           # tape will not be watching anything.\n",
    "  result = b(a(inputs))\n",
    "  g = tape.gradient(result, a.variables)  # The result of this computation will be\n",
    "                                      # a list of `None`s since a's variables\n",
    "                                      # are not being watched.\n",
    "print(g)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
