{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphs and sessions\n",
    "\n",
    "TF 1.14 的图和会话还是要熟悉，不然历史遗留代码看不懂呀。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow 使用**数据流图**将计算表示为独立的指令之间的依赖关系。在这种编程模型中，你首先需要先定义数据流图，然后创建 TensorFlow 会话，然后就可以本地和远程设备上运行图中的各个部分了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 为什么使用数据流图？\n",
    "\n",
    "<img src=\"https://www.tensorflow.org/images/tensors_flowing.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[数据流(Dataflow Programming)](https://en.wikipedia.org/wiki/Dataflow_programming) 是一种用于并行计算的常用编程模型。在数据流图中：\n",
    "\n",
    "- 节点，表示计算单元\n",
    "- 边，表示计算使用或产生的数据\n",
    "\n",
    "如：在 TF 图中， `tf.matmul` 操作对应于单个节点，该节点有两个输入边（要相乘的矩阵）和一个输出边（乘法结果）\n",
    "\n",
    "在执行程序时，数据流图可以为 TF 提供很多优势：\n",
    "\n",
    "- **并行处理**，通过使用明确的边来表示操作之间的依赖关系，系统可以轻松识别可以并行执行的操作。\n",
    "- **分布式执行**, 通过使用明确的边来表示操作之间流动的值， TF 可以将程序划分到连接至不同机器的多台设备上 （CPU, GPU ，TPU)。TF 将在这些设备之间上进行必要的通信和协调。\n",
    "- **编译优化**，ensorFlow 的 XLA 编译器可以使用数据流图中的信息生成更快的代码，例如将相邻的操作融合到一起。\n",
    "- **可移植性**, 数据流图是一种不依赖于语言的模型代码表示法。您可以使用 Python 构建数据流图，将其存储在 SavedModel 中，并使用 C++ 程序进行恢复，从而实现低延迟的推理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建 `tf.Graph`\n",
    "\n",
    "大多数 TF 程序都以数据流图构建阶段开始。通过调用 TF API 函数，将计算节点 (tf.Operation) 和 边(tf.Tensor) 添加到 `.tf.Graph` 中，TF 提供了一个**默认图**。\n",
    "\n",
    "如：\n",
    "\n",
    "- 调用 `tf.constant(42)` 创建单个 `tf.Operation`，该操作可以生成值 42.0，将该值添加到默认图中，并返回表示常量值的 tf.Tensor。\n",
    "\n",
    "- 调用 `tf.matmul(x, y)` 可创建单个 `tf.Operation`，该操作会将 tf.Tensor 对象 x 和 y 的值相乘，将其添加到默认图中，并返回表示乘法运算结果的 tf.Tensor。\n",
    "\n",
    "- 执行 `v = tf.Variable(0)` 可以向图添加一个 `tf.Operation`, 该操作可以存储一个可写入的张量值，该值在多个 `tf.Session.run` 调用之间保持恒定。\n",
    "\n",
    "- 调用 `tf.train.Optimizer.minimize()` 可以将操作和张量添加到计算梯度的默认图中，并返回一个 `tf.Operation` 节点，该操作在运行时会将这些梯度应用到一组变量上。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 命名指令\n",
    "\n",
    "`tf.Graph` 对象会定义一个命名空间。TF 自动为图中每个指令选择一个唯一名称作为标识，使您的程序阅读和调试起来更加轻松。TF API 提供两种方法来定义操作名称：\n",
    "\n",
    "- 如果 API 函数是创建新的 `tf.Operation` 或返回新的 `tf.Tensor` ，则可以通过 name 参数来命名。如果出现重复，TF 会自动在后面添加上数字序号。\n",
    "\n",
    "- 通过 `tf.name_scope` 可以添加名称作用域前缀。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面的代码，用于说明命名的规则。**注意**，因为 TF 为我们提供一个默认图，所有的节点和边都会添加这个图中，所以执行多次，会添加多次。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"c:0\", shape=(), dtype=int32)\n",
      "Tensor(\"c_1:0\", shape=(), dtype=int32)\n",
      "Tensor(\"outer/c:0\", shape=(), dtype=int32)\n",
      "Tensor(\"outer/inner/c:0\", shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# 自定义名称\n",
    "c1 = tf.constant(0, name=\"c\")\n",
    "c2 = tf.constant(0, name=\"c\")\n",
    "print(c1)\n",
    "print(c2)\n",
    "\n",
    "with tf.name_scope(\"outer\"):\n",
    "    c3 = tf.constant(2, name=\"c\")\n",
    "    with tf.name_scope(\"inner\"):\n",
    "        c4 = tf.constant(2, name=\"c\")\n",
    "print(c3)\n",
    "print(c4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过对指令进行分组后，通过 TensorBoard 可以更好的调试分析。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0000000e+00 9.0960656e-11]\n",
      " [1.3551073e-01 8.6448926e-01]]\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant([[37.0, -23.0], [1.0, 4.0]]) # node\n",
    "w = tf.Variable(tf.random_uniform([2, 2])) # node\n",
    "y = tf.matmul(x, w) # ndoe\n",
    "output = tf.nn.softmax(y) # node\n",
    "init_op = w.initializer # node\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Run fetch node\n",
    "    sess.run(init_op)\n",
    "    print(sess.run(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将操作放置到不同的设备上\n",
    "\n",
    "我们还可以使用 `tf.device()` 函数，将创建的操作放到指定的设备上执行。设备规范如下：\n",
    " \n",
    "``` \n",
    "/job:<JOB_NAME>/task:<TASK_INDEX>/device:<DEVICE_TYPE>:<DEVICE_INDEX>\n",
    "```\n",
    "\n",
    "如指定 CPU, GPU 等。电脑不给力，没办法举例。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用多个图进行编程\n",
    "\n",
    "TensorFlow 提供了一个“默认图”，对于许多应用而言，单个图已足够。但是一些高级应用，或者我们演示可能需要创建多个图。\n",
    "\n",
    "可以调用 `tf.Graph()` 来创建一个图。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 展示我们的计算图"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 例子 a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.1\n"
     ]
    }
   ],
   "source": [
    "g1 = tf.Graph()\n",
    "with g1.as_default():\n",
    "    a = tf.constant(3.1, name=\"a\")\n",
    "    b = tf.placeholder(tf.float32, name=\"b\")\n",
    "    # 使用函数而不是重载的 `+` 是因为我们可以指定 `name`\n",
    "    s = tf.add(a, b, name=\"sum\") \n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        writer = tf.summary.FileWriter(\"./logs/sum\", sess.graph)\n",
    "        print(sess.run(s, feed_dict={b: 4}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们的可视图都生成在 `logs` 目录下，可以通完 `tensorboard --logdir ./logs` 来查看。\n",
    "\n",
    "<img src=\"./sum.png\" width=\"320px\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.99999285e-01 6.71411215e-07]\n",
      " [8.91366303e-01 1.08633675e-01]]\n"
     ]
    }
   ],
   "source": [
    "g2 = tf.Graph()\n",
    "with g2.as_default():   \n",
    "    x = tf.constant([[37.0, -23.0], [1.0, 4.0]], name=\"x\") # node\n",
    "    w = tf.Variable(tf.random_uniform([2, 2]), name=\"w\") # node\n",
    "    y = tf.matmul(x, w, name=\"inner_product\") # ndoe\n",
    "    output = tf.nn.softmax(y) # node\n",
    "    init_op = w.initializer # node\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        writer = tf.summary.FileWriter(\"./logs/ex1\", sess.graph)\n",
    "        # Run fetch node\n",
    "        sess.run(init_op)\n",
    "        print(sess.run(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后在控制到中通过 `tensorboard --logdir ./log/ex1` 就可查看我们的计算图了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.3\n"
     ]
    }
   ],
   "source": [
    "g3 = tf.Graph()\n",
    "with g3.as_default():    \n",
    "    a = tf.placeholder(tf.float32)\n",
    "    b = tf.placeholder(tf.float32)\n",
    "    z = a + b\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        writer = tf.summary.FileWriter(\"./logs/ex3\", sess.graph)\n",
    "        print(sess.run(z, feed_dict={a: 3.1, b: 2.2}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
