{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Word Embedding \n",
    "\n",
    "TLDR;\n",
    "\n",
    "给定句子，每个句子的长短不一，根据 Vocabulary 做 One-hot; 缺点是特征过多。而且单词之间没有关系可言，kitty 和 cat 不分。\n",
    "\n",
    "我人要理解单词之间的意义和关系，根据 contextual information 来学习。有两种方法：\n",
    "\n",
    "1. count-based, 基于词频，非监督学习。\n",
    "2. contexted-based, 基于上下文信息，监督学习。\n",
    "\n",
    "### Context-based: Skip-Gram model\n",
    "\n",
    "- target: 是输入，也是中间词，根据该词按 window size 大小组合成新样本。\n",
    "- output: 输出，与该目录关系最密切的单词的概率。\n",
    "\n",
    "### 损失函数\n",
    "\n",
    "最重要的是损失函数怎么定义。\n",
    "\n",
    "#### References\n",
    "\n",
    "1. [Learning Word Embedding](https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html#count-based-vector-space-model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
