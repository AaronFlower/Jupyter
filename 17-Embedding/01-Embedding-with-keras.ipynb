{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding with keras\n",
    "\n",
    "1. [Neural Network Embeddings Explained](https://towardsdatascience.com/neural-network-embeddings-explained-4d028e6f0526)\n",
    "2. [How to Use Word Embedding Layers for Deep Learning with Keras\n",
    "](https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/?source=post_page---------------------------)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Flatten, Dense\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 创建文档和标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define documents\n",
    "docs = ['Well done!',\n",
    "\t\t'Good work',\n",
    "\t\t'Great effort',\n",
    "\t\t'nice work',\n",
    "\t\t'Excellent!',\n",
    "\t\t'Weak',\n",
    "\t\t'Poor effort!',\n",
    "\t\t'not good',\n",
    "\t\t'poor work',\n",
    "\t\t'Could have done better.']\n",
    "# define class labels\n",
    "labels = np.array([1,1,1,1,1,0,0,0,0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 进行 one-hot 独热编码\n",
    "\n",
    "我们假设 Vocabulary 的大小为 50, 这远比我们给的文件档的单词要多。但是这样可以降低 hash 冲突的可能性。\n",
    "\n",
    "one-hot hash 冲突？\n",
    "\n",
    "Keras 的 `one_hot` 函数内部使用是 `hashing_trick` 函数对 text 进行 hash。hash 可能会有冲突，并不保证惟一。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 8, 2]\n",
      "[4, 2, 3]\n",
      "[2, 2, 1]\n"
     ]
    }
   ],
   "source": [
    "print(keras.preprocessing.text.one_hot('a c aa', 10))\n",
    "print(keras.preprocessing.text.one_hot('a c aa', 5)) # 冲突\n",
    "print(keras.preprocessing.text.one_hot('a c aa', 3)) # 完全冲突"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[38, 15], [23, 40], [8, 7], [36, 40], [5], [34], [23, 7], [36, 23], [23, 40], [43, 32, 15, 42]]\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 50\n",
    "encoded_docs = [keras.preprocessing.text.one_hot(d, vocab_size) for d in docs]\n",
    "print(encoded_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这样我们每个文档中的单词都对应的进行 code 了。但是文档中的长度并不一样，\n",
    "我们可以使用 `pad_sequnces()` 函数来进行对齐。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[38 15  0  0]\n",
      " [23 40  0  0]\n",
      " [ 8  7  0  0]\n",
      " [36 40  0  0]\n",
      " [ 5  0  0  0]\n",
      " [34  0  0  0]\n",
      " [23  7  0  0]\n",
      " [36 23  0  0]\n",
      " [23 40  0  0]\n",
      " [43 32 15 42]]\n"
     ]
    }
   ],
   "source": [
    "# pad_documents to a max length of 4 words\n",
    "max_length = 4\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "print(padded_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在就可以使用 Embedding layer 了。我们需要将一个大小为 50 的 Vocabulary, 每个 input 长度为 4 的样本，嵌入到一个 8 维的 Embedding 空间内。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 4, 8)              400       \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 433\n",
      "Trainable params: 433\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 8, input_length=max_length))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy',\n",
    "             metrics=['acc'])\n",
    "\n",
    "# summarize the model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后进行，模型拟合和评测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 89.999998\n"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "model.fit(padded_docs, labels, epochs=50, verbose=0)\n",
    "\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考资料给出的准确率是 100%, 但我的为什么只有 89% 那？\n",
    "\n",
    "现在可以将我们从 Embedding layer 学习到参数保存下来。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用预先生成的 Embedding Space \n",
    "\n",
    "在 Keras 的 Embedding Layer 中我们可以使用从其它模型中学习到的 Embedding 空间。\n",
    "\n",
    "在 NLP 处理，使用已有的 embedding 是很常见的。\n",
    "\n",
    "例如，我们可以使用 [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/projects/glove/) 的 Embedding."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
