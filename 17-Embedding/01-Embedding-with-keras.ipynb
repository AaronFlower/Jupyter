{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding with keras\n",
    "\n",
    "1. [Neural Network Embeddings Explained](https://towardsdatascience.com/neural-network-embeddings-explained-4d028e6f0526)\n",
    "2. [How to Use Word Embedding Layers for Deep Learning with Keras\n",
    "](https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/?source=post_page---------------------------)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Flatten, Dense\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 创建文档和标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define documents\n",
    "docs = ['Well done!',\n",
    "\t\t'Good work',\n",
    "\t\t'Great effort',\n",
    "\t\t'nice work',\n",
    "\t\t'Excellent!',\n",
    "\t\t'Weak',\n",
    "\t\t'Poor effort!',\n",
    "\t\t'not good',\n",
    "\t\t'poor work',\n",
    "\t\t'Could have done better.']\n",
    "# define class labels\n",
    "labels = np.array([1,1,1,1,1,0,0,0,0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 进行 one-hot 独热编码\n",
    "\n",
    "我们假设 Vocabulary 的大小为 50, 这远比我们给的文件档的单词要多。但是这样可以降低 hash 冲突的可能性。\n",
    "\n",
    "one-hot hash 冲突？\n",
    "\n",
    "Keras 的 `one_hot` 函数内部使用是 `hashing_trick` 函数对 text 进行 hash。hash 可能会有冲突，并不保证惟一。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 5]\n",
      "[2, 4, 2]\n",
      "[2, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "print(one_hot('a c aa', 8))\n",
    "print(one_hot('a c aa', 5)) # 冲突\n",
    "print(one_hot('a c aa', 3)) # 完全冲突"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[45, 38], [35, 49], [6, 11], [24, 49], [33], [8], [19, 11], [32, 35], [19, 49], [26, 32, 38, 22]]\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 50\n",
    "encoded_docs = [one_hot(d, vocab_size) for d in docs]\n",
    "print(encoded_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这样我们每个文档中的单词都对应的进行 code 了。但是文档中的长度并不一样，\n",
    "我们可以使用 `pad_sequnces()` 函数来进行对齐。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[45 38  0  0]\n",
      " [35 49  0  0]\n",
      " [ 6 11  0  0]\n",
      " [24 49  0  0]\n",
      " [33  0  0  0]\n",
      " [ 8  0  0  0]\n",
      " [19 11  0  0]\n",
      " [32 35  0  0]\n",
      " [19 49  0  0]\n",
      " [26 32 38 22]]\n"
     ]
    }
   ],
   "source": [
    "# pad_documents to a max length of 4 words\n",
    "max_length = 4\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length,\n",
    "                        padding='post')\n",
    "print(padded_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在就可以使用 Embedding layer 了。我们需要将一个大小为 50 的 Vocabulary, 每个 input 长度为 4 的样本，嵌入到一个 8 维的 Embedding 空间内。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 4, 8)              400       \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 433\n",
      "Trainable params: 433\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 8, input_length=max_length))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy',\n",
    "             metrics=['acc'])\n",
    "\n",
    "# summarize the model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后进行，模型拟合和评测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 89.999998\n"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "model.fit(padded_docs, labels, epochs=50, verbose=0)\n",
    "\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考资料给出的准确率是 100%, 但我的为什么只有 89% 那？\n",
    "\n",
    "现在可以将我们从 Embedding layer 学习到参数保存下来。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用预先生成的 Embedding Space \n",
    "\n",
    "在 Keras 的 Embedding Layer 中我们可以使用从其它模型中学习到的 Embedding 空间。\n",
    "\n",
    "在 NLP 处理，使用已有的 embedding 是很常见的。\n",
    "\n",
    "例如，我们可以使用 [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/projects/glove/) 的 Embedding."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
