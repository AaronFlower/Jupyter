{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec\n",
    "\n",
    "[ref](https://github.com/headwinds/python-notebooks/blob/master/nlp/tensorflow-word2vec.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "tf.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['he', 'is', 'the', 'king', 'the', 'king', 'is', 'royal', 'she', 'is', 'the', 'royal', 'queen']\n",
      "{'queen', 'king', 'is', 'he', 'she', 'royal', 'the'}\n"
     ]
    }
   ],
   "source": [
    "corpus_raw = 'He is the king . The king is royal . She is the royal  queen '\n",
    "\n",
    "# convert to lower case\n",
    "corpus_raw = corpus_raw.lower()\n",
    "\n",
    "words = []\n",
    "for word in corpus_raw.split():\n",
    "    if word != '.': # because we don't want to treat . as a word\n",
    "        words.append(word)\n",
    "\n",
    "print(words)\n",
    "words = set(words) # so that all duplicate words are removed\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'queen': 0, 'king': 1, 'is': 2, 'he': 3, 'she': 4, 'royal': 5, 'the': 6}\n",
      "\n",
      "{0: 'queen', 1: 'king', 2: 'is', 3: 'he', 4: 'she', 5: 'royal', 6: 'the'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['he', 'is', 'the', 'king'],\n",
       " ['the', 'king', 'is', 'royal'],\n",
       " ['she', 'is', 'the', 'royal', 'queen']]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2int = {}\n",
    "int2word = {}\n",
    "vocab_size = len(words) # gives the total number of unique words\n",
    "\n",
    "for i,word in enumerate(words):\n",
    "    word2int[word] = i\n",
    "    int2word[i] = word\n",
    "\n",
    "# raw sentences is a list of sentences.\n",
    "raw_sentences = corpus_raw.split('.')\n",
    "sentences = []\n",
    "for sentence in raw_sentences:\n",
    "    sentences.append(sentence.split())\n",
    "\n",
    "print(word2int)\n",
    "print()\n",
    "print(int2word)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['he', 'is'],\n",
       " ['he', 'the'],\n",
       " ['is', 'he'],\n",
       " ['is', 'the'],\n",
       " ['is', 'king'],\n",
       " ['the', 'he'],\n",
       " ['the', 'is'],\n",
       " ['the', 'king'],\n",
       " ['king', 'is'],\n",
       " ['king', 'the'],\n",
       " ['the', 'king'],\n",
       " ['the', 'is'],\n",
       " ['king', 'the'],\n",
       " ['king', 'is'],\n",
       " ['king', 'royal'],\n",
       " ['is', 'the'],\n",
       " ['is', 'king'],\n",
       " ['is', 'royal'],\n",
       " ['royal', 'king'],\n",
       " ['royal', 'is'],\n",
       " ['she', 'is'],\n",
       " ['she', 'the'],\n",
       " ['is', 'she'],\n",
       " ['is', 'the'],\n",
       " ['is', 'royal'],\n",
       " ['the', 'she'],\n",
       " ['the', 'is'],\n",
       " ['the', 'royal'],\n",
       " ['the', 'queen'],\n",
       " ['royal', 'is'],\n",
       " ['royal', 'the'],\n",
       " ['royal', 'queen'],\n",
       " ['queen', 'the'],\n",
       " ['queen', 'royal']]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WINDOW_SIZE = 2\n",
    "\n",
    "data = []\n",
    "for sentence in sentences:\n",
    "    for word_index, word in enumerate(sentence):\n",
    "        for nb_word in sentence[max(word_index - WINDOW_SIZE, 0) : min(word_index + WINDOW_SIZE, len(sentence)) + 1] : \n",
    "            if nb_word != word:\n",
    "                data.append([word, nb_word])\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "[[0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# function to convert numbers to one hot vectors\n",
    "def to_one_hot(data_point_index, vocab_size):\n",
    "    temp = np.zeros(vocab_size)\n",
    "    temp[data_point_index] = 1\n",
    "    return temp\n",
    "\n",
    "x_train = [] # input word\n",
    "y_train = [] # output word\n",
    "\n",
    "for data_word in data:\n",
    "    x_train.append(to_one_hot(word2int[ data_word[0] ], vocab_size))\n",
    "    y_train.append(to_one_hot(word2int[ data_word[1] ], vocab_size))\n",
    "\n",
    "# convert them to numpy arrays\n",
    "x_train = np.asarray(x_train)\n",
    "y_train = np.asarray(y_train)\n",
    "\n",
    "print(x_train)\n",
    "print()\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 5\n",
    "\n",
    "def model():\n",
    "    x = tf.placeholder(tf.float32, shape=(None, vocab_size))\n",
    "    y_label = tf.placeholder(tf.float32, shape=(None, vocab_size))\n",
    "    W1 = tf.Variable(tf.random_normal([vocab_size, EMBEDDING_DIM]))\n",
    "    b1 = tf.Variable(tf.random_normal([EMBEDDING_DIM]))\n",
    "    \n",
    "    hidden_representation = tf.add(tf.matmul(x, W1), b1)\n",
    "    \n",
    "    W2 = tf.Variable(tf.random_normal([EMBEDDING_DIM, vocab_size]))\n",
    "    b2 = tf.Variable(tf.random_normal([vocab_size]))\n",
    "    \n",
    "    z = tf.add(tf.matmul(hidden_representation, W2), b2)\n",
    "    \n",
    "    predict = tf.nn.softmax(z)\n",
    "    \n",
    "    cross_entropy_loss = tf.reduce_mean(\n",
    "        -tf.reduce_sum(y_label * tf.log(predict), reduction_indices=[1]))\n",
    "    return x, y_label, predict, cross_entropy_loss, W1, b1\n",
    "                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss is 5.232\n",
      "Epoch 500 loss is 1.377\n",
      "Epoch 1000 loss is 1.335\n",
      "Epoch 1500 loss is 1.327\n",
      "Epoch 2000 loss is 1.325\n",
      "Epoch 2500 loss is 1.324\n",
      "Epoch 3000 loss is 1.323\n",
      "Epoch 3500 loss is 1.322\n",
      "Epoch 4000 loss is 1.322\n",
      "Epoch 4500 loss is 1.322\n",
      "Epoch 5000 loss is 1.322\n",
      "Epoch 5500 loss is 1.321\n",
      "Epoch 6000 loss is 1.321\n",
      "Epoch 6500 loss is 1.321\n",
      "Epoch 7000 loss is 1.321\n",
      "Epoch 7500 loss is 1.321\n",
      "Epoch 8000 loss is 1.321\n",
      "Epoch 8500 loss is 1.321\n",
      "Epoch 9000 loss is 1.321\n",
      "Epoch 9500 loss is 1.321\n",
      "[[ 1.8528585   1.6174512   1.258438    0.28817225 -1.0668479 ]\n",
      " [-0.5379993   0.90102583  1.1329073  -0.44519413  0.7587993 ]\n",
      " [-0.54823804  1.3765346   0.531423    3.0392737  -0.06460135]\n",
      " [ 1.5491688  -1.0920565   1.992212   -0.3576851  -0.02183155]\n",
      " [-0.779871   -0.19996865  2.5206764  -0.5892549   0.57225513]\n",
      " [-2.298608   -2.3394256  -0.8665231   0.05630873  0.61125386]\n",
      " [ 1.3928018  -0.57235616 -1.1741842   0.7932451   2.5386894 ]]\n",
      "[ 1.1257846  -1.0519121   1.7099932   0.65605074 -0.13203743]\n"
     ]
    }
   ],
   "source": [
    "n_iters = 10000\n",
    "with tf.Session() as sess:\n",
    "    x, y_label, predict, loss, W1, b1 = model()\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss)\n",
    "    \n",
    "    feed_dict = {\n",
    "        x: x_train,\n",
    "        y_label: y_train\n",
    "    }\n",
    "    sess.run(init)\n",
    "    for i in range(n_iters):\n",
    "        _, loss_val = sess.run([train_step, loss], feed_dict)\n",
    "        if (i % 500 == 0):\n",
    "            print('Epoch %d loss is %4.3f' % (i, loss_val))\n",
    "    \n",
    "    print(sess.run(W1))\n",
    "    print(sess.run(b1))\n",
    "    vectors = sess.run(W1 + b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# making placeholders for x_train and y_train\n",
    "x = tf.placeholder(tf.float32, shape=(None, vocab_size))\n",
    "y_label = tf.placeholder(tf.float32, shape=(None, vocab_size))\n",
    "\n",
    "EMBEDDING_DIM = 5 # you can choose your own number\n",
    "W1 = tf.Variable(tf.random_normal([vocab_size, EMBEDDING_DIM]))\n",
    "b1 = tf.Variable(tf.random_normal([EMBEDDING_DIM])) #bias\n",
    "hidden_representation = tf.add(tf.matmul(x,W1), b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([EMBEDDING_DIM, vocab_size]))\n",
    "b2 = tf.Variable(tf.random_normal([vocab_size]))\n",
    "prediction = tf.nn.softmax(tf.add( tf.matmul(hidden_representation, W2), b2))\n",
    "\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init) #make sure you do this!\n",
    "\n",
    "# define the loss function:\n",
    "cross_entropy_loss = tf.reduce_mean(-tf.reduce_sum(y_label * tf.log(prediction), reduction_indices=[1]))\n",
    "\n",
    "# define the training step:\n",
    "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(cross_entropy_loss)\n",
    "\n",
    "n_iters = 10000\n",
    "# train for n_iter iterations\n",
    "\n",
    "for i in range(n_iters):\n",
    "    sess.run(train_step, feed_dict={x: x_train, y_label: y_train})\n",
    "    if (i % 500 == 0):\n",
    "        print('loss is : ', sess.run(cross_entropy_loss, feed_dict={x: x_train, y_label: y_train}))\n",
    "\n",
    "vectors = sess.run(W1 + b1)\n",
    "\n",
    "def euclidean_dist(vec1, vec2):\n",
    "    return np.sqrt(np.sum((vec1-vec2)**2))\n",
    "\n",
    "def find_closest(word_index, vectors):\n",
    "    min_dist = 10000 # to act like positive infinity\n",
    "    min_index = -1\n",
    "    query_vector = vectors[word_index]\n",
    "    for index, vector in enumerate(vectors):\n",
    "        if euclidean_dist(vector, query_vector) < min_dist and not np.array_equal(vector, query_vector):\n",
    "            min_dist = euclidean_dist(vector, query_vector)\n",
    "            min_index = index\n",
    "    return min_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.9786432  0.5655391  2.9684312  0.944223  -1.1988853]\n"
     ]
    }
   ],
   "source": [
    "# 看下 'queen' 单词的表示\n",
    "print(vectors[word2int['queen']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.9786432   0.5655391   2.9684312   0.944223   -1.1988853 ]\n",
      " [ 0.58778536 -0.15088624  2.8429005   0.21085662  0.6267619 ]\n",
      " [ 0.5775466   0.3246225   2.2414162   3.6953244  -0.19663878]\n",
      " [ 2.6749535  -2.1439686   3.7022052   0.29836565 -0.15386899]\n",
      " [ 0.34591365 -1.2518808   4.2306695   0.06679583  0.4402177 ]\n",
      " [-1.1728234  -3.3913376   0.84347016  0.7123595   0.47921643]\n",
      " [ 2.5185864  -1.6242683   0.53580904  1.4492958   2.406652  ]]\n",
      "[[ 0.20083235 -0.9796256 ]\n",
      " [-0.9999582  -0.00913954]\n",
      " [ 0.7678239  -0.640661  ]\n",
      " [-0.4945566  -0.8691454 ]\n",
      " [ 0.5028128  -0.86439526]\n",
      " [ 0.99960124  0.02823829]\n",
      " [ 0.4537208   0.89114386]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "print(vectors)\n",
    "\n",
    "model = TSNE(n_components=2, random_state=0)\n",
    "np.set_printoptions(suppress=True)\n",
    "vectors = model.fit_transform(vectors) \n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "normalizer = preprocessing.Normalizer()\n",
    "vectors =  normalizer.fit_transform(vectors, 'l2')\n",
    "\n",
    "print(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'queen', 'king', 'is', 'he', 'she', 'royal', 'the'}\n",
      "queen -0.9796256\n",
      "king -0.00913954\n",
      "is -0.640661\n",
      "he -0.8691454\n",
      "she -0.86439526\n",
      "royal 0.028238285\n",
      "the 0.89114386\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "print(words)\n",
    "for word in words:\n",
    "    print(word, vectors[word2int[word]][1])\n",
    "    ax.annotate(word, (vectors[word2int[word]][0],vectors[word2int[word]][1] ))\n",
    "    ax.set_xlim(min([vectors[word2int[w]][0] for w in words])-1, max([vectors[word2int[w]][0] for w in words])+1)\n",
    "    ax.set_ylim(min([vectors[word2int[w]][1] for w in words])-1, max([vectors[word2int[w]][1] for w in words])+1)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
