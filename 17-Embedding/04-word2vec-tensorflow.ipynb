{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn Word2Vec by implementing it in TF\n",
    "\n",
    "Best way to understand an algorithm is to implement it.\n",
    "\n",
    "[Best way to understand an algorithm is to implement it](https://towardsdatascience.com/learn-word2vec-by-implementing-it-in-tensorflow-45641adaf2ac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "tf.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['he', 'is', 'the', 'king', 'the', 'king', 'is', 'royal', 'she', 'is', 'the', 'royal', 'queen']\n",
      "{'queen', 'royal', 'he', 'the', 'she', 'is', 'king'}\n"
     ]
    }
   ],
   "source": [
    "corpus_raw = 'He is the king . The king is royal . She is the royal  queen '\n",
    "corpus_raw = corpus_raw.lower()\n",
    "\n",
    "words = []\n",
    "for word in corpus_raw.split():\n",
    "    if word != '.':\n",
    "        words.append(word)\n",
    "print(words)\n",
    "words = set(words)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['queen', 'king', 'is', 'he', 'she', 'royal', 'the']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = ['queen', 'king', 'is', 'he', 'she', 'royal', 'the']\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'queen': 0, 'king': 1, 'is': 2, 'he': 3, 'she': 4, 'royal': 5, 'the': 6}\n",
      "\n",
      "{0: 'queen', 1: 'king', 2: 'is', 3: 'he', 4: 'she', 5: 'royal', 6: 'the'}\n"
     ]
    }
   ],
   "source": [
    "word2int = {}\n",
    "int2word = {}\n",
    "vocab_size = len(words)\n",
    "\n",
    "for i, word in enumerate(words):\n",
    "    word2int[word] = i\n",
    "    int2word[i] = word\n",
    "\n",
    "print(word2int)\n",
    "print()\n",
    "print(int2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['he', 'is', 'the', 'king'],\n",
       " ['the', 'king', 'is', 'royal'],\n",
       " ['she', 'is', 'the', 'royal', 'queen']]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = []\n",
    "for sentence in corpus_raw.split('.'):\n",
    "    sentences.append(sentence.split())\n",
    "    \n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 现在我们需要生成训练数据了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['he', 'is'],\n",
       " ['he', 'the'],\n",
       " ['is', 'he'],\n",
       " ['is', 'the'],\n",
       " ['is', 'king'],\n",
       " ['the', 'he'],\n",
       " ['the', 'is'],\n",
       " ['the', 'king'],\n",
       " ['king', 'is'],\n",
       " ['king', 'the'],\n",
       " ['the', 'king'],\n",
       " ['the', 'is'],\n",
       " ['king', 'the'],\n",
       " ['king', 'is'],\n",
       " ['king', 'royal'],\n",
       " ['is', 'the'],\n",
       " ['is', 'king'],\n",
       " ['is', 'royal'],\n",
       " ['royal', 'king'],\n",
       " ['royal', 'is'],\n",
       " ['she', 'is'],\n",
       " ['she', 'the'],\n",
       " ['is', 'she'],\n",
       " ['is', 'the'],\n",
       " ['is', 'royal'],\n",
       " ['the', 'she'],\n",
       " ['the', 'is'],\n",
       " ['the', 'royal'],\n",
       " ['the', 'queen'],\n",
       " ['royal', 'is'],\n",
       " ['royal', 'the'],\n",
       " ['royal', 'queen'],\n",
       " ['queen', 'the'],\n",
       " ['queen', 'royal']]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "\n",
    "WINDOW_SIZE = 2\n",
    "\n",
    "for sentence in sentences:\n",
    "    for word_index, word in enumerate(sentence):\n",
    "        low_bound = max(word_index - WINDOW_SIZE, 0)\n",
    "        up_bound = min(word_index + WINDOW_SIZE, len(sentence)) + 1\n",
    "        for nb_word in sentence[low_bound:up_bound]:\n",
    "            if nb_word != word:\n",
    "                data.append([word, nb_word])\n",
    "data\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 训练数据\n",
    "\n",
    "对于训练数据，我们需要划分出 X 和 Y, 并且对它们其做 one_hot。从上面的盒子，第一列是 X, input_word; 第二列是 Y, output_word;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "[[0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "def to_one_hot(data_point_index, vocab_size):\n",
    "    one_hot_code = np.zeros(vocab_size)\n",
    "    one_hot_code[data_point_index] = 1\n",
    "    return one_hot_code\n",
    "\n",
    "x_train = [] # input word\n",
    "y_train = [] # output word\n",
    "\n",
    "for word_pair in data:\n",
    "    x_train.append(to_one_hot(word2int[ word_pair[0] ], vocab_size))\n",
    "    y_train.append(to_one_hot(word2int[ word_pair[1] ], vocab_size))\n",
    "\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "print(x_train)\n",
    "print()\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34, 7) (34, 7)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  TensorFlow 模型\n",
    "\n",
    "有了 X, Y，我们就可以构建我们的 TF 模型了。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 5\n",
    "\n",
    "def model():\n",
    "    x = tf.placeholder(tf.float32, shape=(None, vocab_size))\n",
    "    y_label = tf.placeholder(tf.float32, shape=(None, vocab_size))\n",
    "    W1 = tf.Variable(tf.random_normal([vocab_size, EMBEDDING_DIM]))\n",
    "    b1 = tf.Variable(tf.random_normal([EMBEDDING_DIM]))\n",
    "    \n",
    "    hidden_representation = tf.add(tf.matmul(x, W1), b1)\n",
    "    \n",
    "    W2 = tf.Variable(tf.random_normal([EMBEDDING_DIM, vocab_size]))\n",
    "    b2 = tf.Variable(tf.random_normal([vocab_size]))\n",
    "    \n",
    "    z = tf.add(tf.matmul(hidden_representation, W2), b2)\n",
    "    \n",
    "    predict = tf.nn.softmax(z)\n",
    "    \n",
    "    cross_entropy_loss = tf.reduce_mean(\n",
    "        -tf.reduce_sum(y_label * tf.log(predict), reduction_indices=[1]))\n",
    "    return x, y_label, predict, cross_entropy_loss, W1, b1\n",
    "                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.6037853   0.6518512  -2.9716039   1.2559943   1.0694216 ]\n",
      " [ 0.04293491 -0.595455   -2.0238845  -0.6429834  -2.4206436 ]\n",
      " [-1.2269549   2.5857193   0.38521102  0.15727368  0.6382453 ]\n",
      " [-0.34678775 -1.3753464   0.07432036 -0.337929   -0.01936926]\n",
      " [-0.74828684 -1.2002064   0.3237147  -0.35348463 -0.66281503]\n",
      " [ 0.02022447 -1.3378646   1.4180533   1.6034536   1.2215294 ]\n",
      " [ 2.7362833   1.3575916   1.9014097   1.1230485   0.4995059 ]]\n",
      "[-0.43320796 -0.67651874 -0.3098079  -0.7158712  -0.16610207]\n"
     ]
    }
   ],
   "source": [
    "n_iters = 10000\n",
    "with tf.Session() as sess:\n",
    "    x, y_label, predict, loss, W1, b1 = model()\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss)\n",
    "    \n",
    "    feed_dict = {\n",
    "        x: x_train,\n",
    "        y_label: y_train\n",
    "    }\n",
    "    sess.run(init)\n",
    "    for i in range(n_iters):\n",
    "        sess.run(train_step, feed_dict)\n",
    "        _, loss_val = sess.run([train_step, loss], feed_dict)\n",
    "#         if (i % 500 == 0):\n",
    "#             print('Epoch %d loss is %4.3f' % (i, loss_val))\n",
    "    \n",
    "    print(sess.run(W1))\n",
    "    print(sess.run(b1))\n",
    "    vectors = sess.run(W1 + b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.17057732 -0.02466756 -3.2814116   0.5401231   0.9033196 ]\n",
      " [-0.39027303 -1.2719737  -2.3336926  -1.3588545  -2.5867457 ]\n",
      " [-1.6601629   1.9092007   0.07540312 -0.55859756  0.47214323]\n",
      " [-0.7799957  -2.051865   -0.23548754 -1.0538002  -0.18547133]\n",
      " [-1.1814948  -1.8767252   0.01390681 -1.0693558  -0.8289171 ]\n",
      " [-0.41298348 -2.0143833   1.1082454   0.8875824   1.0554273 ]\n",
      " [ 2.3030753   0.6810729   1.5916018   0.40717733  0.33340383]]\n"
     ]
    }
   ],
   "source": [
    "# 现在我们字典中的 7 个单词就被转换成了 vector 了。\n",
    "print(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.17057732 -0.02466756 -3.2814116   0.5401231   0.9033196 ]\n"
     ]
    }
   ],
   "source": [
    "# 看下 'queen' 单词的表示\n",
    "print(vectors[word2int['queen']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Embeddins\n",
    "\n",
    "其实上面学习到的 Vectors 就是一个 Embeddings 矩阵。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.17057732 -0.02466756 -3.2814116   0.5401231   0.9033196 ]\n",
      " [-0.41298348 -2.0143833   1.1082454   0.8875824   1.0554273 ]]\n"
     ]
    }
   ],
   "source": [
    "inputs = tf.placeholder(tf.int32, [None], name='word_ids')\n",
    "embeddings = tf.constant(vectors)\n",
    "embedded = tf.nn.embedding_lookup(embeddings, inputs)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    transformed = sess.run(embedded, {\n",
    "        inputs: [word2int['queen'], word2int['royal']]\n",
    "    })\n",
    "    print(transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec \n",
    "\n",
    "Wrod2Vec 转换完成后，我们就可以计算它们的相似度，并把他们给画出来了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_dist(vec1, vec2):\n",
    "    return np.sqrt(np.sum((vec1 - vec2) ** 2))\n",
    "\n",
    "def find_closest(word_index, vectors):\n",
    "    min_dist = np.inf\n",
    "    min_index = -1\n",
    "    query_vector = vectors[word_index]\n",
    "    for index, vector in enumerate(vectors):\n",
    "        dist = euclidean_dist(query_vector, vector)\n",
    "        if dist < min_dist and not np.array_equal(vector, query_vector):\n",
    "            min_dist = dist\n",
    "            min_index = index\n",
    "    return min_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he\n",
      "king\n",
      "he\n"
     ]
    }
   ],
   "source": [
    "print(int2word[find_closest(word2int['king'], vectors)])\n",
    "print(int2word[find_closest(word2int['queen'], vectors)])\n",
    "print(int2word[find_closest(word2int['royal'], vectors)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  绘制图形\n",
    "\n",
    "首先对数据进行下降维，从 5 维降到 2 维。可以使用 tSNE(teesnee)!\n",
    "\n",
    "在绘制之前，我们需要先对向量做一下规一化处理，这样才能更好的绘制。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "model = TSNE(n_components=2, random_state=0)\n",
    "np.set_printoptions(suppress=True)\n",
    "vectors = model.fit_transform(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.6365995   0.77119464]\n",
      " [-0.39817494  0.9173095 ]\n",
      " [ 0.9058182  -0.4236667 ]\n",
      " [ 0.57071555 -0.8211478 ]\n",
      " [-0.9965617  -0.08285362]\n",
      " [ 0.99798524 -0.0634459 ]\n",
      " [-0.3477955  -0.93757045]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "normalizer = preprocessing.Normalizer()\n",
    "vectors = normalizer.fit_transform(vectors, 'l2')\n",
    "print(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "she 0.6365995 0.77119464\n",
      "is -0.39817494 0.9173095\n",
      "king 0.9058182 -0.4236667\n",
      "queen 0.57071555 -0.8211478\n",
      "the -0.9965617 -0.082853615\n",
      "royal 0.99798524 -0.0634459\n",
      "he -0.3477955 -0.93757045\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for word in words:\n",
    "    vec = vectors[word2int[word]]\n",
    "    print(word, vec[0], vec[1])\n",
    "    ax.annotate(word, (vec[0], vec[1]))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
