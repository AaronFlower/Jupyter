{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 分类损失函数\n",
    "\n",
    "[Loss functions for classification](https://en.m.wikipedia.org/wiki/Loss_functions_for_classification), wiki 最下面的有一个 mobile view. 可以多用一用。\n",
    "\n",
    "损失函数表示分类器在错误分类时所付出的代价。\n",
    "\n",
    "假设有样本集 $X$, 其标签类为 $Y = \\{-1, 1\\}$, 我们希望找一个最优的映射，$f: X \\mapsto R$, 使得 $f$ 是 $\\vec{x}$ 到 $y$ 的最佳映射。\n",
    "\n",
    "但是由于样本集信息不完整或者样本噪声，可能会出现映射错误的概率。当然我们希望这个概率是最小的，即有以下期望，最小化期望风险（minimize risk), 所有下面的期望公式：\n",
    "\n",
    "$$\n",
    "I[f] = \\int_{X \\times Y} V(f(\\vec{x}), y)p(\\vec{x}, y)d\\vec{x}dy\n",
    "$$\n",
    "\n",
    "- $ V(f(\\vec{x}), y) $ 是损失函数\n",
    "- $ p(\\vec{x}, y) $ 是概率密度函数，也可以写为 $p(y | \\vec{x})p(\\vec{x})$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "但实际上, $p(\\vec{x}, y)$ 是未知的。那么，我们就需要根据样本集，利用仅知的样本信息，来最小化经验风险（minimize empirical risk):\n",
    "\n",
    "假设有下面的样本集，\n",
    "\n",
    "$$\n",
    "S = \\{(\\vec{x_1}, y_1), \\dots, \\vec{x_n}, y_n)\\}\n",
    "$$\n",
    "\n",
    "样本之间都是 IID 的。则从样本空间中，我可以求得最小化的经验风险：\n",
    "\n",
    "$$\n",
    "I_{S} = \\frac{1}{n} \\sum_{i = 1}^{n} V(f(\\vec{x_i}), y_i)\n",
    "$$\n",
    "\n",
    "用经验风险来代替期望风险。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最佳实践是，将损失函数定义为仅与一个变量相关的函数，这样计算最为方便。\n",
    "\n",
    "在分类问题中，损失函数通常被写成仅实际标签 y 与预测标签类 $\\hat{y} = f(\\vec{x})$ 之间的乘积，那么 $y f(\\vec{x})$ 即可看成惟一的变量了。即一般的选择形式如下：\n",
    "\n",
    "$$\n",
    "V(f(\\vec{x}), y) = \\phi (-y f(\\vec{x}))\n",
    "$$\n",
    "\n",
    "这就意味着，如果 $f_{S}^{*}$ 是最优的，那么其经验风险最小。\n",
    "\n",
    "根据 \\phi (-y f(\\vec{x})) 的形式，下面是一些常见的损失函数。\n",
    "\n",
    "1. 平方损失函数 (square loss)\n",
    "\n",
    "$$\n",
    "V(f(\\vec{x}), y) = (1 - y f(\\vec{x}))^2\n",
    "$$\n",
    "\n",
    "    常用在最小二乘法中。\n",
    "\n",
    "2. 铰链损失（hinge loss)\n",
    "\n",
    "$$\n",
    "V(f({\\vec  {x}}),y)=\\max(0,1-yf({\\vec  {x}}))=|1-yf({\\vec  {x}})|_{{+}}.\n",
    "$$\n",
    "\n",
    "    主要用于支持向量机（SVM） 中。\n",
    "\n",
    "3. 平滑的 hinge loss, 给定一个 $\\alpha$：\n",
    "\n",
    "    其中：${\\displaystyle z=yf({\\vec {x}})}$\n",
    "\n",
    "\\begin{align}\n",
    "{\\displaystyle f_{\\alpha }^{*}(z)\\;=\\;{\\begin{cases}{\\frac {\\alpha }{\\alpha +1}}&{\\text{if }}z<0\\\\{\\frac {1}{\\alpha +1}}z^{\\alpha +1}-z+{\\frac {\\alpha }{\\alpha +1}}&{\\text{if }}0<z<1\\\\0&{\\text{if }}z\\geq 1\\end{cases}}.}\n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Logistic loss\n",
    "\n",
    "$$\n",
    "V(f({\\vec  {x}}),y)={\\frac  {1}{\\ln 2}}\\ln(1+e^{{-yf({\\vec  {x}})}})\n",
    "$$\n",
    "\n",
    "5. Cross entropy loss(Log loss, 互熵损失）\n",
    "\n",
    "    我们可以把标签从 $\\{-1, 1\\}$ 转到 $\\{0, 1\\}$, 使用 $t = (1 + y) / 2$, 所二元互熵损失函数为：\n",
    "\n",
    "$$\n",
    "{\\displaystyle V(f({\\vec {x}}),t)=-t\\ln(\\sigma ({\\vec {x}}))-(1-t)\\ln(1-\\sigma ({\\vec {x}}))}\n",
    "$$\n",
    "\n",
    "    其中：\n",
    "\n",
    "$$\n",
    "{\\displaystyle \\sigma ({\\vec {x}})={\\frac {1}{1+e^{-f({\\vec {x}})}}}}\n",
    "$$\n",
    "\n",
    "    可以证明 4， 5 是等价的。用于Logistic 回归与Softmax 分类中；\n",
    "    \n",
    "6. 指数损失 (exponential loss)\n",
    "\n",
    "$$\n",
    "{\\displaystyle V(f({\\vec {x}}),y)=e^{-\\beta yf({\\vec {x}})}}\n",
    "$$\n",
    "\n",
    "    用在 Ada boost 中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"300px\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/f/f3/Loss_function_surrogates.svg/1280px-Loss_function_surrogates.svg.png\" />\n",
    "\n",
    "Plot of various functions. Blue is the 0–1 indicator function. Green is the square loss function. Purple is the hinge loss function. Yellow is the logistic loss function. Note that all surrogates give a loss penalty of 1 for y=f(x= 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "1. [损失函数 hinge losss vs. softmax loss](https://blog.csdn.net/u010976453/article/details/78488279)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [ ] SVM 是 Hinge 损失函数加上一个 L2 正则。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
