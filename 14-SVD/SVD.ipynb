{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD\n",
    "SVD (Sigular Value Decomposition, Sigular Vector Decomposition) 奇异值分解，是提取信息的强大工具。下面的简单的例子可能只是解释其威力的一小部分。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD 的应用\n",
    "先介绍下 SVD 的应用。\n",
    "- 优点：简化数据，提取数据去除噪声，提高算法的结果。\n",
    "- 缺点：数据的转换可能难以理解。\n",
    "\n",
    "#### 1. 隐主义索引\n",
    "最早的 SVD 应用之一就是信息检索。我们称利用 SVD 的方法为**隐语义索引 (Latent Semantic Indexing, LSI)** 或 **隐语义分析（Latent Semantic Analysis, LSA）**。\n",
    "\n",
    "在 LSI 中，一个矩阵是由文档和词语组成的。在该矩阵上应用 SVD 时，就会构建出多个奇异值。这些奇异值代表了文档中的概念或主题，这一点可以用于更高效的文档搜索。\n",
    "\n",
    "#### 2. 推荐系统\n",
    "#### 3. 数据压缩"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD 原理\n",
    "SVD, 奇异向量分解 (Singular Vector Decomposition). 对于 $\\forall A$ ($A$ 不一定是方阵)，都可以写成： $ AV = U\\Sigma $。 即：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "A = U \\Sigma V^T\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "其中：\n",
    "- U, V 是正定矩阵，即 $V^{T} = V^{-1}, U^{T} = U^{-1}$\n",
    "- $ U $ 是 $ AA^T $ 的特征向量矩阵。\n",
    "- $ V $ 是 $ A^TA $ 的特征向量矩阵。\n",
    "\n",
    "要求 A 是非方阵，而组成 $A^TA, AA^T$ 对称矩阵。为什么要是对称矩阵那？因为对称矩阵有以下性质：\n",
    "\n",
    "1. 对称矩阵的特征值都是实数，即：$\\forall  \\lambda_i \\in R $.\n",
    "2. 若特征值 $\\lambda_i \\ne \\lambda_j$, 则分别对应的特征向量是正交的。\n",
    "3. 其特征值组成的特征向量矩阵 Q 是正交的，即 $Q^T = Q^{-1}$\n",
    "\n",
    "根据上面的两个性质，则可以证明对称矩阵一定可以对角化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "需要注意的是矩阵的维度，我们并不要示 A 一定是方阵。假设 A 的是维度是 $m \\times n$。则下面矩阵各自的维度是：\n",
    "\n",
    "- $AA^T_{(m\\times m)}$, 则有其特征向量矩阵 $U_{(m\\times m)}$\n",
    "- $A^TA_{(n\\times n)}$, 则有其特征向量矩阵 $V_{(n\\times n)}$\n",
    "- 从上面可知有 $\\Sigma_(m \\times n)$\n",
    "\n",
    "Wiki 上有一个很好的[例子](https://en.wikipedia.org/wiki/Singular-value_decomposition#Example)说明这一点。\n",
    "\n",
    "关于 $\\Sigma_(m \\times n)$ 矩阵，p = min(m, n) 则 $\\Sigma_(p \\times p)$ 是一个对角矩阵，则对角线上的每一个元素都是 U, V 特征值的平方，即 $ {\\sigma_i}^2 = \\lambda_i $。当我们取 Rank `k` (k <= p, 前 k 个特征值) 时就有下面的式子。\n",
    "\n",
    "$$\n",
    "A = \\vec{u_1}\\sigma_1\\vec{v_1}^T + \\vec{u_2}\\sigma_2\\vec{v_2}^T + ... + \\vec{u_k}\\sigma_n\\vec{v_k}^T\n",
    "$$\n",
    "\n",
    "- $ {\\sigma_i}^2 = \\lambda_i $,  $ \\lambda_i $ 是 $AA^T$ 的特征向值，当然也是 $A^TA$ 的特征值。$ \\sigma_1 \\sigma_2 ... \\sigma_k $ 的排列顺序是按特征值的大小降序排列的。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 隐语义索引"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "矩阵 X 中的每一个元素 $x_{i,j}$ 表示词语$i$ (term i)， 在文档 $j$(document j) 中出现的值（如频率）。\n",
    "\n",
    "$$\n",
    "\\begin{matrix}&{\\textbf {d}}_{j}\\\\&\\downarrow \\\\{\\textbf {t}}_{i}^{T}\\rightarrow &{\\begin{bmatrix}x_{1,1}&\\dots &x_{1,j}&\\dots &x_{1,n}\\\\\\vdots &\\ddots &\\vdots &\\ddots &\\vdots \\\\x_{i,1}&\\dots &x_{i,j}&\\dots &x_{i,n}\\\\\\vdots &\\ddots &\\vdots &\\ddots &\\vdots \\\\x_{m,1}&\\dots &x_{m,j}&\\dots &x_{m,n}\\\\\\end{bmatrix}}\\end{matrix}\n",
    "$$\n",
    "\n",
    "那么该矩阵的每一行就表示一个词语(term) 与所有文档之间的关系。即：\n",
    "\n",
    "$\n",
    "\\textbf{t}_{i}^{T}={\\begin{bmatrix}x_{i,1}&\\dots &x_{i,j}&\\dots &x_{i,n}\\end{bmatrix}}\n",
    "$\n",
    "\n",
    "而矩阵的每一列就表示一个文档(document) 与所有词语之间的关系。 即：\n",
    "\n",
    "$\n",
    "\\textbf{d}_{j}={\\begin{bmatrix}x_{1,j}\\\\\\vdots \\\\x_{i,j}\\\\\\vdots \\\\x_{m,j}\\\\\\end{bmatrix}}\n",
    "$\n",
    "\n",
    "现在，$\\textbf{t}_{i}^T\\textbf{t}_{p}$ **就是两个词语之间在所有文档上的关系**，那 $XX^T$ 就是**词语向量之间的关系矩阵**，相应的 $X^TX$ 是**文档向量之间的关系矩阵**。这两个矩阵都是对称矩阵，存在对应的正交特征矩阵。使得 X 可以分解成：$ X = U \\Sigma V^T $。\n",
    "\n",
    "$$\n",
    " \\begin{matrix}XX^{T}&=&(U\\Sigma V^{T})(U\\Sigma V^{T})^{T}=(U\\Sigma V^{T})(V^{T^{T}}\\Sigma ^{T}U^{T})=U\\Sigma V^{T}V\\Sigma ^{T}U^{T}=U\\Sigma \\Sigma ^{T}U^{T}=U\\Sigma ^{2}U^{T}\\\\X^{T}X&=&(U\\Sigma V^{T})^{T}(U\\Sigma V^{T})=(V^{T^{T}}\\Sigma ^{T}U^{T})(U\\Sigma V^{T})=V\\Sigma ^{T}U^{T}U\\Sigma V^{T}=V\\Sigma ^{T}\\Sigma V^{T}=V\\Sigma ^{2}V^{T}\\end{matrix}\n",
    "$$\n",
    "\n",
    "最终的分解形式如下：\n",
    "\n",
    "$$\n",
    "{\\displaystyle {\\begin{matrix}&X&&&U&&\\Sigma &&V^{T}\\\\&({\\textbf {d}}_{j})&&&&&&&({\\hat {\\textbf {d}}}_{j})\\\\&\\downarrow &&&&&&&\\downarrow \\\\({\\textbf {t}}_{i}^{T})\\rightarrow &{\\begin{bmatrix}x_{1,1}&\\dots &x_{1,j}&\\dots &x_{1,n}\\\\\\vdots &\\ddots &\\vdots &\\ddots &\\vdots \\\\x_{i,1}&\\dots &x_{i,j}&\\dots &x_{i,n}\\\\\\vdots &\\ddots &\\vdots &\\ddots &\\vdots \\\\x_{m,1}&\\dots &x_{m,j}&\\dots &x_{m,n}\\\\\\end{bmatrix}}&=&({\\hat {\\textbf {t}}}_{i}^{T})\\rightarrow &{\\begin{bmatrix}{\\begin{bmatrix}\\,\\\\\\,\\\\{\\textbf {u}}_{1}\\\\\\,\\\\\\,\\end{bmatrix}}\\dots {\\begin{bmatrix}\\,\\\\\\,\\\\{\\textbf {u}}_{l}\\\\\\,\\\\\\,\\end{bmatrix}}\\end{bmatrix}}&\\cdot &{\\begin{bmatrix}\\sigma _{1}&\\dots &0\\\\\\vdots &\\ddots &\\vdots \\\\0&\\dots &\\sigma _{l}\\\\\\end{bmatrix}}&\\cdot &{\\begin{bmatrix}{\\begin{bmatrix}&&{\\textbf {v}}_{1}&&\\end{bmatrix}}\\\\\\vdots \\\\{\\begin{bmatrix}&&{\\textbf {v}}_{l}&&\\end{bmatrix}}\\end{bmatrix}}\\end{matrix}}} \n",
    "$$\n",
    "\n",
    "所我们只选择前 k 个最大的特征值后，那么我就用了最小误差来近似表示了 X。 这种表示方法有点误差，但是我们却可以把词语和文档在同一个语义空间(semantic space) 上表示。$\\textbf{t}_{i}^{T}$ 从现在从 n 维映射到**更小的 k 维空间**（降维）上，相应的 $\\textbf{d}_{j}$ 也是一样。我们把这近似写成：\n",
    "\n",
    "$$\n",
    "{\\displaystyle X_{k}=U_{k}\\Sigma _{k}V_{k}^{T}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们就可以在低维空间上进行以下操作了：\n",
    "1. 计算文档 j, q 之间的相似性，计算 $ {\\hat {\\textbf {d}}}_{j} $ 与 $ {\\hat {\\textbf {d}}}_{q} $ 之间的 cosine。\n",
    "2. 比较词语 i, p 之间的相似性，计算 $ {\\hat {\\textbf {t}}}_{i} $ 与 $ {\\hat {\\textbf {t}}}_{p} $ 之间的 cosine。注意现在 $ {\\hat {\\textbf {t}}}_{i} $ 也是列向量了。\n",
    "3. 通过计算词语、文档之间的相似性，可能通过聚簇算法 (如：k-means) 来对词语和文档进行分类。\n",
    "4. 给出一个查询字符串 (query), 可以把这个 query 生成一个小型的文档向量 $\\textbf {q} $ ，与低维空间中的文档进行比较相似度进行搜索。\n",
    "\n",
    "要做第 4 种操作，你需要把 $\\textbf {q} $  文档变换到低维空间中去。因为 \n",
    "$$\n",
    "\\textbf{d}_j = U_k \\Sigma_k \\hat{\\textbf{d}}_j \\\\\n",
    "\\hat{\\textbf{d}}_j  = \\Sigma_k^{-1} U_k^T  \\textbf{d}_j\n",
    "$$\n",
    "\n",
    "其中, $ \\textbf{d}_j $ 是原始空间上的文档向量，$\\hat{\\textbf{d}}_j$ 是降维后的文档向量。\n",
    "\n",
    "所以，通过相同的变换可以得到 $\\hat{\\textbf{q}}$，即 $ \\hat{\\textbf{q}}  = \\Sigma_k^{-1} U_k^T  \\textbf{q} $。\n",
    "同样的对于词语变换公式是：$ \\hat{\\textbf{t}}  = \\Sigma_k^{-1} V_k^T  \\textbf{t} $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "应用：\n",
    "1. 在低维空间上可以进行数据聚簇，文档分类。\n",
    "2. 不同语言之间的信息检索(cross language retrieval)。\n",
    "3. 寻找同义词，多义词 （Synonymy and polysemy），这是 NLP(Natural Language Processing) 的基础。\n",
    "4. 根据查询字符串，找到相应文档（information retrieval）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 词语-文档矩阵 (term-document matrix)\n",
    "应用 LSI 首先要先建立 term-document 矩阵 A。 在矩阵 A 中，一个词语代表一行，每个文档代表一列，初始时 $a_{ij}$ 表示词语 i 在文档 j 中出现的次数。 A 通常是一个很大的稀疏矩阵。一但 A 构建好之后，我们则需要两个函数来调整元素的权重。\n",
    "\n",
    "1. Term Frequency 函数 (局部的），$l_{ij}$，即词语在某一个文档中出现的相对频率。\n",
    "2. Document Frequence 函数(全局的）$g_{i} $，即词语在所有文档中出现的相对频率。\n",
    "\n",
    "而最终 $a_{ij} = l_{ij} * g_{i}$。 \n",
    "\n",
    "#### 2.1.1 一些常见的 Term Frequency 函数 (局部的)\n",
    "假设 $tf_{ij}$ 是词语在文档中出现的次数。\n",
    "\n",
    "\n",
    "| 名称  |  定义 |\n",
    "|---|---|\n",
    "| Binary  | $l_{ij} = 1$ |\n",
    "| Raw count  | $l_{ij} = tf_{ij}$ |\n",
    "| Log  | $l_{ij} = log(tf_{ij} + 1)$ |\n",
    "\n",
    "\n",
    "#### 2.1.1 一些常见的 Document Frequency 函数 (全局的)\n",
    "\n",
    "- n: 是所有文档的个数。\n",
    "- $df_i$: 词语在所有文档中出现的指示次数（在文档中出现 +1， 否则 + 0）。\n",
    "- ${gf} _{i}$: 词语在所有文档中出的次数。\n",
    "- ${p} _{ij}$: 是词语 i 在文档 j 出现的次数除去所有文档中出的次数，即在所有文档中所占的比例。\n",
    "\n",
    "| 名称  |  定义 |\n",
    "|---|---|\n",
    "| Binary  | $g_{i} = 1$ |\n",
    "| idf(Inverse Document Frequence)  | $g_{i} = log_2{\\frac{n}{1 + df_i}}$ |\n",
    "| Entropy|$ g_{i}=1+ \\sum _{j}{\\frac  {p_{{ij}}\\log p_{{ij}}}{\\log n}} , \\space {\\displaystyle p_{ij}={\\frac {\\mathrm {tf} _{ij}}{\\mathrm {gf} _{i}}}}$|\n",
    "\n",
    "\n",
    "\n",
    "[倒排索引](https://en.wikipedia.org/wiki/Tf%E2%80%93idf#Inverse_document_frequency_2)给出了一个词语所能提供多少信息，该词语在所有文档中都是常见的，还是很少出现的。常见的其全局权重自己就低，而少见的其全局权重自然就很高了。\n",
    "\n",
    "**tf-idf** 是最常用的组合方式，\n",
    "$$\n",
    "tf\\texttt{-} idf(t, d, D) = tf(t, d) \\times idf(t, D)\n",
    "$$\n",
    "- t: 词语\n",
    "- d: 文档\n",
    "- D: 所及文档集\n",
    "\n",
    "推荐的组合下面的方法来计算 A 中的元素值。\n",
    "\n",
    "$$ a_{ij} = l_{ij} * g_{i} = log(tf_{ij} + 1) \\cdot log_2{\\frac{n}{1 + df_i}}$$\n",
    "\n",
    "解释:  $term_i$ 在文档 j 中出现的次数取对数 乘上 $term_i$ 在所有文档中出现的次数的倒数取对数。\n",
    "\n",
    "或\n",
    "\n",
    "$$ a_{ij} = l_{ij} * g_{i} = log(tf_{ij} + 1) \\cdot 1+ \\sum _{j}{\\frac  {p_{{ij}}\\log p_{{ij}}}{\\log n}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 隐语义举例\n",
    "#### 3.1 问题描述 （参考：Grossman and Frieder’s Information Retrieval, Algorithms and Heuristics ）\n",
    "\n",
    "假设语料库 (corpus) 有下面三个文档：\n",
    "\n",
    "d1: Shipment of gold damaged in a fire.\n",
    "\n",
    "d2: Delivery of silver arrived in a silver truck.\n",
    "\n",
    "d3: Shipment of gold arrived in a truck.\n",
    "\n",
    "给出查询字符串 `gold silver truck`，请使用 LIS 对文档进行排序。\n",
    "#### 3.2 解决方法\n",
    "1. [Raw count方法](http://www1.se.cuhk.edu.hk/~seem5680/lecture/LSI-Eg.pdf) 即只做统计。\n",
    "2. td-idf，倒排索引方法。\n",
    "\n",
    "#### 3.3 代码实现\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def raw_count_TF(wordSet, docWords):\n",
    "    '''\n",
    "        计算 term-frequency, 词语出现的相对频率。\n",
    "        BoW: Bag of Words, 词袋模型。\n",
    "        \n",
    "    '''\n",
    "    tfBoW = dict.fromkeys(wordSet, 0)\n",
    "    for word in docWords:\n",
    "        tfBoW[word] += 1\n",
    "    return list(tfBoW.values())\n",
    "\n",
    "def binary_DF(tfBoW):\n",
    "    '''\n",
    "        计算 document-freqency, 文档出现的相对频率。\n",
    "        在这个例子中，每个单词都会至少在文档中出现一次。我们也不需要统计单词在文档中的指示器次数。\n",
    "    '''\n",
    "    occur = tfBoW >= 1\n",
    "    tfBoW[occur] = 1\n",
    "    tfBoW[~occur] = 0\n",
    "    return tfBoW\n",
    "\n",
    "def cosine_sim(vec_a, vec_b):\n",
    "    num = vec_a.T * vec_b\n",
    "    denom = np.linalg.norm(vec_a) * np.linalg.norm(vec_b)\n",
    "    return (num / denom).A.flatten()[0]\n",
    "    \n",
    "\n",
    "\n",
    "class LSI(object):\n",
    "    '''\n",
    "        Latent Semantic Indexing\n",
    "    '''\n",
    "    def __init__(self, calc_tf = raw_count_TF, calc_df = binary_DF, k = 2):\n",
    "        self.calc_tf = calc_tf\n",
    "        self.calc_df = calc_df\n",
    "        self.k = k\n",
    "        self.wordSet = set()\n",
    "        self.U = None\n",
    "        self.Sigma = None\n",
    "        self.VT = None\n",
    "        return\n",
    "        \n",
    "    def gen_docs(self, corpus):\n",
    "        docs = []\n",
    "        wordSet = set()\n",
    "        for doc_str in corpus:\n",
    "            doc = self.tokenize(doc_str)\n",
    "            docs.append(doc)\n",
    "            wordSet = wordSet | set(doc)\n",
    "        return docs, wordSet\n",
    "    \n",
    "    def tokenize(self, doc_str):\n",
    "        words = doc_str.split(' ')\n",
    "        return [word.lower() for word in words]\n",
    "    \n",
    "    def vectorize (self, doc):\n",
    "        words = self.tokenize(doc)\n",
    "        return np.array(self.calc_tf(self.wordSet, words)).reshape(-1, 1)\n",
    "    \n",
    "    def transform_doc_vec(self, qvec):\n",
    "        k = self.k\n",
    "        Sigma = np.mat(self.Sigma[0:k, 0:k])\n",
    "        U = np.mat(self.U[:, 0:k])\n",
    "        return Sigma.I * U.T * qvec\n",
    "    \n",
    "    def query_sim(self, query):\n",
    "        q = self.vectorize(query)\n",
    "        q_hat = self.transform_doc_vec(q)\n",
    "        VT = self.VT[0:self.k, :]\n",
    "        n = VT.shape[1]\n",
    "        sims = []\n",
    "        docs = ['doc' + str(i + 1) for i in range(n)]\n",
    "        for i in range(n):\n",
    "            sims.append(cosine_sim(q_hat, VT[:, i:i+1]))\n",
    "        \n",
    "        rankIndices = np.argsort(sims)[::-1]\n",
    "        print(sims)\n",
    "        return [docs[i] for i in rankIndices]\n",
    "    \n",
    "    def lsi(self, corpus):\n",
    "        '''\n",
    "            根据语料库 corpus 生成隐语义索引 LSI。\n",
    "        '''\n",
    "        docs, wordSet = self.gen_docs(corpus)\n",
    "        self.wordSet = wordSet\n",
    "        m = len(wordSet)\n",
    "        n = len(docs)\n",
    "        A = np.zeros((m, n))\n",
    "        \n",
    "        for i, doc in enumerate(docs):\n",
    "            A[:, i] = self.calc_tf(wordSet, doc)\n",
    "        for i in range(m):\n",
    "            A[i] = A[i] * self.calc_df(A[i].copy())\n",
    "            \n",
    "        self.U, Sigma, self.VT = np.linalg.svd(A)\n",
    "        self.Sigma = np.eye(len(Sigma)) * Sigma\n",
    "        \n",
    "        return docs, wordSet, A\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.49446664 -0.64582238 -0.58173551]\n",
      " [ 0.64917576 -0.71944692  0.24691489]\n",
      " [-0.57799098 -0.25555741  0.77499473]]\n",
      "[-0.05395084366642496, 0.9909874267484721, 0.44795946582829754]\n",
      "The similarity order of docs for the query \"gold silver truck\" is : ['doc2', 'doc3', 'doc1']\n"
     ]
    }
   ],
   "source": [
    "corpus = [\n",
    "    'Shipment of gold damaged in a fire', \n",
    "    'Delivery of silver arrived in a silver truck',\n",
    "    'Shipment of gold arrived in a truck'\n",
    "]\n",
    "\n",
    "query = 'gold silver truck'\n",
    "\n",
    "lsi = LSI()\n",
    "lsi.lsi(corpus)\n",
    "print(lsi.VT)\n",
    "print('The similarity order of docs for the query \"{0}\" is :'.format(query),lsi.query_sim(query))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参考\n",
    "1. [LSI](https://en.wikipedia.org/wiki/Latent_semantic_analysis)\n",
    "2. [tf-idf](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
