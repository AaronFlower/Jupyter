{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting (梯度提升算法）\n",
    "\n",
    "Boosting 提升算法的实质是在函数空间(损失函数)上做梯度下降。\n",
    "\n",
    "\"GB is performing gradient descent in (loss) function space.\"\n",
    "\n",
    "GB 梯度提升算法可以用于分类和回归，它通过对一组弱分类器的集成。而弱分类器通常使用决策树（Decision Tree), 所以就有了我们的 GBDT (Gradient Boosting Decision Tree), 梯度提升决策树。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 算法\n",
    "\n",
    "- Input: \n",
    "    - 给定样本集： $\\{(x_i, y_i)\\}_{i = 1}^{n}$\n",
    "    - 可微损失函数：$L(y, F(x))$\n",
    "    - 基学习器: $h_m(x)$\n",
    "    - 迭代次数: $M$\n",
    "- Algorithm:\n",
    "    1. 使用一个常数来初始化模型\n",
    "        $$\n",
    "        F_0(x) = \\underset{\\gamma}{\\arg\\min} \\sum_{i=1}^n L(y_i, \\gamma).\n",
    "        $$\n",
    "    2. For m = 1 to M:\n",
    "        1. 根据损失函数计算残差\n",
    "            $$\n",
    "            r_{im} = -\\left[\\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)}\\right]_{F(x)=F_{m-1}(x)} \\quad \\mbox{for } i=1,\\ldots,n.\n",
    "            $$\n",
    "        2. 根据残差组成新的样本集 $\\{(x_i, r_{im})\\}_{i=1}^n.$ 来训拟合出一个弱分类器 $h_m(x)$ 。\n",
    "        3. 计算出参数 $\\gamma_m$\n",
    "        $$\n",
    "        \\gamma_m = \\underset{\\gamma}{\\operatorname{arg\\,min}} \\sum_{i=1}^n L\\left(y_i, F_{m-1}(x_i) + \\gamma h_m(x_i)\\right).\n",
    "        $$\n",
    "        4. 更新模型\n",
    "        $$\n",
    "        F_{m}(x)=F_{{m-1}}(x)+\\gamma _{m}h_{m}(x).    \n",
    "        $$\n",
    "    3. 输出 $F_{M}(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 算法实现\n",
    "\n",
    "看上面的算法也话感觉会很复杂，我们可以用 MSE 损失函数来举例说明一下。\n",
    "\n",
    "即 $ L(y, F(x)) = \\frac{1}{2}(y_i - F(x))^2 $, 为了方便，我们用 $ \\hat{y} $ 来代替 $ F(x) $。\n",
    "\n",
    "所有下面的实现算法。\n",
    "\n",
    "- Input: \n",
    "    - 给定样本集： $ \\{(x_i, y_i)\\}_{i = 1}^{n} $\n",
    "    - 可微损失函数：$ L(y, F(x)) = \\frac{1}{2}(y_i - \\hat{y_i})^2 $\n",
    "    - 基学习器: $ h_m(x) $\n",
    "    - 迭代次数: $ M $\n",
    "- Algorithm:\n",
    "    1. 使用一个常数来初始化模型\n",
    "        $$\n",
    "        F_0(x) = \\underset{\\gamma}{\\arg\\min} \\sum_{i=1}^n L(y_i, \\gamma).\n",
    "        $$\n",
    "        \n",
    "        寻找这个常数 $\\gamma$，一个直观的想法用平均值就好，即 $\\gamma = \\frac{1}{n}\\sum_{i = 1}^{n}y_i$ 。\n",
    "        \n",
    "        其实这是可以推导的，因为 $F_0(x) = \\sum_{i=1}^n L(y_i, \\gamma) = \\frac{1}{2} \\sum_{i = 1}(y_i - r)^2$。对该函数示驻点即可：\n",
    "        $$\n",
    "            \\frac{dF_0}{d \\gamma} = -\\sum_{i = 1}^{n}(y_i - \\gamma)\n",
    "        $$\n",
    "        \n",
    "        所以，$ \\gamma = \\frac{1}{n}\\sum_{i = 1}^{n}y_i $\n",
    "        \n",
    "    2. For m = 1 to M:\n",
    "        1. 根据损失函数计算残差\n",
    "            $$\n",
    "            r_{im} = -\\left[\\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)}\\right]_{F(x)=F_{m-1}(x)} \\quad \\mbox{for } i=1,\\ldots,n.\n",
    "            $$\n",
    "            \n",
    "            根据 $L = \\frac{1}{2}(y_i - \\hat{y_i})^2$ 为 MSE 函数，可以求得：\n",
    "            \n",
    "            $$\n",
    "                \\frac{\\partial L}{\\partial y_i} = -(y_i - \\hat{y_i})\n",
    "            $$\n",
    "            \n",
    "            所以 $r_{im} =(y_i - \\hat{y_i}), i = 1, \\dots, n $   \n",
    "        2. 根据残差组成新的样本集 $\\{(x_i, r_{im})\\}_{i=1}^n.$ 来训拟合出一个弱分类器 $h_m(x)$ 。\n",
    "        3. 计算出参数 $\\gamma_m$\n",
    "        $$\n",
    "        \\gamma_m = \\underset{\\gamma}{\\operatorname{arg\\,min}} \\sum_{i=1}^n L\\left(y_i, F_{m-1}(x_i) + \\gamma h_m(x_i)\\right).\n",
    "        $$\n",
    "        上面的式子也用 MSE 替换可以写为: \n",
    "        $$\n",
    "        L = \\frac{1}{2}\\sum_{i=1}^n \\bigg(y_i - \\big(F_{m-1}(x_i) + \\gamma_m h_m(x_i)\\big)\\bigg)^2  \n",
    "        $$\n",
    "        对 $L$ 求导，找到 gamma $\\gamma_m$ 的驻点。就是我们所求的参数了。\n",
    "        \n",
    "        $$\n",
    "        \\frac{\\partial L}{\\partial \\gamma_m} = \\frac{\\partial \\bigg(\\frac{1}{2}\\sum_{i=1}^n \\big(y_i - \\big(F_{m-1}(x_i) + \\gamma_m h_m(x_i)\\big)\\big)^2\\bigg)}{\\partial \\gamma_m}\n",
    "        $$\n",
    "        \n",
    "        最终求得: $\\gamma_m $。\n",
    "        \n",
    "        对于上面的算法 $\\gamma_m$ 并不容易求得。根据 [Friedman](https://en.m.wikipedia.org/wiki/Gradient_boosting) 将上式替换成：\n",
    "        \n",
    "        $$\n",
    "        {\\displaystyle F_{m}(x)=F_{m-1}(x)+\\sum _{j=1}^{J_{m}}\\gamma _{jm}\\mathbf {1} _{R_{jm}}(x),\\quad \\gamma _{jm}={\\underset {\\gamma }{\\operatorname {arg\\,min} }}\\sum _{x_{i}\\in R_{jm}}L(y_{i},F_{m-1}(x_{i})+\\gamma ).}\n",
    "        $$\n",
    "        \n",
    "        最终可以求得:\n",
    "        $$\n",
    "        \\gamma_m = mean(y_i - F_{m - 1}(x_i))\n",
    "        $$\n",
    "        4. 更新模型\n",
    "        $$\n",
    "        F_{m}(x)=F_{{m-1}}(x)+\\gamma _{m}h_{m}(x).    \n",
    "        $$\n",
    "    3. 输出 $F_{M}(x)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有了 $r$, $\\gamma_m$ 的求法，我们就可实现我们的算法了。\n",
    "\n",
    "对最不同的损失函数时，如: MSE, MSE，  $r$, $\\gamma_m$ 的更新公式需要自己再算一遍才行。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，AdaBoost 只是 GB 一个特例而已。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
