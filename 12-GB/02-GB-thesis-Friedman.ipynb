{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GBM\n",
    "\n",
    "[Greedy Function Approximation: A Gradient Boosting Machine](https://statweb.stanford.edu/~jhf/ftp/trebst.pdf) 论文真的很长呀。\n",
    "\n",
    "只有多读几遍，然后实现才能豁然开朗。\n",
    "\n",
    "『纸上得来终觉浅，绝知此事要躬行』\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 算法 Gradient_Boost\n",
    "\n",
    "- Input: \n",
    "    - 给定样本集： $\\{(x_i, y_i)\\}_{i = 1}^{N}$\n",
    "    - 可微损失函数：$L(y, F(x))$\n",
    "    - 基学习器: $h_m(x)$\n",
    "    - 迭代次数: $M$\n",
    "- Algorithm:\n",
    "    1. 初始化模型: $ F_0(x) = \\underset{\\rho}{\\arg\\min} \\sum_{i = 1}^{N}L(y_i, \\rho)$\n",
    "    2. For m = 1 to M:\n",
    "        1. 根据损失函数计算 \"pseudo-responses\"\n",
    "            $$\n",
    "            \\tilde{y_i} = -\\left[\\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)}\\right]_{F(x)=F_{m-1}(x)} \\quad \\mbox{for } i=1,\\ldots,N.\n",
    "            $$\n",
    "        2. 根据 $\\{(x_i,\\tilde{y_i})\\}_{i=1}^n.$ 来训拟合出一个弱分类器 $h(x_i;\\mathbf{a})$ \n",
    "        $$\n",
    "        \\mathbf{a}_m = \\underset{\\mathbf{a},\\beta} {\\arg\\min} \\sum_{i=1}^{N}[\\tilde{y_i} - \\beta h(x_i;\\mathbf{a})]^2\n",
    "        $$\n",
    "        3. 计算出 $\\rho_m$\n",
    "        $$\n",
    "        \\rho_m = \\underset{\\rho}{\\operatorname{arg\\,min}} \\sum_{i=1}^n L\\left(y_i, F_{m-1}(x_i) + \\rho h(x_i; \\mathbf{a}_m)\\right).\n",
    "        $$\n",
    "        4. 更新模型\n",
    "        $$\n",
    "        F_{m}(x)=F_{{m-1}}(x)+\\rho_{m}h(x;\\mathbf{a}_m).   \n",
    "        $$\n",
    "    3. 输出 $F_{M}(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boost 算法中的 $F_0, \\tilde{y_i}, \\rho_m$ 根据不同的损失函数  $L$ 其计算方法也是不同的。\n",
    "\n",
    "下面讨论下，LSE, LAE, Huger，"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. LSE\n",
    "\n",
    "LSE, Least Squared Error, 即 $L(y, F) =\\frac{1}{2}(y - F)^2$.\n",
    "\n",
    "1. $ F_0(X) = \\bar{y} $\n",
    "2. $ \\tilde{y_i} = y_i - F_{m-1}(X_i)$\n",
    "3. $\\rho_m = \\beta_m$，即训练与预测是一致的\n",
    "    $$\n",
    "    (\\rho_m, \\mathbf{a}_m) = \\underset{\\mathbf{a},\\rho}{\\arg\\min} \\sum_{i = 1}^{N}[\\tilde{y_i} - \\rho h(x_i;\\mathbf{a})]^2\n",
    "    $$\n",
    "4. 模型迭代更新: $F_{m}(x)=F_{{m-1}}(x)+\\rho_{m}h(x;\\mathbf{a}_m).$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. LAE\n",
    "\n",
    "LSE, Least Absolute Error, 即 $L(y, F) =|y - F|$.\n",
    "\n",
    "1. $ F_0(X) = \\bar{y} $\n",
    "2. $ \\tilde{y_i} = sign(y_i - F_{m-1}(X_i))$\n",
    "3. 使用 LAE 时训练与预测是**不一致**的，训练时使用的是残差符号，预测时需要用叶子节点上样本上的中位数(median)来预测。\n",
    "    \\begin{align}\n",
    "    \\rho_m &=     \\underset{\\rho}{\\arg\\min} \\sum_{i = 1}^{N} L(y_i,  F_{m-1}(X_i) + \\rho h(X_i;\\mathbf{a}_m)) \\\\\n",
    "    &= \\underset{\\rho}{\\arg\\min} \\sum_{i = 1}^{N}|{y_i} - F_{m-1}(X_i) - \\rho h(X_i;\\mathbf{a}_m))|\n",
    "    \\end{align}\n",
    "4. 模型迭代更新: $F_{m}(x)=F_{{m-1}}(x)+\\rho_{m}h(x;\\mathbf{a}_m).$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Regression Trees （GBDT, 用是 CART 是回归树）\n",
    "\n",
    "回归树，考虑一个特殊情况，当基学习器是一个 J-叶节点回归树时，我们可以使用一个更通用的模型。\n",
    "\n",
    "假设每一个回归树的模型为: \n",
    "\n",
    "$$\n",
    "h(X;\\{b_j, R_j\\}_1^J) = \\sum_{j=1}^{J}b_j1(X\\in R_j)\n",
    "$$\n",
    "\n",
    "$\\{R_j\\}_1^J$ 是对目标值的一个划分， $1$ 是指示函数。则回归树的递归更新就变成了：\n",
    "$$\n",
    "F_m(X) = F_{m-1}(X) + \\rho_m\\sum_{j=1}^{J}b_{jm}1(X\\in R_{jm})\n",
    "$$\n",
    "\n",
    "当使用 **LSE** 来拟合残差时，\n",
    "\n",
    "$$\n",
    "b_{jm} = ave_{x_i \\in R_{jm}} \\tilde{y_i}\n",
    "$$\n",
    "\n",
    "就对应回归树上叶子结点残差的均值。即更新公式可以写为：\n",
    "\n",
    "\n",
    "$$\n",
    "F_m(X) = F_{m-1}(X) + \\sum_{j=1}^{J}\\gamma_{jm}1(X\\in R_{jm})\n",
    "$$\n",
    "\n",
    "其中：$\\gamma_{jm} = \\rho_m b_{jm}$\n",
    "\n",
    "\n",
    "当使用 **LAE** 来拟合残差时，\n",
    "\n",
    "$$\n",
    "\\gamma_{jm} = median_{x_i \\in R_{jm}}\\{y_i - F_{m-1}(X_i)\\}\n",
    "$$\n",
    "\n",
    "就对应回归树上叶子上残差的中位数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以给出当使用 LAE 时提升树的算法：\n",
    "\n",
    "<img src=\"./images/LAD.png\" width=\"360px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Huger 损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Two-class LR and Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "搞清楚上面对应的损失函数，对应残差的计算方法，模型更新方法，就可写出我们的模型了。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
