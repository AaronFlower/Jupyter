{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 决策树\n",
    "### Splitting Datasets one feature at a time: Decision Tree\n",
    "决策树分类器就像带有终止块的流程图，终止块表示分类结果。开始处理数据集时，我们首先需要测量数据集合中数据的不一致性，也就是熵(*Entropy*)，然后寻找最优的特征划分数据集，直到数据集中的所有数据属于同一个分类。\n",
    "### 熵与信息增益(Entropy and Information Gain) \n",
    "#### 定义\n",
    ">通常，一个信源发送出什么符号是不确定的，衡量它可以根据其出现的概率来度量。\n",
    "\n",
    ">概率大，出现机会多，不确定性小；反之就大。不确定性函数 f 是概率 P 的单调递降函数，即概率越大，不确定性越小；反之亦然。\n",
    "\n",
    ">假设两个符号(信源)出现的概率是 $p_1, p_2$, 不确定函数为  $ f $ ，则两个独立符号所产生的不确定性应等于各自不确定性之和，即$f(p_1，p_2)=f(p_1)+f(p_1)$\n",
    "这称为可加性。同时满足这两个条件的函数$f$是对数函数，即 \n",
    ">>$f(p_i) = \\log\\frac1p_i = -\\log{p_i}$, 式中对数一般取2为底，单位为比特。\n",
    "\n",
    "> 所以我们把符号 $x_i$ 的*信息定义*为：$l(x_i) = -\\log_2{p(x_i)}$, 其中 $ p(x_i) $ 是符号 $x_i$ 出现的概率。\n",
    "\n",
    "> 这时，信源的平均不确定性应当为单个符号不确定性$-log_2{P_i}$的统计平均值（E），可称为*信息熵* ([香农熵, Shannon entropy](https://youtu.be/R4OlXb9aTvQ))，即\n",
    ">> $ H = - \\sum_{i=1}^n p(x_i)\\log_2{p(x_i)} $\n",
    "\n",
    "#### 信息增益\n",
    "> 信息增益是熵的减少或者是数据无序度的减少。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 构造决策树\n",
    "Ex. 海洋生物数据，两个特征：\n",
    "\n",
    "1. no surfacing; \n",
    "2. flippers\n",
    "\n",
    "Order| No Surfacing| flippers | fish\n",
    "---|---|---|---|\n",
    " 1| 1 | 1| y\n",
    " 2| 1 | 1| y\n",
    " 3| 1 | 0| n\n",
    " 4| 0 | 1| n\n",
    " 5| 0 | 1| n\n",
    " \n",
    "#### 1. 计算香农熵(熵越高，则混合的数据也就越多)\n",
    " \\begin{align}\n",
    " H & = -p(x_y) * \\log_2{p(x_y)}   -p(x_n) * \\log_2{p(x_n)}\n",
    "   \\\\ & = -(\\frac{2}{5} *  log_2{\\frac{2}{5}}) - -(\\frac{3}{5} *  log_2{\\frac{3}{5}})\n",
    "   \\\\ &\\approx 0.5288 + 0.4422 \\\\ &= 0.971\n",
    " \\end{align}\n",
    " \n",
    "#### 2. 按照获取最大信息增益的方法划分数据集(第一轮)\n",
    "分别根据不同的特征来确定数据集的划分，用最大信息增益的特征来划分。\n",
    "1. 以 no-surfacing 特征来尝试分类：\n",
    "```\n",
    "    feature = 'no surfacing',    value = 1 : [1, y], [1, y], [0, n]\n",
    "                                 value = 0 : [1, no], [1, no]\n",
    "```\n",
    "则新的熵为 $h_1$ 与信息增益 $g_1$：\n",
    "\\begin{align}\n",
    "h_1 &= \\frac{3}{5}* (-\\frac{2}{3} * \\log_2{\\frac{2}{3}}  -\\frac{1}{3} * \\log_2{\\frac{1}{3}}) + \\frac{2}{5} * (-\\log_2{1})\n",
    "\\\\ &\\approx \\frac{3}{5} * (0.39 + 0.528) \\\\ &= 0.5508\n",
    "\\\\ \\\\ g_1 &= H - h_1 = 0.971 - 0.5508 = 0.4202\n",
    "\\end{align}\n",
    "\n",
    "2. 以 flippers 第二个特征来尝试分类：\n",
    "```\n",
    "    feature = 'flippers',       value = 1: [1, y], [1,y], [0, n], [0, n]\n",
    "                                value = 0: [1, no]\n",
    "```\n",
    "则新的熵为 $h_2$ 与信息增益 $g_2$:\n",
    "\\begin{align}\n",
    "h_2 &= \\frac{4}{5}(-\\frac{1}{2} * \\log_2{\\frac{1}{2}} - \\frac{1}{2} * \\log_2{\\frac{1}{2}}) + \\frac{1}{5}(-\\log_2{1})\n",
    "\\\\ & = 0.8 \n",
    "\\\\ \\\\ g_2 &= H - h_2 = 0.971 - 0.8 = 0.170951\n",
    "\\end{align}\n",
    "\n",
    "根据最大的信息增益来划分数据集，即根据第一个特征来划分：\n",
    "<img src=\"./decisionTrees01.svg\" />\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 递归按照获取最大信息增益的方法划分数据集(第二轮)¶\n",
    "即对还需要划分的子树进行划分，待划分的子树就只有右子树了。\n",
    "其基本熵为：\n",
    "\\begin{align}\n",
    "H = - \\frac{2}{3} * log_2{\\frac{2}{3}} - \\frac{1}{3} * log_2{\\frac{1}{3}} \\approx 0.6365\n",
    "\\end{align}\n",
    "该子树就只有一个特征值 flippers 了, 根据 flippers 来进行划分。\n",
    "```\n",
    "    features = flippers, value = 1: [1, y], [1, y]\n",
    "                         value = 0: [0, n]\n",
    "```\n",
    "新的划分熵 $h_1$ 及信息增益 $g_1$:\n",
    "\\begin{align}\n",
    "h_1 &= \\frac{2}{3} * (-log_2{1}) + \\frac{1}{3} * (-log_2{1}) = 0\n",
    "\\\\ g_1 & = 0.6365 - 0 = 0.6365\n",
    "\\end{align}\n",
    "无更多信息增益，可以直接划分。\n",
    "\n",
    "#### 递归结束的条件\n",
    "决策树递归结束的条件是：程序遍历完所有划分数据集的属性，或者每个分支下的所有实例都具有相同的类。\n",
    "\n",
    "#### 最终生成的决策树\n",
    "<img src=\"./decisionTrees.svg\" />\n",
    "\n",
    "#### 其它\n",
    "\n",
    "如果数据集已经处理了所有属性，但是类标签依然不是惟一，即叶子节点还是可以再分的，此时我们需要决定如何定义该叶子节点，在这种情况下通常会采用多数表决的方法决定该叶子节点的分类。\n",
    "\n",
    "后继还会介绍其它决策树算法，如 C4.5 和 CART，这些算法并不总是在每次划分分组时都会消耗特征。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 算法实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "决策树算法实现\n",
    "'''\n",
    "from numpy import *\n",
    "from math import log\n",
    "import operator\n",
    "\n",
    "\n",
    "# 计算香农熵\n",
    "def calcShannonEntropy(dataSet):\n",
    "    numEntries = len(dataSet)\n",
    "    labelCounts = {}\n",
    "    for vec in dataSet:\n",
    "        label = vec[-1]\n",
    "        labelCounts[label] = labelCounts.get(label, 0) + 1\n",
    "    \n",
    "    shannonEntropy = 0.0\n",
    "    for label in labelCounts:\n",
    "        probability = float(labelCounts[label]) / numEntries\n",
    "        shannonEntropy -= probability * log(probability, 2)\n",
    "    return shannonEntropy\n",
    "\n",
    "# 根据特征划分数据集\n",
    "def splitDataSet(dataSet, fAxis, value):\n",
    "    retDat = []\n",
    "    for vec in dataSet:\n",
    "        if vec[fAxis] == value:\n",
    "            tmp = vec.copy() # 等价于 tmp = vec[:fAxis].extend(vec[fAxis+1:])\n",
    "            del tmp[fAxis]\n",
    "            retDat.append(tmp)\n",
    "    return retDat\n",
    "\n",
    "# 选择最优的数据集划分特征\n",
    "def chooseBestFeature(dataSet):\n",
    "    numDs = float(len(dataSet))\n",
    "    numFeatures = len(dataSet[0]) - 1\n",
    "    baseEntropy = calcShannonEntropy(dataSet)\n",
    "    bestInfoGain = 0.0; bestFeature = -1\n",
    "    for i in range(numFeatures):\n",
    "        values = [example[i] for example in dataSet]\n",
    "        valuesSet = set(values)\n",
    "        newEntropy = 0.0\n",
    "        for v in valuesSet:\n",
    "            subDataV = splitDataSet(dataSet, i, v)\n",
    "            prop = float(len(subDataV)) / numDs\n",
    "            newEntropy += prop * calcShannonEntropy(subDataV)\n",
    "            \n",
    "        infoGain = baseEntropy - newEntropy\n",
    "        print ('feature ', i, 'infoGain: ', infoGain)\n",
    "        if infoGain >= bestInfoGain:\n",
    "            bestFeature = i\n",
    "            bestInfoGain = infoGain\n",
    "    return bestFeature\n",
    "\n",
    "# 对于未能完全划分的叶子节点根据投票来获取分类\n",
    "def cleafMajorityCount(leafList):\n",
    "    labelCount = {}\n",
    "    for vote in leafList:\n",
    "        labelCount[vote] = labelCount.get(vote, 0) + 1\n",
    "    sortedCount = sorted(labelCount.items(), key = operator.itemgetter(1), reverse = True)\n",
    "    return sortedCount[0][0]\n",
    "\n",
    "# 递归创建决策树\n",
    "def createDecisionTree(dataSet, labels):\n",
    "    classList = [example[-1]  for example in dataSet]\n",
    "    # 如果类别完全相同则信上继续划分\n",
    "    if len(set(classList)) == len(dataSet):\n",
    "        return classList[0]\n",
    "    # 如果没有特征可用时，用投票算法来完成分类。\n",
    "    if len(dataSet[0]) == 1:\n",
    "        return cleafMajorityCount(classList)\n",
    "    \n",
    "    bestFeature = chooseBestFeature(dataSet)\n",
    "    bestFeatureLabel = labels[bestFeature]\n",
    "    myTree = {bestFeatureLabel: {}}\n",
    "    del labels[bestFeature]\n",
    "    values = [example[bestFeature] for example in dataSet]\n",
    "    valuesSet = set(values)\n",
    "    for value in valuesSet:\n",
    "        subLabels = labels[:]\n",
    "        myTree[bestFeatureLabel][value] = createDecisionTree(splitDataSet(dataSet, bestFeature, value), subLabels)\n",
    "    return myTree\n",
    "    \n",
    "            \n",
    "# 创建数据集\n",
    "def createDataSet():\n",
    "    dataSet = [\n",
    "        [1, 1, 'yes'],\n",
    "        [1, 1, 'yes'],\n",
    "        [1, 0, 'no'],\n",
    "        [0, 1, 'no'],\n",
    "        [0, 1, 'no']\n",
    "    ]\n",
    "    labels = ['no surfacing', 'flippers']\n",
    "    return dataSet, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 1, 'yes'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no']]\n",
      "Initial Shannon Entropy:  0.9709505944546686 \n",
      "\n",
      "[[1, 1, 'maybe'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no']]\n",
      "Add another label 1.3709505944546687\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "# 熵函数测试\n",
    "myDat, labels = createDataSet()\n",
    "print (myDat)\n",
    "print ('Initial Shannon Entropy: ', calcShannonEntropy(myDat), '\\n')\n",
    "'''\n",
    "熵越高，则混合的数据也就越多，我们可以在数据集增加新的分类，观察熵的变化，这里增加一个新的 'maybe' 分类。\n",
    "'''\n",
    "testDat = copy.deepcopy(myDat)\n",
    "testDat[0][-1] = 'maybe'\n",
    "print (testDat)\n",
    "print ('Add another label', calcShannonEntropy(testDat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split by feature 0(no surfacing)\n",
      "[[1, 'no'], [1, 'no']] \t [[1, 'yes'], [1, 'yes'], [0, 'no']]\n",
      "Split by feature 1(flippers)\n",
      "[[1, 'no']] \t [[1, 'yes'], [1, 'yes'], [0, 'no'], [0, 'no']]\n"
     ]
    }
   ],
   "source": [
    "# 测试根据特征划分数据集函数 \n",
    "print ('Split by feature 0(no surfacing)')\n",
    "print(splitDataSet(myDat, 0, 0), '\\t', splitDataSet(myDat, 0, 1))\n",
    "print ('Split by feature 1(flippers)')\n",
    "print(splitDataSet(myDat, 1, 0), '\\t', splitDataSet(myDat, 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 1, 'yes'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no']]\n",
      "feature  0 infoGain:  0.4199730940219749\n",
      "feature  1 infoGain:  0.17095059445466854\n",
      "Initial Choose: 0 \n",
      "\n",
      "[[1, 'no'], [1, 'no']]\n",
      "feature  0 infoGain:  0.0\n",
      "Another Choose: 0\n"
     ]
    }
   ],
   "source": [
    "# 测试选择最优的划分特征\n",
    "print (myDat)\n",
    "print ('Initial Choose:', chooseBestFeature(myDat), '\\n')\n",
    "print (splitDataSet(myDat, 0, 0))\n",
    "print ('Another Choose:',chooseBestFeature(splitDataSet(myDat, 0, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'y'"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test cleafMajorityCount\n",
    "cleafMajorityCount(['y','y','n'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 'yes'], [1, 'yes'], [0, 'no']] ['flippers']\n",
      "feature  0 infoGain:  0.9182958340544896\n",
      "{'flippers': {0: 'no', 1: 'yes'}}\n",
      "\n",
      "feature  0 infoGain:  0.4199730940219749\n",
      "feature  1 infoGain:  0.17095059445466854\n",
      "feature  0 infoGain:  0.0\n",
      "feature  0 infoGain:  0.9182958340544896\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'no surfacing': {0: {'flippers': {1: 'no'}},\n",
       "  1: {'flippers': {0: 'no', 1: 'yes'}}}}"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获取最终的决策树\n",
    "myDat, labels = [[1, 'yes'], [1, 'yes'], [0, 'no']], ['flippers']\n",
    "print (myDat, labels)\n",
    "print (createDecisionTree(myDat, labels))\n",
    "print ()\n",
    "myDat, labels = createDataSet()\n",
    "createDecisionTree(myDat, labels)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
