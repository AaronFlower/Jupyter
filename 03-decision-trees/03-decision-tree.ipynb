{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 决策树\n",
    "### Splitting Datasets one feature at a time: Decision Tree\n",
    "决策树分类器就像带有终止块的流程图，终止块表示分类结果。\n",
    "\n",
    "### 1. 决策树学习基本算法\n",
    "\n",
    "**输入**：训练集 $ D = \\{(\\textbf{x}_1, y_1), (\\textbf{x}_2, y_2), ..., (\\textbf{x}_m, y_m)\\} $\n",
    "         , 特征集 $ A = \\{a_1, a_2, .., a_n\\} $\n",
    "         \n",
    "**算法函数**: `TreeGenerate(D, A)`\n",
    "```python\n",
    "    def treeGenerate(D, A):\n",
    "        node = generateNode\n",
    "        if (D 中样本同属于一类 C) then: (情况 1)\n",
    "            将 node 标记为 C 类叶结点，\n",
    "            return node\n",
    "        if (A = ∅ OR D 中的样本在 A 上的所有特征的取值都相同) then: (情况 2)\n",
    "            将 node 标记为叶结点，其类别标记为 D 中样本数最多的类 (即投票);\n",
    "            return node\n",
    "        \n",
    "        从 A 中选择最优的划分特征 a*;\n",
    "        \n",
    "        for a* 的每一个值 vi do:\n",
    "            为 node 生成一个分支；令 Dv 为 D 在a* 取值为 vi 时的样本子集。\n",
    "            if Dv = ∅ ：\n",
    "                将新的分支结点标记为叶子结点，基类别是 D 中样本最多的类 (投票) (情况 3)\n",
    "                return\n",
    "            else:\n",
    "                以 TreeGenerate(Dv, A - {a*}) 为分支节点进行递归创建。\n",
    "            end if\n",
    "        end for            \n",
    "```\n",
    "\n",
    "**输出**: 以 node 为根结点的一棵决策树。\n",
    "\n",
    "可以看出来，生成决策树是一个递归过程。有三种情况会导致递归返回：\n",
    "\n",
    "1. 当前结点包含的样本全属于同一个类别，无需要划分。\n",
    "2. 当前特征集为空，或者所有样本在所有属性上的取值相同，无法划分。 （以当前结点的样本投票）\n",
    "3. 当前结点包含的样本集合为空，不能划分。（以父节点的样本投票）\n",
    "\n",
    "### 2. 划分选择\n",
    "\n",
    "决策树最关键的问题是怎么选择最优的划分。一般而言，随着划分过程的不断进行，我们希望决策树的分支节点所包含的样本尽可能属于同一类别，即结点的纯度(purity) 越来越高。\n",
    "#### 2.1 使用信息增益划分\n",
    "\n",
    "熵是度量数据集中数据无序度的一种方法。\n",
    "##### 2.1.1 熵与信息增益(Entropy and Information Gain) \n",
    "#####  定义\n",
    ">通常，一个信源发送出什么符号是不确定的，衡量它可以根据其出现的概率来度量。\n",
    "\n",
    ">概率大，出现机会多，不确定性小；反之就大。不确定性函数 f 是概率 P 的单调递降函数，即概率越大，不确定性越小；反之亦然。\n",
    "\n",
    ">假设两个符号(信源)出现的概率是 $p_1, p_2$, 不确定函数为  $ f $ ，则两个独立符号所产生的不确定性应等于各自不确定性之和，即$f(p_1，p_2)=f(p_1)+f(p_1)$\n",
    "这称为可加性。同时满足这两个条件的函数$f$是对数函数，即 \n",
    ">>$f(p_i) = \\log\\frac1p_i = -\\log{p_i}$, 式中对数一般取2为底，单位为比特。\n",
    "\n",
    "> 所以我们把符号 $x_i$ 的**自信息**定义为：$l(x_i) = -\\log_2{p(x_i)}$, 其中 $ p(x_i) $ 是符号 $x_i$ 出现的概率。\n",
    "\n",
    "> 这时，信源的平均不确定性应当为单个符号不确定性$-log_2{P_i}$的统计平均值（E），可称为*信息熵* ([香农熵, Shannon entropy](https://youtu.be/R4OlXb9aTvQ))，即香农熵是样本空间内所有符号信息的 **期望**。\n",
    ">> $ H = - \\sum_{i=1}^n p(x_i)\\log_2{p(x_i)} $\n",
    "\n",
    "熵越大，则说明变量的不确定性越大，无序度越大。而熵越小，则表明数据集的纯度越高。\n",
    "\n",
    "##### 信息增益\n",
    "> 信息增益是熵的减少或者是数据无序度的减少。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假定样本集合 D 中，第 k 类样本所占的比例为 $p_k （k = 1, 2, ...|y|）$, 则 D 的信息熵为:\n",
    "\n",
    "$$\n",
    "Ent(D) = -\\sum_{k = 1}^{|y|} {p_k} log{p_k}\n",
    "$$\n",
    "\n",
    "Ent(D) 越小，其纯度越高。越大则越无序。\n",
    "\n",
    "假设 **离散** 属性 $a$ 有 V 个可能的取值 $\\{a^1, a^2, ..., a^V \\}$。 若使用 a 来对样本集 D 进行划分，则会产生 V 个分支节点。假设 $D_v$ 表示在 a 在取值 $a^v$ 时的所有样本，那么我们可以计算出在使用 a 对样本集 D 进行划分时所获得的 『信息增益(information gain)』。\n",
    "\n",
    "$$\n",
    "    Gain(D, a) = Ent(D) - \\sum_{v=1}^{V} \\frac{|D_v|}{|D|}Ent(D_v)\n",
    "$$\n",
    "\n",
    "一般而言，信息增益越大，则意味着使用属性 a 来进行划分所获得的『纯度』提升越大。所以划分属性的选择如下：\n",
    "\n",
    "$$\n",
    "a_* = arg \\space max \\space Gain(D, a), a \\in A\n",
    "$$\n",
    "\n",
    "著名的 **ID3 (Iterative Dichotomiser)** 就是以信息增益来进行划分的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 使用增益率来进行划分\n",
    "在使用信息增益来进行划分时，如果样本的编号作为一个特性，那么该特征的信息增益一定是最大的。那么使用该特征进行划分，那就会生成和样本集一样多的分支，而这些分支结点只有一个样本，显然纯度已经最大了，不能进行划分了。这样决策树显然不具有泛化能力了，无法对新的样本进行有效的预测。\n",
    "\n",
    "实际上，信息增益准则对可取值数目较多的属性有所偏好，为了减少这种偏好，我们可以使用 **增益率(gain ratio)** 来选择最优划分属性。增益率的定义为：\n",
    "\n",
    "$$\n",
    "Gain\\_ratio(D, a) = \\frac{Gain(D, a)}{IV(a)}\n",
    "$$\n",
    "\n",
    "其中：\n",
    "\n",
    "$$\n",
    "IV(a) = -\\sum_{v=1}^{V}\\frac{|D^v|}{|D|}log(\\frac{|D^v|}{|D|})\n",
    "$$\n",
    "\n",
    "IV, 称为**固有值 (intrinsic value)**。属性 a 取值的可能数目越多则 $IV(a)$ 的值就越大。增益率就是让其与 $IV(a)$ 成反比。\n",
    "\n",
    "著名的 **C4.5 (Iterative Dichotomiser)** 就是以增益率来进行划分的。\n",
    "\n",
    "\n",
    "\n",
    "**Note**： **信息增益**偏好取值多的属性，而**信息增益率**准则又偏好于取值少的属性。所以实现中，可以用一个启发式的搜索，即先从候选划分属性中找出信息增益高于平均水平的属性，现从中选择增益率最好的。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 使用基尼指数来划分\n",
    "**CART** 决策树使用『基尼指数』(Gini Index) 来选择划分属性。数据集 D 的纯度可以用基尼值来度量：\n",
    "\n",
    "$$\n",
    "Gini(D) = \\sum_{k=1}^{|y|}\\sum_{k' \\neq k} p_k p_{k'} = \\sum_{k=1}^{|y|}p_k(1 - p_k)  =  1 - \\sum_{k=1}^{|y|}p_k^2\n",
    "$$\n",
    "\n",
    "上式一个简单的推导，注意: $ \\sum_{k=1}^{|y|}p_k = 1$.\n",
    "\n",
    "\\begin{align}\n",
    "Gini(D) &= \\sum_{k=1}^{|y|}\\sum_{k' \\neq k} p_k p_{k'}\\\\\n",
    "&= p_1 \\sum_{k' \\neq 1} p_k' + p_2 \\sum_{k' \\neq 2} p_k' + \\cdots +  p_{|y|} \\sum_{k' \\neq |y|} p_k' \\\\\n",
    "&= p_1(1 - p_1) + p_2(1 - p_2) + \\cdots + p_{|y|}(1 - p_{|y|}) \\\\\n",
    "&= \\sum_{k=1}^{|y|}p_k(1 - p_k) \\\\\n",
    "&= \\sum_{k=1}^{|y|}(p_k - p_k^2) \\\\\n",
    "&= \\sum_{k=1}^{|y|}p_k - \\sum_{k=1}^{|y|}p_k^2 \\\\\n",
    "&= 1 - \\sum_{k=1}^{|y|}p_k^2\n",
    "\\end{align}\n",
    "\n",
    "直观来说，Gini(D) 反映了从数据集 D 中随机制取两个样本，其类别标记不一致的概率，因此， Gini(D) 越小，则数据集 D 的纯度越高。\n",
    "\n",
    "而属性 a 的基尼指数定义为:\n",
    "\n",
    "$$\n",
    "Gini\\_index(D, a) = \\sum_{v=1}^{V} \\frac{D^v}{D}Gini(D^v)\n",
    "$$\n",
    "选择最优的划分属性，则使用划分全基尼指数**最小**的属性。\n",
    "\n",
    "$$\n",
    "a_* = arg \\space min \\space Gini\\_index(D, a), a \\in A\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 方差缩减（Variance Reduction) \n",
    "\n",
    "在 ML 书中一般只介绍了信息熵和 Gini。但是还有一个常用的划分方法那就是**方差缩减(Variance Reduciton)**。 \n",
    "\n",
    "在 CART 中，当目标问题是回归问题时就会使用 Varaince Reduction 来生成回归树。因为应用其它划分方法需要先对特征值进行离散化才行。\n",
    "\n",
    "$$\n",
    "I_{V}(N)={\\frac {1}{|S|^{2}}}\\sum _{i\\in S}\\sum _{j\\in S}{\\frac {1}{2}}(x_{i}-x_{j})^{2}-\\left({\\frac {1}{|S_{t}|^{2}}}\\sum _{i\\in S_{t}}\\sum _{j\\in S_{t}}{\\frac {1}{2}}(x_{i}-x_{j})^{2}+{\\frac {1}{|S_{f}|^{2}}}\\sum _{i\\in S_{f}}\\sum _{j\\in S_{f}}{\\frac {1}{2}}(x_{i}-x_{j})^{2}\\right)\n",
    "$$\n",
    "\n",
    "上式中，$S, S_t, S_f$ 分别表示原始样本集，正(true)样本集，正(false)样本集。上面的公式只是写的复杂，其实就是元素之间相互减而已。最终是求是方差。\n",
    "\n",
    "可以简化为下面的公式：\n",
    "\n",
    "$$\n",
    "I_V(N) = Var(S) - \\big(\\frac{|S_t|}{|S|}Var(S_t) + \\frac{|S_f|}{|S|}Var(S_f)\\big)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CART 对回归树用方差缩减准则，而对分类树则用基尼指数(Gini index) 最小化准则，进行特征选择，生成二叉树。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 剪枝处理 (pruning)\n",
    "\n",
    "剪枝(pruning) 是决策树防止『过拟合』的主要手段。因为有时分支太多，会降低树的泛化能力导到过拟合。有两种剪枝策略：**预剪枝(prepruning), 后剪枝(postpruning)**.\n",
    "\n",
    "**预剪枝**是在构建决策树时，判断该划分是否提高了树的泛化能力，如果提升了则进行划分，如是没有提升则停止划分。\n",
    "\n",
    "**后剪枝**是在决策树生成之后，通过自底向上的对**非叶子**节点进行考察，若将子树替换成叶子节点能提升泛化能力，则进行替换。\n",
    "\n",
    "泛化能力的检测，需要我们使用验证集 (Validation Set）来对比剪枝前后的准确率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 连续值属性处理\n",
    "\n",
    "如果属性的取值是连续性的，那么连续性可以取值数目不再有限了。我们可以对连续值进行离散化。最简单的策略是采用 **二分法(bi-partion)** 对连续性进行处理。其策略如下：\n",
    "\n",
    "假设样本集 D 和 连续性属性 a, 对 a 的 n 个连续值进行递增排序， 记为: $\\{a^1, a^2, ..., a^n\\}$。我们需要找到一个划分点 t , 将划分后的两个集合 $D_t^-, D_t^+$ 的纯度达到最高, 其中 $D_t^-$ 是值不大于 t 的样本集合，而 $D_t^+$ 是大于 t 的样本集合。我们可取的划分点候选集合定义为：\n",
    "\n",
    "$$\n",
    "T_a = \\{\\frac{a^i + a^{i + 1}} {2} \\space | \\space 1 \\leqslant i \\leqslant n - 1\\}\n",
    "$$\n",
    "\n",
    "即把取两个相邻点的中间值作为划分点，若有 n 个取值，则有 $n -1$ 个划分点。然后我们从这个  $n - 1$ 个划分点找出一个最佳的划分点, 可以用离散值的试来考察候选划分点，如使用下面的式子：\n",
    "\n",
    "$$\n",
    "Gain(D, a) = \\max_{t \\in T_a} Gain(D, a, t) = \\max_{t \\in T_a} Ent(D) - \\sum_{\\lambda \\in \\{-,+\\}}\\frac{|D_t^\\lambda|}{|D|}Ent(D_t^\\lambda|)\n",
    "$$\n",
    "\n",
    "其中 Gain(D, a, t) 是 D 根据划分点 t 二分后的信息增益。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 缺失值处理\n",
    "\n",
    "当样本集中的样本在属性 a 上的值缺失时，我们将面临两个问题。\n",
    "\n",
    "1. 在样本值缺失时，怎么计算信息增益，来确定划分属性那？\n",
    "2. 在样本值缺件时，那么该样本应该属于那个子节点那？\n",
    "\n",
    "上面两个问题，我们需要计算属性 a 上所有可能的取值的 $a^v$ 在所有可能取值上的比例，即 $\\frac{|\\tilde{D}^v|}{|\\tilde{D}|}$。其中 $\\tilde{D}$ 是在属性 a 上无缺失值的集合。 即 $\\tilde{D} \\subset D$, $\\tilde{D} $ 在属性 a 上无缺失值。\n",
    "\n",
    "具体的参考周志华老师的《机器学习》。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 多变量决策树\n",
    "多变量决策树(multivariate decision tree) 也称为 『斜决策树』(oblique decision tree)， 在此类决策树中非叶子节点不再是对单个属性，而是对多个属性的线性组合来进行划分。即每一个非叶子节点是一个形如  $\\sum_{i=1}^{d} w_i a_i = t$ 的线性分类器, 即对 d 个属性找出一个合适的线性分类器，而不是找一个最优的划分属性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构造决策树\n",
    "Ex. 海洋生物数据，两个特征：\n",
    "\n",
    "1. no surfacing; \n",
    "2. flippers\n",
    "\n",
    "Order| No Surfacing| flippers | fish\n",
    "---|---|---|---|\n",
    " 1| 1 | 1| y\n",
    " 2| 1 | 1| y\n",
    " 3| 1 | 0| n\n",
    " 4| 0 | 1| n\n",
    " 5| 0 | 1| n\n",
    " \n",
    "#### 1. 计算香农熵(熵越高，则混合的数据也就越多)\n",
    " \\begin{align}\n",
    " H & = -p(x_y) * \\log_2{p(x_y)}   -p(x_n) * \\log_2{p(x_n)}\n",
    "   \\\\ & = -(\\frac{2}{5} *  log_2{\\frac{2}{5}}) - -(\\frac{3}{5} *  log_2{\\frac{3}{5}})\n",
    "   \\\\ &\\approx 0.5288 + 0.4422 \\\\ &= 0.971\n",
    " \\end{align}\n",
    " \n",
    "#### 2. 按照获取最大信息增益的方法划分数据集(第一轮)\n",
    "分别根据不同的特征来确定数据集的划分，用最大信息增益的特征来划分。\n",
    "1. 以 no-surfacing 特征来尝试分类：\n",
    "```\n",
    "    feature = 'no surfacing',    value = 1 : [1, y], [1, y], [0, n]\n",
    "                                 value = 0 : [1, no], [1, no]\n",
    "```\n",
    "\n",
    "则新的熵为 $h_1$ 与信息增益 $g_1$：\n",
    "\\begin{align}\n",
    "h_1 &= \\frac{3}{5}* (-\\frac{2}{3} * \\log_2{\\frac{2}{3}}  -\\frac{1}{3} * \\log_2{\\frac{1}{3}}) + \\frac{2}{5} * (-\\log_2{1})\n",
    "\\\\ &\\approx \\frac{3}{5} * (0.39 + 0.528) \\\\ &= 0.5508\n",
    "\\\\ \\\\ g_1 &= H - h_1 = 0.971 - 0.5508 = 0.4202\n",
    "\\end{align}\n",
    "\n",
    "2. 以 flippers 第二个特征来尝试分类：\n",
    "```\n",
    "    feature = 'flippers',       value = 1: [1, y], [1,y], [0, n], [0, n]\n",
    "                                value = 0: [1, no]\n",
    "```\n",
    "则新的熵为 $h_2$ 与信息增益 $g_2$:\n",
    "\\begin{align}\n",
    "h_2 &= \\frac{4}{5}(-\\frac{1}{2} * \\log_2{\\frac{1}{2}} - \\frac{1}{2} * \\log_2{\\frac{1}{2}}) + \\frac{1}{5}(-\\log_2{1})\n",
    "\\\\ & = 0.8 \n",
    "\\\\ \\\\ g_2 &= H - h_2 = 0.971 - 0.8 = 0.170951\n",
    "\\end{align}\n",
    "\n",
    "根据最大的信息增益来划分数据集，即根据第一个特征来划分：\n",
    "<img src=\"./decisionTrees01.svg\" />\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 递归按照获取最大信息增益的方法划分数据集(第二轮)¶\n",
    "即对还需要划分的子树进行划分，待划分的子树就只有右子树了。\n",
    "其基本熵为：\n",
    "\\begin{align}\n",
    "H = - \\frac{2}{3} * log_2{\\frac{2}{3}} - \\frac{1}{3} * log_2{\\frac{1}{3}} \\approx 0.6365\n",
    "\\end{align}\n",
    "该子树就只有一个特征值 flippers 了, 根据 flippers 来进行划分。\n",
    "```\n",
    "    features = flippers, value = 1: [1, y], [1, y]\n",
    "                         value = 0: [0, n]\n",
    "```\n",
    "新的划分熵 $h_1$ 及信息增益 $g_1$:\n",
    "\\begin{align}\n",
    "h_1 &= \\frac{2}{3} * (-log_2{1}) + \\frac{1}{3} * (-log_2{1}) = 0\n",
    "\\\\ g_1 & = 0.6365 - 0 = 0.6365\n",
    "\\end{align}\n",
    "无更多信息增益，可以直接划分。\n",
    "\n",
    "#### 递归结束的条件\n",
    "决策树递归结束的条件是：程序遍历完所有划分数据集的属性，或者每个分支下的所有实例都具有相同的类。\n",
    "\n",
    "#### 最终生成的决策树\n",
    "<img src=\"./decisionTrees.svg\" />\n",
    "\n",
    "#### 其它\n",
    "\n",
    "如果数据集已经处理了所有属性，但是类标签依然不是惟一，即叶子节点还是可以再分的，此时我们需要决定如何定义该叶子节点，在这种情况下通常会采用多数表决的方法决定该叶子节点的分类。\n",
    "\n",
    "后继还会介绍其它决策树算法，如 C4.5 和 CART，这些算法并不总是在每次划分分组时都会消耗特征。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 算法实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "决策树算法实现\n",
    "'''\n",
    "from numpy import *\n",
    "from math import log\n",
    "import operator\n",
    "\n",
    "\n",
    "# 计算香农熵\n",
    "def calcShannonEntropy(dataSet):\n",
    "    numEntries = len(dataSet)\n",
    "    labelCounts = {}\n",
    "    for vec in dataSet:\n",
    "        label = vec[-1]\n",
    "        labelCounts[label] = labelCounts.get(label, 0) + 1\n",
    "    \n",
    "    shannonEntropy = 0.0\n",
    "    for label in labelCounts:\n",
    "        probability = float(labelCounts[label]) / numEntries\n",
    "        shannonEntropy -= probability * log(probability, 2)\n",
    "    return shannonEntropy\n",
    "\n",
    "# 根据特征划分数据集\n",
    "def splitDataSet(dataSet, fAxis, value):\n",
    "    retDat = []\n",
    "    for vec in dataSet:\n",
    "        if vec[fAxis] == value:\n",
    "            tmp = vec.copy() # 等价于 tmp = vec[:fAxis].extend(vec[fAxis+1:])\n",
    "            del tmp[fAxis]\n",
    "            retDat.append(tmp)\n",
    "    return retDat\n",
    "\n",
    "# 选择最优的数据集划分特征\n",
    "def chooseBestFeature(dataSet):\n",
    "    numDs = float(len(dataSet))\n",
    "    numFeatures = len(dataSet[0]) - 1\n",
    "    baseEntropy = calcShannonEntropy(dataSet)\n",
    "    bestInfoGain = 0.0; bestFeature = -1\n",
    "    for i in range(numFeatures):\n",
    "        values = [example[i] for example in dataSet]\n",
    "        valuesSet = set(values)\n",
    "        newEntropy = 0.0\n",
    "        for v in valuesSet:\n",
    "            subDataV = splitDataSet(dataSet, i, v)\n",
    "            prop = float(len(subDataV)) / numDs\n",
    "            newEntropy += prop * calcShannonEntropy(subDataV)\n",
    "            \n",
    "        infoGain = baseEntropy - newEntropy\n",
    "        print ('feature ', i, 'infoGain: ', infoGain)\n",
    "        if infoGain >= bestInfoGain:\n",
    "            bestFeature = i\n",
    "            bestInfoGain = infoGain\n",
    "    return bestFeature\n",
    "\n",
    "# 对于未能完全划分的叶子节点根据投票来获取分类\n",
    "def cleafMajorityCount(leafList):\n",
    "    labelCount = {}\n",
    "    for vote in leafList:\n",
    "        labelCount[vote] = labelCount.get(vote, 0) + 1\n",
    "    sortedCount = sorted(labelCount.items(), key = operator.itemgetter(1), reverse = True)\n",
    "    return sortedCount[0][0]\n",
    "\n",
    "# 递归创建决策树\n",
    "def createDecisionTree(dataSet, labels):\n",
    "    classList = [example[-1]  for example in dataSet]\n",
    "    # 如果类别完全相同则停止继续划分\n",
    "    if classList.count(classList[0]) == len(classList):\n",
    "        return classList[0]\n",
    "    # 如果没有特征可用时，用投票算法来完成分类。\n",
    "    if len(dataSet[0]) == 1:\n",
    "        return cleafMajorityCount(classList)\n",
    "    \n",
    "    bestFeature = chooseBestFeature(dataSet)\n",
    "    bestFeatureLabel = labels[bestFeature]\n",
    "    myTree = {bestFeatureLabel: {}}\n",
    "    del labels[bestFeature]\n",
    "    values = [example[bestFeature] for example in dataSet]\n",
    "    valuesSet = set(values)\n",
    "    for value in valuesSet:\n",
    "        subLabels = labels[:]\n",
    "        myTree[bestFeatureLabel][value] = createDecisionTree(splitDataSet(dataSet, bestFeature, value), subLabels)\n",
    "    return myTree\n",
    "    \n",
    "            \n",
    "# 创建数据集\n",
    "def createDataSet():\n",
    "    dataSet = [\n",
    "        [1, 1, 'yes'],\n",
    "        [1, 1, 'yes'],\n",
    "        [1, 0, 'no'],\n",
    "        [0, 1, 'no'],\n",
    "        [0, 1, 'no']\n",
    "    ]\n",
    "    labels = ['no surfacing', 'flippers']\n",
    "    return dataSet, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 1, 'yes'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no']]\n",
      "Initial Shannon Entropy:  0.9709505944546686 \n",
      "\n",
      "[[1, 1, 'maybe'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no']]\n",
      "Add another label 1.3709505944546687\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "# 熵函数测试\n",
    "myDat, labels = createDataSet()\n",
    "print (myDat)\n",
    "print ('Initial Shannon Entropy: ', calcShannonEntropy(myDat), '\\n')\n",
    "'''\n",
    "熵越高，则混合的数据也就越多，我们可以在数据集增加新的分类，观察熵的变化，这里增加一个新的 'maybe' 分类。\n",
    "'''\n",
    "testDat = copy.deepcopy(myDat)\n",
    "testDat[0][-1] = 'maybe'\n",
    "print (testDat)\n",
    "print ('Add another label', calcShannonEntropy(testDat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split by feature 0(no surfacing)\n",
      "[[1, 'no'], [1, 'no']] \t [[1, 'yes'], [1, 'yes'], [0, 'no']]\n",
      "Split by feature 1(flippers)\n",
      "[[1, 'no']] \t [[1, 'yes'], [1, 'yes'], [0, 'no'], [0, 'no']]\n"
     ]
    }
   ],
   "source": [
    "# 测试根据特征划分数据集函数 \n",
    "print ('Split by feature 0(no surfacing)')\n",
    "print(splitDataSet(myDat, 0, 0), '\\t', splitDataSet(myDat, 0, 1))\n",
    "print ('Split by feature 1(flippers)')\n",
    "print(splitDataSet(myDat, 1, 0), '\\t', splitDataSet(myDat, 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 1, 'yes'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no']]\n",
      "feature  0 infoGain:  0.4199730940219749\n",
      "feature  1 infoGain:  0.17095059445466854\n",
      "Initial Choose: 0 \n",
      "\n",
      "[[1, 'no'], [1, 'no']]\n",
      "feature  0 infoGain:  0.0\n",
      "Another Choose: 0\n"
     ]
    }
   ],
   "source": [
    "# 测试选择最优的划分特征\n",
    "print (myDat)\n",
    "print ('Initial Choose:', chooseBestFeature(myDat), '\\n')\n",
    "print (splitDataSet(myDat, 0, 0))\n",
    "print ('Another Choose:',chooseBestFeature(splitDataSet(myDat, 0, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'y'"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test cleafMajorityCount\n",
    "cleafMajorityCount(['y','y','n'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 'yes'], [1, 'yes'], [0, 'no']] ['flippers']\n",
      "feature  0 infoGain:  0.9182958340544896\n",
      "{'flippers': {0: 'no', 1: 'yes'}}\n",
      "\n",
      "[[1, 'yes'], [1, 'yes']] ['flippers']\n",
      "yes\n",
      "\n",
      "feature  0 infoGain:  0.4199730940219749\n",
      "feature  1 infoGain:  0.17095059445466854\n",
      "feature  0 infoGain:  0.9182958340544896\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'no surfacing': {0: 'no', 1: {'flippers': {0: 'no', 1: 'yes'}}}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获取最终的决策树\n",
    "myDat, labels = [[1, 'yes'], [1, 'yes'], [0, 'no']], ['flippers']\n",
    "print (myDat, labels)\n",
    "print (createDecisionTree(myDat, labels))\n",
    "print ()\n",
    "myDat, labels = [[1, 'yes'], [1, 'yes']], ['flippers']\n",
    "print (myDat, labels)\n",
    "print (createDecisionTree(myDat, labels))\n",
    "print ()\n",
    "myDat, labels = createDataSet()\n",
    "createDecisionTree(myDat, labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用 Matplotlib 绘制决策树\n",
    "```javascript\n",
    "// 获取树的深度 js 版实现\n",
    "var tree = {'no surfacing': {0: 'no', 1: {'flippers': {0: 'no', 1: 'yes'}}}}\n",
    "var getTreeDepth = function(tree){\n",
    "  if (tree && typeof tree == 'object' &&  Object.keys(tree).length > 0) {\n",
    "    var keys = Object.keys(tree)\n",
    "    var subTree = tree[keys[0]]\n",
    "    var subKeys = Object.keys(subTree)\n",
    "    var leftDepth = 1 +  getTreeDepth(subTree[subKeys[0]])\n",
    "    var rightDepth = 1 +  getTreeDepth(subTree[subKeys[1]])\n",
    "    return leftDepth > rightDepth ? leftDepth : rightDepth\n",
    "  }\n",
    "  return 0\n",
    "}\n",
    "getTreeDepth(tree)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "绘制决策树\n",
    "'''\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 获取树的深度\n",
    "def getTreeDepth(tree):\n",
    "    if tree == None or (not isinstance(tree, dict)) or len(tree.keys()) == 0:\n",
    "        return 0\n",
    "    subTreeKey = list(tree)[0]\n",
    "    subTree = tree[subTreeKey]\n",
    "    subTreeKeys = subTree.keys()\n",
    "    maxSubTreeDepth = 0\n",
    "    for subKey in subTreeKeys:\n",
    "        depth = getTreeDepth(subTree[subKey])\n",
    "        if depth >= maxSubTreeDepth:\n",
    "            maxSubTreeDepth = depth\n",
    "    return 1 + maxSubTreeDepth\n",
    "\n",
    "# 获取树的最大宽度，即叶子节点个数。\n",
    "def getTreeWidth(tree):\n",
    "    if tree == None:\n",
    "        return 0\n",
    "    if isinstance(tree, str):\n",
    "        return 1\n",
    "    keys = list(tree.keys())\n",
    "    subTree = tree[keys[0]]\n",
    "    subTreeKeys = list(subTree)\n",
    "    numLeaves = 0\n",
    "    for subKey in subTreeKeys:\n",
    "        numLeaves += getTreeWidth(subTree[subKey])\n",
    "    return numLeaves\n",
    "\n",
    "# 判断是否是叶子节点\n",
    "def isLeaf(node):\n",
    "    return isinstance(node, dict) and len(node.keys()) == 1 and isinstance(node[list(node)[0]], str)\n",
    "\n",
    "# 获取树的根节点位置\n",
    "def getRootPos(tree, xStartPos = 0):\n",
    "    width = getTreeWidth(tree)\n",
    "    height = getTreeDepth(tree)\n",
    "    return (width - 1) * 4 + xStartPos, height * 2\n",
    "\n",
    "# 绘制树的节点\n",
    "def plotTreeNodes(tree, xStartPos = 0):\n",
    "    if isLeaf(tree):\n",
    "        print(xStartPos + 2, 2)\n",
    "        return\n",
    "    xPos, yPos = getRootPos(tree)\n",
    "    keys = list(tree.keys())\n",
    "    rootTree = tree[keys[0]]\n",
    "    subKeys = list(rootTree)\n",
    "    xStartPos = 0\n",
    "    for subKey in subKeys:\n",
    "        print ('subTree:', rootTree, subKey)\n",
    "        subTree = rootTree[subKey]\n",
    "        plotTreeNodes(subTree, xStartPos)\n",
    "        xStartPos, y = getRootPos(subTree)\n",
    "    print (xPos, yPos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 2 1\n",
      "4 3 3\n",
      "True False\n",
      "(12, 6) (8, 4) (8, 2)\n",
      "Plot tree\n",
      "subTree: {0: 'no', 1: {'flippers': {0: {'flippers': {0: 'no', 1: 'yes'}}, 1: 'yes'}}} 0\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-c7f556c6fbba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetRootPos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetRootPos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetRootPos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'Plot tree'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mplotTreeNodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-ae66559e1319>\u001b[0m in \u001b[0;36mplotTreeNodes\u001b[0;34m(tree, xStartPos)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'subTree:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrootTree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubKey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0msubTree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrootTree\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msubKey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mplotTreeNodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubTree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxStartPos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0mxStartPos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetRootPos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubTree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mxPos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myPos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-ae66559e1319>\u001b[0m in \u001b[0;36mplotTreeNodes\u001b[0;34m(tree, xStartPos)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mxPos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myPos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetRootPos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0mkeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[0mrootTree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0msubKeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrootTree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'keys'"
     ]
    }
   ],
   "source": [
    "tree1 = {'no surfacing': {0: 'no', 1: {'flippers': {0: {'flippers': {0: 'no', 1: 'yes'}}, 1: 'yes'}}}}\n",
    "tree2 = {'no surfacing': {0: 'no', 1: {'flippers': {0: 'no', 1: 'yes'}}}}\n",
    "tree3 = {'no surfacing': {0: 'no', 1: 'no', 2: 'yes'}}\n",
    "print(getTreeDepth(tree1), getTreeDepth(tree2), getTreeDepth(tree3))\n",
    "print(getTreeWidth(tree1), getTreeWidth(tree2), getTreeWidth(tree3))\n",
    "print(isLeaf({0: 'yes'}), isLeaf(tree3))\n",
    "print(getRootPos(tree1), getRootPos(tree2), getRootPos(tree3))\n",
    "print ('Plot tree')\n",
    "plotTreeNodes(tree1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
