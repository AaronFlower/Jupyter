{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FFM\n",
    "\n",
    "[Field-aware Factorization Machines for CTR Prediction](https://www.csie.ntu.edu.tw/~cjlin/papers/ffm.pdf) 论文。\n",
    "\n",
    "TL;DR;\n",
    "\n",
    "- 提出了 Field 的概念。\n",
    "- 训练时梯度计算使用 AdaGrad 方法\n",
    "- 参考[HOGWILD!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent](https://arxiv.org/abs/1106.5730) 给了一个并行设计的方案。\n",
    "\n",
    "关键词：CTR, 计算广告，FM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Introduciton\n",
    "\n",
    "假设有 $m$ 个样本，每个样本有 $n$ 个特征，即:$\\{(x_i, y_i)\\}_{i=1}^{m}, x_i \\in \\mathbb R^n$, 则我们的模型为：\n",
    "\n",
    "$$\n",
    "\\underset{w}{min} \\frac{\\lambda}{2}\\|\\mathbf{w}\\|_2^2 + \\sum_{i=1}^{m} \\log \\big(1 + exp(-y_i \\phi_{LM}(\\mathbf{w}, \\mathbf{x_i}))\\big) \\tag{1}\n",
    "$$\n",
    "\n",
    "在 $(1)$ 中， $\\lambda$ 是正则参数，在损失函数中我们考虑的是一个线性模型：\n",
    "\n",
    "$$\n",
    "\\phi_{LM}(\\mathbf{w}, \\mathbf{x}) = \\mathbf{w} \\cdot \\mathbf{x}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| clicked |\t unclicked |\t Publisher |\tAdvertiser |\n",
    "|----|-------|--------|------|\n",
    "|+80 |\t −20 |\t ESPN |\tNike |\n",
    "|+10 |\t −90 |\t ESPN |\tGucci |\n",
    "|+0 |\t −1 |\t \t ESPN |\tAdidas |\n",
    "|+15 |\t −85 |\t Vogue |\tNike |\n",
    "|+90 |\t −10 |\t Vogue |\tGucci |\n",
    "|+10 |\t −90 |\t Vogue |\tAdidas |\n",
    "|+85 |\t −15 |\t NBC |\tNike |\n",
    "|+0 |\t −0 |\t \t NBC |\tGucci |\n",
    "|+90 |\t −10 |\t NBC |\tAdidas |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上面表格的数据为例子，我们可以看到 Cucci 和 Vogue 组合有很高的点击率。但是普通的线性模型中很难能学到相应的特征关系。所以我们需要使用组合策略来对特征进行组合。\n",
    "\n",
    "常用的组合策略有， 多项式组合和FM。SVM 通过核也可以进行特征组合，但计算性能不高。\n",
    "\n",
    "### 2. POLY2 和 FM\n",
    "\n",
    "2 阶多项式的特征组合模型为：\n",
    "\n",
    "$$\n",
    "\\phi_{Poly2}(\\mathbf{w}, \\mathbf{x}) = \\sum_{j_1 = 1}^n \\sum_{j_2 = j_1 + 1}^n w_{h(j_1, j_2)} x_{j_1} x_{j_2}\n",
    "$$\n",
    "\n",
    "FM 模型通过学习隐向量来完成特征组合：\n",
    "\n",
    "$$\n",
    "\\phi_{FM}(\\mathbf{w}, \\mathbf{x}) = \\sum_{j_1 = 1}^n \\sum_{j_2 = j_1 + 1}^n (w_{j_1} w_{j_2}) x_{j_1} x_{j_2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. FFM\n",
    "\n",
    "FFM 参考了 PITF, 将 Field 的概念引入了进来。以下面的例子来说明：\n",
    "\n",
    "| clicked |\t Publisher(P) |\t Advertiser(A) |\tGender(G) |\n",
    "|----|-------|--------|------|\n",
    "|Yes |\t ESPN |\t Nike |\tMake |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果以 FM 为模型，则 $\\phi_{FM}(\\mathbf{w}, \\mathbf{x})$ 模型如下：\n",
    "\n",
    "$$\n",
    "\\mathbf{w}_{ESPN} \\cdot \\mathbf{w}_{Nike} + \\mathbf{w}_{ESPN} \\cdot \\mathbf{w}_{Male} + \\mathbf{w}_{Nike} \\cdot \\mathbf{w}_{Male} \n",
    "$$\n",
    "\n",
    "即每一个特征仅有一个隐向量与其它特征相关联。在 FFMs 中，每一个特征多个隐向量，这取决于特征所属于的 Field. \n",
    "\n",
    "在上面的例子中，我们的 $\\phi_{FMM}(\\mathbf{w}, \\mathbf{x})$ 模型如下：\n",
    "\n",
    "$$\n",
    "\\mathbf{w}_{ESPN, A} \\cdot \\mathbf{w}_{Nike, P} + \\mathbf{w}_{ESPN, G} \\cdot \\mathbf{w}_{Male, P} + \\mathbf{w}_{Nike, G} \\cdot \\mathbf{w}_{Male, A} \n",
    "$$\n",
    "\n",
    "用数学公式来描述，则用：\n",
    "\n",
    "$$\n",
    "\\phi_{FMM}(\\mathbf{w}, \\mathbf{x}) = \\sum_{j_1 = 1}^n \\sum_{j_2 = j_1 + 1}^n (w_{j_1, f_2} w_{j_2, f_1}) x_{j_1} x_{j_2} \\tag{2}\n",
    "$$\n",
    "\n",
    "**注意**，特征所属的 Field $f_1, f_2$ 需要交叉学习。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 模型求解\n",
    "\n",
    "上面的模型没有考虑偏置 $w_0$, 及一阶参数 $w_1, w_2, ..., w_m$。仅考虑组件特征参数的求解。对 $(2)$ 式进行求导，可以得到对应的梯度更新公式：\n",
    "\n",
    "$$\n",
    "\\mathbf{g}_{j_1, f_2} = \\nabla_{w_{j_1}, f_2} f(w) = \\lambda \\cdot w_{{j_1}, f_2} + \\kappa \\cdot w_{{j_2}, f_1} x_{j_1} x_{j_2} \\tag{5} \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{g}_{j_2, f_1} = \\nabla_{w_{j_2}, f_1} f(w) = \\lambda \\cdot w_{{j_2}, f_1} + \\kappa \\cdot w_{{j_1}, f_2} x_{j_1} x_{j_2} \\tag{6} \n",
    "$$\n",
    "\n",
    "其中：\n",
    "\n",
    "$$\n",
    "\\kappa = \\frac{\\partial \\log(1 + exp(-y \\phi_{FFM}(w, x))}{\\partial \\phi_{FFM}(w, x)} = - \\frac{y}{1 + exp(-y \\phi_{FFM}(w, x))}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用 AdaGrad 算法来提升性能，即自适应的来调节学习因子。则有梯度累加平方和为，当 $d =  1, ..., k$ 时，\n",
    "\n",
    "$$\n",
    "(G_{j_1, f_2})_d := (G_{j_1, f_2})_d + (g_{j_1, f_2})_d^2 \\tag{7}\n",
    "$$\n",
    "\n",
    "$$\n",
    "(G_{j_2, f_1})_d := (G_{j_2, f_1})_d + (g_{j_2, f_1})_d^2 \\tag{8}\n",
    "$$\n",
    "\n",
    "最终我们的梯度更新为：\n",
    "\n",
    "$$\n",
    "(w_{j_1, f_2})_d := (w_{j_1, f_2})_d - \\frac{\\eta}{\\sqrt{(G_{j_1, f_2})_d}} (g_{j_1, f_2})_d \\tag{9}\n",
    "$$\n",
    "\n",
    "$$\n",
    "(w_{j_2, f_1})_d := (w_{j_2, f_1})_d - \\frac{\\eta}{\\sqrt{(G_{j_2, f_1})_d}} (g_{j_2, f_1})_d \\tag{10}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在参数初始化时 $\\mathbb{w}$ 可以按 $[0, \\frac{1}{\\sqrt{k}}]$ 的均匀分布来初始化。 另外，在初始化时对特征的规一化处理也很重要。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最终的算法为：\n",
    "\n",
    "#### Algorithm 1 Training FFM using SG\n",
    "\n",
    "1. Let $G \\in R^{n \\times f \\times k}$ be a tensor of all ones.\n",
    "2. Run the following loop for t epoches\n",
    "3. for $i \\in \\{1,\\cdots, m \\}$ do\n",
    "    - Smaple a data point $(y, \\mathbf{x})$\n",
    "    - calculate $\\kappa$\n",
    "    - for $j_1 \\in \\{1, \\cdots , n\\} $ do\n",
    "        - for $j_2 \\in \\{j_1 +1 \\cdots , n\\}$ do\n",
    "            - calculate sub-gradient by (5) and (6)\n",
    "            - for $d \\in \\{1, \\cdots , k\\} $ do\n",
    "                - Update the gradient sum by (7) and (8)\n",
    "                - Update by (9) and (10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 并行化训练\n",
    "\n",
    "并行化训练请参考 参考[HOGWILD!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent](https://arxiv.org/abs/1106.5730) 并行设计的方案。\n",
    "\n",
    "对于上面的算法，我们可从第 3 步中开始并行计算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 添加 Field 信息\n",
    "\n",
    "通常在训练时都使用 LibSVM 提供的数据格式：\n",
    "\n",
    "```\n",
    "label feat1:val1 feat2:val2 ....\n",
    "```\n",
    "\n",
    "而对于 FFM，我们需要将特征所属的 Field 也加进来，加入 Field 后的格式为：\n",
    "\n",
    "```\n",
    "label field1:feat1:val1 field2:feat2:val2 ....\n",
    "```\n",
    "\n",
    "在加入 Field 的时候，我们需要考虑以下三种特征的处理方式：\n",
    "\n",
    "1. Categorical Features\n",
    "\n",
    "处理方法最简单\n",
    "\n",
    "2. Numerical Features\n",
    "\n",
    "3. Signle-field Features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 实验\n",
    "\n",
    "实验数据使用了 Criteo, Avazu 的数据。分别考虑了 $k, \\lambda, \\eta$ 的取值对模型的影响，以及并行计算时线程个数的选择。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
