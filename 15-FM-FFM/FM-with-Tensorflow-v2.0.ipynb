{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FM-with-Tensorflow-v2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example dummy data from Rendle 2010 \n",
    "# http://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf\n",
    "# Stolen from https://github.com/coreylynch/pyFM\n",
    "# Categorical variables (Users, Movies, Last Rated) have been one-hot-encoded \n",
    "x_data = np.matrix([\n",
    "#    Users  |     Movies     |    Movie Ratings   | Time | Last Movies Rated\n",
    "#   A  B  C | TI  NH  SW  ST | TI   NH   SW   ST  |      | TI  NH  SW  ST\n",
    "    [1, 0, 0,  1,  0,  0,  0,   0.3, 0.3, 0.3, 0,     13,   0,  0,  0,  0 ],\n",
    "    [1, 0, 0,  0,  1,  0,  0,   0.3, 0.3, 0.3, 0,     14,   1,  0,  0,  0 ],\n",
    "    [1, 0, 0,  0,  0,  1,  0,   0.3, 0.3, 0.3, 0,     16,   0,  1,  0,  0 ],\n",
    "    [0, 1, 0,  0,  0,  1,  0,   0,   0,   0.5, 0.5,   5,    0,  0,  0,  0 ],\n",
    "    [0, 1, 0,  0,  0,  0,  1,   0,   0,   0.5, 0.5,   8,    0,  0,  1,  0 ],\n",
    "    [0, 0, 1,  1,  0,  0,  0,   0.5, 0,   0.5, 0,     9,    0,  0,  0,  0 ],\n",
    "    [0, 0, 1,  0,  0,  1,  0,   0.5, 0,   0.5, 0,     12,   1,  0,  0,  0 ]\n",
    "])\n",
    "# ratings\n",
    "y_data = np.array([5, 3, 1, 4, 5, 1, 5], dtype=np.float)\n",
    "\n",
    "# Let's add an axis to make tensoflow happy.\n",
    "y_data.shape += (1, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "n_features = 16\n",
    "k_degree = 2\n",
    "batch_size = 1000\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((x_data, y_data)).repeat().shuffle(2000).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 16)\n",
      "(1000, 16) (1000, 1)\n"
     ]
    }
   ],
   "source": [
    "# 所以说，repeat 不用传具体的数，take 会帮我们解决问题。\n",
    "dataset10 = dataset.take(1)\n",
    "print(dataset.element_spec[0].shape)\n",
    "for x,y in dataset10:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0 = tf.Variable(np.zeros(1), name=\"bias\", dtype=tf.float64)\n",
    "W = tf.Variable(np.random.randn(n_features), name=\"weights\", dtype=tf.float64)\n",
    "V = tf.Variable(np.random.randn(n_features, k_degree), name=\"factors\", dtype=tf.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'bias:0' shape=(1,) dtype=float64, numpy=array([0.])>\n"
     ]
    }
   ],
   "source": [
    "print(w0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'weights:0' shape=(16,) dtype=float64, numpy=\n",
      "array([-0.4172011 , -0.73362866, -0.10311634,  1.2632677 ,  0.65341653,\n",
      "        0.31545171, -0.11232642, -1.99357185, -0.41388427, -1.15188704,\n",
      "        0.95121691, -1.01368156, -1.12185865, -0.74307947,  0.11064095,\n",
      "        0.67660343])>\n"
     ]
    }
   ],
   "source": [
    "print(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'factors:0' shape=(16, 2) dtype=float64, numpy=\n",
      "array([[ 0.18687186,  0.9558971 ],\n",
      "       [ 0.23523107,  1.15699869],\n",
      "       [ 1.17717205,  2.29807029],\n",
      "       [ 1.23333281, -0.19038046],\n",
      "       [ 0.10306188,  0.11041068],\n",
      "       [ 0.03360757,  0.66864629],\n",
      "       [-0.95631733, -1.02660211],\n",
      "       [ 0.81056354,  0.28159882],\n",
      "       [ 1.92104722, -0.71260133],\n",
      "       [-0.4882758 , -0.03113105],\n",
      "       [-0.39190392,  0.16720936],\n",
      "       [-0.58209245, -0.95043103],\n",
      "       [ 1.51312858,  0.31423097],\n",
      "       [ 0.45299798, -0.7138732 ],\n",
      "       [ 0.15699325,  0.95455789],\n",
      "       [ 0.05039035,  1.17311192]])>\n"
     ]
    }
   ],
   "source": [
    "print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    " optimizer = tf.keras.optimizers.Adagrad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_optimization(X, y):\n",
    "    # Wrap computation inside a GradientTape for automatic differentiation\n",
    "    with tf.GradientTape() as g:\n",
    "        pred = tf.add(w0, tf.reduce_sum(tf.multiply(X, W), 1, keepdims=True))\n",
    "        loss = tf.reduce_mean(tf.square(tf.subtract(y, pred)))\n",
    "    \n",
    "    # compute gradients\n",
    "    gradients = g.gradient(loss, [w0, W])\n",
    "    \n",
    "    # Update w0, W following gradients\n",
    "    optimizer.apply_gradients(zip(gradients, [w0, W]))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch i: 0, loss: 253.696 \n",
      "Epoch i: 1, loss: 268.306 \n",
      "Epoch i: 2, loss: 261.798 \n",
      "Epoch i: 3, loss: 263.841 \n",
      "Epoch i: 4, loss: 262.959 \n",
      "Epoch i: 5, loss: 260.340 \n",
      "Epoch i: 6, loss: 265.139 \n",
      "Epoch i: 7, loss: 266.931 \n",
      "Epoch i: 8, loss: 263.871 \n",
      "Epoch i: 9, loss: 267.577 \n"
     ]
    }
   ],
   "source": [
    "train_data = dataset.take(10)\n",
    "\n",
    "for i, data in enumerate(train_data):\n",
    "    X, y = data\n",
    "    loss = run_optimization(X, y)\n",
    "#     if i % 1000 == 0:\n",
    "    print(\"Epoch i: %d, loss: %.3f \" % (i, loss.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'bias:0' shape=(1,) dtype=float64, numpy=array([0.00504385])>\n",
      "<tf.Variable 'weights:0' shape=(16,) dtype=float64, numpy=\n",
      "array([-0.41213096, -0.72867341, -0.09808359,  1.26826951,  0.65853045,\n",
      "        0.32048822, -0.10731099, -1.98850929, -0.40881832, -1.14685238,\n",
      "        0.95616597, -1.00862184, -1.11671271, -0.73809859,  0.11565637,\n",
      "        0.67660343])>\n"
     ]
    }
   ],
   "source": [
    "print(w0)\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = tf.add(w0, tf.reduce_sum(tf.multiply(x_data, W), 1, keepdims=True))\n",
    "loss = tf.reduce_mean(tf.square(tf.subtract(y_data, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-13.31415547]\n",
      " [-16.04922908]\n",
      " [-18.02590087]\n",
      " [ -5.54159372]\n",
      " [ -8.87960207]\n",
      " [ -9.47004759]\n",
      " [-14.56040711]], shape=(7, 1), dtype=float64)\n",
      "tf.Tensor(262.311872875665, shape=(), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "print(y_pred)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LR, epochs = 100000, 最终 loss = 4.679 这真是坑呀。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model(object):\n",
    "    def __init__(self):\n",
    "        self.name = \"base\"\n",
    "        \n",
    "class lr_model(model):\n",
    "    def __init__(self):\n",
    "        self.name = 'lr'\n",
    "        \n",
    "class poly_model(model):\n",
    "    def __init__(self):\n",
    "        self.name = 'order-2-poly-model'\n",
    "    \n",
    "class fm_model(model):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 dataset, \n",
    "                 epochs, \n",
    "                 degree = 2,\n",
    "                 optimizer = None, \n",
    "                 verbosity=1):\n",
    "        self.name = 'fm'\n",
    "        if optimizer:\n",
    "            self.optimizer = optimizer\n",
    "        else:\n",
    "            self.optimizer = tf.optimizers.Adagrad(learning_rate=0.1)\n",
    "            \n",
    "        self.dataset = dataset\n",
    "        self.epochs  = epochs\n",
    "        self.degree = degree\n",
    "        self.verbosity = verbosity\n",
    "        \n",
    "        # Xn is the features.\n",
    "        for X, y in dataset.take(1):\n",
    "            self.Xm, self.Xn = X.shape\n",
    "            self.ym, self.yn = y.shape\n",
    "        \n",
    "        self.n_features = self.Xn\n",
    "        \n",
    "        self.w0 = tf.Variable(np.zeros(1), \n",
    "                              dtype=tf.float64, name=\"bias\")\n",
    "        self.W = tf.Variable(np.random.randn(self.n_features, 1), \n",
    "                             dtype=tf.float64, name=\"weights\")\n",
    "        self.V = tf.Variable(np.random.randn(self.n_features, self.degree), \n",
    "                             dtype=tf.float64, name=\"factors\")\n",
    "        \n",
    "        if self.verbosity:\n",
    "            print(self.w0)\n",
    "            print(self.W)\n",
    "            print(self.V)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        linear_terms = tf.add(self.w0, tf.reduce_sum(tf.matmul(X, self.W), axis=1, keepdims=True))\n",
    "        interactions = tf.multiply(0.5,\n",
    "                                   tf.reduce_sum(\n",
    "                                       tf.subtract(\n",
    "                                            tf.square(tf.matmul(X, self.V)),\n",
    "                                            tf.matmul(tf.square(X), tf.square(self.V))\n",
    "                                        ),\n",
    "                                       axis=1,\n",
    "                                       keepdims=True,\n",
    "                                    )\n",
    "                                  )\n",
    "        pred = linear_terms + interactions\n",
    "        return pred\n",
    "    \n",
    "    def loss(self, y_pred, y):\n",
    "        loss = tf.reduce_mean(tf.square(tf.subtract(y_pred, y)))\n",
    "        return loss\n",
    "        \n",
    "    \n",
    "    def train(self):   \n",
    "        dataset = self.dataset.take(self.epochs)\n",
    "        for i, data in enumerate(dataset):\n",
    "            with tf.GradientTape() as g:\n",
    "                y_pred = self.predict(X)\n",
    "                cost = self.loss(y_pred, y)\n",
    "                \n",
    "            gradients = g.gradient(cost, [self.w0, self.W, self.V])\n",
    "            self.optimizer.apply_gradients(zip(gradients, [self.w0, self.W, self.V]))\n",
    "            if self.verbosity:\n",
    "                print(\"Epoch %d, loss: %0.3f\" % (i + 1, loss.numpy()))\n",
    "        self.cost = cost\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(7.601638336981552e-18, shape=(), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "m = fm_model(dataset, epochs=1000, degree=2, verbosity=0)\n",
    "m.train()\n",
    "print(m.cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0 5.000000003279677\n",
      "3.0 2.9999999987619557\n",
      "1.0 0.9999999988176684\n",
      "4.0 3.999999999040022\n",
      "5.0 5.000000000502729\n",
      "1.0 0.9999999947654405\n",
      "5.0 5.0000000026476075\n"
     ]
    }
   ],
   "source": [
    "y_pred = m.predict(x_data)\n",
    "for y, y_hat in zip(y_data, y_pred):\n",
    "    print(y[0], y_hat.numpy()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面绝对过拟合了。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
