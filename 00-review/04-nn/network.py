# encoding: utf-8

'''
å®ç°éšæœºæ¢¯åº¦ä¸‹é™çš„ NN
'''

import random
import numpy as np

def sigmoid(z):
    '''
        compute the sigmoid function.
    '''
    return 1 / (1 + np.exp(-z))

def sigmoid_prime(z):
    '''
        Derivative of the sigmoid function
    '''
    f = sigmoid(z)
    return f * (1 - f)

class Network(object):

    def __init__(self, layer_sizes):
        '''
            åˆå§‹åŒ– Network, åˆå§‹åŒ–æ¯å±‚çš„æƒé‡çŸ©é˜µï¼Œåç½®å‘é‡.
            - layer_sizes : array-like, ç”¨äºæŒ‡å®šæ¯å±‚çš„ç¥ç»å…ƒä¸ªæ•°ã€‚
            å¦‚ï¼šlayer_sizes = [3, 4, 4, 1], åˆ™è¯´æ˜æœ‰ 4 å±‚ï¼Œæ¯å±‚çš„ç¥ç»å…ƒä¸ªæ•°åˆ†åˆ«
            ä¸ºï¼š3, 4, 4, 1.
        '''
        self.layer_sizes = layer_sizes
        self.num_layers = len(layer_sizes)

        self.biases = [np.random.randn(nLayer, 1) for nLayer in layer_sizes[1:]]
        self.weights = [
            np.random.randn(nLayer, nPreLayer)
            for nLayer, nPreLayer in zip(layer_sizes[1:], layer_sizes[:-1])
        ]

        return

    def FP(self, x):
        '''
            Forward Process, å‰å‘è®¡ç®—
        '''
        a = x
        for W, b in zip(self.weights, self.biases):
            z = np.dot(W, a) + b
            a = sigmoid(z)
        return a

    def train(self,
              train_data,
              epoches,
              batch_size,
              alpha,
              test_data = None):
        '''
            ä½¿ç”¨ mini-batch stochastic gradient descent æ¥è®­ç»ƒæ¨¡å‹
            - train_data : è®­ç»ƒæ•°æ®é›†
            - epoches: è¿­ä»£æ¬¡æ•°
            - batch_size: æ›´æ–°æ¢¯åº¦æ—¶æ¯æ¬¡æ‰€ç”¨çš„æ ·æœ¬æ•°
            - alpha: å­¦ä¹ å› å­
            - test_data: æµ‹è¯•æ•°æ®é›†
        '''
        num_train = len(train_data)
        if test_data:
            num_test = len(test_data)

        for i in range(epoches):
            random.shuffle(train_data)
            batches = [
                train_data[k : k + batch_size]
                for k in range(0, num_train, batch_size)
            ]

            for batch in batches:
                self.batch_GD(batch, alpha)

            if test_data:
                print("Epoch {0}: {1} / {2}".format(
                    i, self.evaluate(test_data), num_test
                ))
            else:
                print("Epock {0} complete".format(i))

        return

    def batch_GD(self, train_data, alpha):
        '''
            æ‰¹é‡æ¢¯åº¦ä¸‹é™, éœ€è¦ç”¨ BP æ¥è®¡ç®—æ¢¯åº¦
            - train_data: è®­ç»ƒæ•°æ®é›†
            - alpha: å­¦ä¹ å› å­
        '''
        nabla_b = [np.zeros(b.shape) for b in self.biases]
        nabla_w = [np.zeros(w.shape) for w in self.weights]

        # å¯¹æ•°æ®é›†ä¸­çš„æ¯ä¸ªæ ·æœ¬çš„è¿›è¡Œç´¯åŠ 
        for x, y in train_data:
            delta_nabla_b, delta_nabla_w = self.BP(x, y)
            nabla_b = [nb + dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]
            nabla_w = [nw + dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]

        am = alpha / len(train_data)
        self.weights = [w - am * nw for w, nw in zip(self.weights, nabla_w)]
        self.biases = [b - am * nb for b, nb in zip(self.biases, nabla_b)]

        return

    def BP(self, x, y):
        '''
            Backward Process
            - x: è®­ç»ƒæ ·æœ¬ç‰¹å¾
            - y: è®­ç»ƒæ ·æœ¬åˆ†ç±»
            Return: (nabla_b, nabla_w), å³ (ğ›b, ğ›w), å…¶æ¯å±‚çš„ b, w çš„ç»´åº¦ä¸€è‡´.
        '''
        nabla_b = [np.zeros(b.shape) for b in self.biases]
        nabla_w = [np.zeros(w.shape) for w in self.weights]

        # FP

        a = x
        cache_a = [x] # cache all the activations for all layer
        cache_z = []  # cache all the z   vectors for all layer

        for b, w in zip(self.biases, self.weights):
            z = np.dot(w, a) + b
            a = sigmoid(z)
            cache_z.append(z)
            cache_a.append(a)

        # Backward process Output layer delta
        delta = (a - y) * sigmoid_prime(z)

        # è®¡ç®—è¾“å‡ºå±‚çš„ ğ›b, ğ›w
        nabla_b[-1] = delta
        nabla_w[-1] = np.dot(delta, cache_a[-2].T)

        # è®¡ç®—éšè—å±‚çš„ (ğ›b, ğ›w)
        for i in range(2, self.num_layers):
            z = cache_z[-i]
            sp = sigmoid_prime(z)
            delta = np.dot(self.weights[-i + 1].T, delta) * sp
            nabla_b[-i] = delta
            nabla_w[-i] = np.dot(delta, cache_a[-i -1].T)

        return (nabla_b, nabla_w)

    def evaluate(self, test_data):
        '''
            è¿”å›æµ‹è¯•æ ·æœ¬çš„åˆ†ç±»æ•°
        '''
        test_results = [
            (np.argmax(self.FP(x)), np.argmax(y)) for (x, y) in test_data
        ]

        return sum(int(x == y) for (x, y) in test_results)
