{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分类与 Logistic 回归\n",
    "\n",
    "分类用来预测若干个离散值。目前我们仅关注二分分类 (binary classifcation) 即 y 只有两个值 0， 1。0，被称为负类 (negative class）； 1 被称为正类 (positive class)，或者有时被记为 \"-\" 和 “+” , 对于一个训练样本， 给定 $x^{(i)}$,  $y^{(i)}$ 也被称作为 label 标签。\n",
    "\n",
    "Logistic 回归就是来预测二分类 0, 1 的一种回归学习算法。Logistic 回归与一般的线性回归主要不同是选择假设函数不一样。\n",
    "\n",
    "### Logistic 回归\n",
    "\n",
    "改变我们的 **hypothesis** 形式， $h_\\theta(x) = g(\\theta^Tx) = \\frac{1}{1 + e^{-\\theta^Tx}}$ , 即 $ g(z) = \\frac{1}{1 + e^{-z}}$, $g(z)$  被称为 Logistic 函数或 Sigmoid 函数。 Sigmoid 函数有如下的性质: \n",
    "\n",
    "\\begin{align*}\n",
    "g'(z) &= \\frac{d}{dz}(\\frac{1}{1 + e^{-z}}) \n",
    "\\\\&= \\frac{1}{1 + e^{-z}} e^{-z}\n",
    "\\\\ &=\\frac{1}{1 + e^{-z}} (1- \\frac{1}{1 + e^{-z}})\n",
    "\\\\ &= g(z)(1-g(z))\n",
    "\\end{align*}\n",
    "\n",
    "给出样本的假设函数，怎样才能拟合出 $\\theta$ 那？尝试用概率知识对参数 $\\theta$ 进行极大似然估计。\n",
    "\n",
    "在极大似然估计之前，让我们先写出样本的概率密度函数。固定 $\\theta$, 在给出 $x^{(i)}$ 时， $y^{(i)}$  的离散概率分布为：\n",
    "\n",
    "\\begin{align*}\n",
    "f(y^{(i)} = 0 \\mid x^{(i)}; \\theta) &= 1 - h_\\theta(x)\n",
    "\\\\\n",
    "f(y^{(i)} = 1 \\mid x^{(i)}; \\theta) &= h_\\theta(x)\n",
    "\\\\\n",
    "f(y^{(i)}\\mid x^{(i)}; \\theta) &= h_\\theta(x)^ {y^{(i)}} (1 - h_\\theta(x))^{1 - y^{(i)}}\n",
    "\\end{align*}\n",
    "\n",
    "写出样本的联合概率函数:\n",
    "\n",
    "\\begin{equation*}\n",
    "L(\\theta) = L(Y\\mid X; \\theta) = \\prod_{i=1}^{m} h_\\theta(x)^ {y^{(i)}} (1 - h_\\theta(x))^{1 - y^{(i)}}\n",
    "\\end{equation*}\n",
    "\n",
    "则有：\n",
    "\n",
    "\\begin{equation*}\n",
    "\\ell(\\theta) = \\ln L(\\theta) = \\sum_{i=1}^{m} \\ln h_\\theta(x)^ {y^{(i)}} (1 - h_\\theta(x))^{1 - y^{(i)}}\n",
    "\\end{equation*}\n",
    "\n",
    "求出 $ \\ell(\\theta) $ 的**最大值**，我们可以使用**梯度上升**的方法来求得，即先随机初始化下 $\\theta$ ，然后利用梯度上升更新规则 $\\theta_j := \\theta_j + \\alpha \\frac{\\partial}{\\partial{\\theta_j}}\\ell({\\theta})$ 来更新  $\\theta$。\n",
    "\n",
    "我们需要求出 $\\frac{\\partial}{\\partial{\\theta_j}}\\ell({\\theta})$, 让我们假设仅有一个样本的情况，则：\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial}{\\partial{\\theta_j}}\\ell({\\theta}) &=  \\frac{\\partial}{\\partial{\\theta_j}} \\ln [h_\\theta(x)^y(1-h_\\theta(x))^(1-y)]\n",
    "\\\\ &= \\frac{\\partial}{\\partial{\\theta_j}} [y \\ln h_\\theta(x) + (1-y) \\ln (1 - h_\\theta(x))]\n",
    "\\\\ &= \\frac{\\partial}{\\partial{\\theta_j}} [y \\ln g(\\theta ^ Tx) + (1-y) \\ln (1 - g(\\theta ^ Tx))]\n",
    "\\\\ &= [y \\frac{1}{g(\\theta ^ Tx)} - (1-y) \\frac{1}{ (1 - g(\\theta ^ Tx)}] \\frac{\\partial}{\\partial{\\theta_j}}g(\\theta ^ Tx)\n",
    "\\\\ &= [y \\frac{1}{g(\\theta ^ Tx)} - (1-y) \\frac{1}{ (1 - g(\\theta ^ Tx)}] g(\\theta ^ Tx)(1 - g(\\theta ^ Tx) \\frac{\\partial}{\\partial{\\theta_j}} \\theta^Tx\n",
    "\\\\ &= [y (1 - g(\\theta ^ Tx) - (1-y)g(\\theta ^ Tx)] x_j\n",
    "\\\\ &= (y-h_\\theta(x))x_j\n",
    "\\end{align*}\n",
    "\n",
    "即有更新规则为:$\\theta_j := \\theta_j + \\alpha (y^{(i)}-h_\\theta(x^{(i)}))x_j^{(i)}$ 。看起来和我们的 LMS 类似，但是却不是同一个算法。因为现在我们的假设 $h_\\theta(x^{(i)})$ 是一个非线性函数。但是更新规则的相似是巧合吗？ 我们将在 GLM 广义线性模型中讨论这个话题。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
