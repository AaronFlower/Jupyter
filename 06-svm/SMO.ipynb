{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMO\n",
    "\n",
    "### 1. SVM 优化目标\n",
    "\n",
    "SVM 优化目标是一个 QP(二次规化，Quadratic Progamming) 问题。其基本形式如下：\n",
    "\n",
    "$$\n",
    "\\max_{\\textbf{⍺}} W(\\textbf{⍺}) = \\sum_{i = 1}^{m}\\alpha_i - \\frac{1}{2}\\sum_{i = 1}^{m}\\sum_{j = 1}^{m}y_iy_j\n",
    "\\textbf{k}(\\textbf{x}_i, \\textbf{x}_j)\\alpha_i\\alpha_j\n",
    "\\\\ s.t. \\space 0 \\leqslant \\alpha_i \\leqslant C, i = 1, .., m\n",
    "\\\\ \\sum_{i = 1}^{m}y_i\\alpha_i = 0\n",
    "\\tag 1\n",
    "$$\n",
    "\n",
    "(1)可以通过 SMO 来求解。当上面有 QP 满足 KKT 条件，并且 $Q_{ij} = y_iy_j\n",
    "\\textbf{k}(\\textbf{x}_i, \\textbf{x}_j)$ 矩阵是半正定时，则我们所得的解就是最优解。\n",
    "\n",
    "KKT 条件：\n",
    "\n",
    "$$\n",
    "\\alpha_i = 0 \t\\Rightarrow y_i f(\\textbf{x}_i) \\geqslant 1 \\\\\n",
    "0 < \\alpha_i < C \t\\Rightarrow y_i f(\\textbf{x}_i) = 1 \\\\\n",
    "\\alpha_i = C \t\\Rightarrow y_i f(\\textbf{x}_i) \\leqslant 1\n",
    "\\tag 2\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. SMO 原理\n",
    "\n",
    "SMO 的思想是将 QP 问题分解成若干个小 QP 问题(Sub-QP) 问题。而这些 Sub-QP 问题通过分析就可求得其解。 SMO 在稀疏数据上表现非常好，特别适合文本分类。\n",
    "\n",
    "在执行 SMO 算法时，可以对每个样本验证 KKT 条件，如果条件满足则就把样本加进来直到收敛。\n",
    "\n",
    "SMO 每次只选择 2 个拉格朗日乘子来进行一起优化。SMO 算法有三个要点：\n",
    "1. 如何求解更新所选的拉格朗日乘子 $\\alpha_1, \\alpha_2$。\n",
    "2. 如何选择两个拉格朗日乘子 $\\alpha_1, \\alpha_2$。\n",
    "3. 如果更新 b."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 如何求解两个拉格朗日乘子 $\\alpha_1, \\alpha_2$\n",
    "\n",
    "<img src=\"smo-alpha.png\" width=\"300\">\n",
    "\n",
    "$$\n",
    "{\\alpha}_2^{new, clipped} =\\begin{cases}\n",
    "               H, \\space {\\alpha}_2^{new} \\geqslant H \\\\\n",
    "               {\\alpha}_2^{new}, L < a_2^{new} < H \\\\\n",
    "               L, \\space {\\alpha}_2^{new} \\leqslant L \\\\\n",
    "            \\end{cases}\n",
    "$$\n",
    "\n",
    "其中：\n",
    "\n",
    "$$\n",
    "{\\alpha}_2^{new} = {\\alpha}_2^{old} - \\frac{y_2(E_1 - E_2)}{\\eta} \\\\\n",
    "E_i = f^{old}(\\textbf{x}_i) - y_i\n",
    "$$\n",
    "\n",
    "当 $y_i = y_j$ 时：\n",
    "$$\n",
    "L = max(0, {\\alpha}_1^{old} + {\\alpha}_2^{old} - C) \\\\\n",
    "H = min(C, {\\alpha}_1^{old} + {\\alpha}_2^{old})\n",
    "$$\n",
    "\n",
    "当 $y_i \\neq y_j$ 时：\n",
    "\n",
    "$$\n",
    "L = max(0, {\\alpha}_2^{old} - {\\alpha}_1^{old}) \\\\\n",
    "H = min(C, C + {\\alpha}_2^{old} - {\\alpha}_1^{old})\n",
    "$$\n",
    "\n",
    "而别一个乘子的计算方法为：\n",
    "\n",
    "$$\n",
    "{\\alpha}_1^{new} = {\\alpha}_1^{old} + y_1 y_2({\\alpha}_2^{old} - {\\alpha}_2^{new, clipped})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 启发式搜索 ${\\alpha}_1, {\\alpha}_2$\n",
    "\n",
    "两个乘子的选择策略是不一样的。\n",
    "\n",
    "第一个：寻找一个违反 KKT 条件的样本，如果违反了 KKT 条件，那么这个样本对就的 $\\alpha_i$ 就可拿来做优化。我们也不必遍历所有样本，我们只需要找到 $ 0 < \\alpha_i < C$ 的乘子，并且该乘子对应的样本违反了 KKT 条件。直到所有乘子都满足 KKT  条件，允许的误差是 $\\epsilon$。\n",
    "\n",
    "第二个：基于第一相样本的误差 $E_1$， 寻找一个令 $|E_2 - E_1|$ 最大的样本，这样更新的速度最快(big step)， 最大化步长。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 计算 b 及误差缓存更新\n",
    "\n",
    "第一步都需要重新计算 b，这样才能让所有优化的样本满足 KKT 条件。我们需要计算两个 threshold , 即 b1 和 b2。然后根据条件来选择 b1, b2 或两个的平均值。\n",
    "计算的公式：\n",
    "\n",
    "$$\n",
    "b_1 = E_1 + y_1(\\alpha_1^{new} - \\alpha_1^{old})\\textbf{k}(\\textbf{x}_1, \\textbf{x}_1) + y_2(\\alpha_2^{new,clipped} - \\alpha_2^{old})\\textbf{k}(\\textbf{x}_1, \\textbf{x}_2) + b^{old} \\\\\n",
    "b_2 = E_2 + y_1(\\alpha_1^{new} - \\alpha_1^{old})\\textbf{k}(\\textbf{x}_1, \\textbf{x}_2) + y_2(\\alpha_2^{new,clipped} - \\alpha_2^{old})\\textbf{k}(\\textbf{x}_2, \\textbf{x}_2) + b^{old}\n",
    "$$\n",
    "\n",
    "选择的条件：\n",
    "\n",
    "```python\n",
    "if 0 < ⍺1 < C: \n",
    "    b = b1\n",
    "else 0 < ⍺2 < C: \n",
    "    b = b2\n",
    "else:\n",
    "    b = (b1 + b2) / 2\n",
    "```\n",
    "\n",
    "误差缓存更新：\n",
    "\n",
    "$$\n",
    "E_k^{new} = E_k^{old} + y_1(\\alpha_1^{new} - \\alpha_1^{old})\\textbf{k}(\\textbf{x}_1, \\textbf{x}_k) + y_2(\\alpha_2^{new,clipped} - \\alpha_2^{old})\\textbf{k}(\\textbf{x}_2, \\textbf{x}_k) + b^{old} - b^{new}\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMO 优化\n",
    "\n",
    "权重向量更新：\n",
    "$$\n",
    "\\textbf{w}^{new} = \\textbf{w}^{old} + y_1(\\alpha_1^{new} - \\alpha_1^{old})\\textbf{x}_1 + y_2(\\alpha_2^{new,clipped} - \\alpha_2^{old})\\textbf{x}_2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMO 实现\n",
    "\n",
    "输入：样本集 $X \\in \\mathbb{R}^ {m * n}$, 即共有 m 个样本，n 个特征。 $\\textbf{y} \\in \\mathbb{R} ^{m*1}$。\n",
    "一个值得注意的地方是：我们的 SVM, SMO 推荐中的向量 $\\textbf{w}, \\textbf{b}, \\textbf{x}_i, \\textbf{y}$ 都是列向量。所在计算时需注意 X 的每一行的转置才是对应的列向量。\n",
    "\n",
    "如 $E_i$ 的计算公式推导：\n",
    "\n",
    "$$\n",
    "f(\\textbf{x}_i) = \\textbf{w}^T \\textbf{x}_i + b \\\\\n",
    "\\textbf{w} = \\sum_{i = 1}^{m} \\alpha_i y_i \\textbf{x}_i = X^T \\textbf{⍺} \\odot \\textbf{y} \\\\\n",
    "$$\n",
    "所以有：\n",
    "$$\n",
    "\\begin{align}\n",
    "f(\\textbf{x}_i) &= \\textbf{w}^T \\textbf{x}_i + b \\\\\n",
    "&= (X^T \\textbf{⍺} \\odot \\textbf{y})^T \\textbf{x}_i + b \\\\\n",
    "&= (\\textbf{⍺} \\odot \\textbf{y})^T X \\textbf{x}_i + b \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "最终 $E_i$ 的计算公式为:\n",
    "\n",
    "$$\n",
    "E_i = (\\textbf{⍺} \\odot \\textbf{y})^T X \\textbf{x}_i + b - y_i\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
