{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost\n",
    "\n",
    "[AadBoost](https://en.wikipedia.org/wiki/AdaBoost) wiki 上有很好的推导过程。\n",
    "\n",
    "1. AdaBoost 获取了 Kurt Gödel 哥德尔奖。\n",
    "2. 推导过程在计算损失函数时，可以根据二分类问题，是否分类正确的正负符号来拆分损失函数。最后求解的问题可以转换为：\n",
    "\n",
    "假设 $\\epsilon_t$ 为误差。求解下面导数为零时的解。\n",
    "\n",
    "$$\n",
    "-e^{\\alpha_t}(1 - \\epsilon_t) + e^{\\alpha_t}\\epsilon_t = 0  \\\\ \n",
    "即， \\space e^{\\alpha_t}(1 - \\epsilon_t) = e^{\\alpha_t}\\epsilon_t \\\\\n",
    "两边同时取对数，有 \\alpha_t = \\frac {1}{2} ln (\\frac {1 - \\epsilon_t}{\\epsilon_t})\n",
    "$$\n",
    "\n",
    "下面是简单的翻译。\n",
    "\n",
    "### 『休斯』效应\n",
    "\n",
    "在一些模型的应用中，特征维数的增加到某一临界点后，继续增加反而会导致分类器的性能变差。这种现象称之为 \"Hugues Effect\", 休斯效应或休斯现象。\n",
    "\n",
    "AdaBoost 是可以解决休斯效应， The curse of dimensionality 。\n",
    "\n",
    "### 训练\n",
    "\n",
    "$$\n",
    "F_T(x) = \\sum_{t=1}^{T}f_t(x)\n",
    "$$\n",
    "\n",
    "$f_t$ 是一个弱学习器，对应一个假设 $h(x_i)$。\n",
    "\n",
    "### 推导"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
