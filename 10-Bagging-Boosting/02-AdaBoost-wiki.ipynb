{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost\n",
    "\n",
    "[AdaBoost](https://en.wikipedia.org/wiki/AdaBoost) wiki 上有很好的推导过程。\n",
    "\n",
    "AdaBoost(Adaptive Boost), 自适应 Boost, 即当前迭代的分类器会根据之前的分类器做自适应的调整。\n",
    "\n",
    "AdaBoost, 对噪声样本和 Outlier 敏感，但不容易过拟合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 概览\n",
    "\n",
    "#### 训练\n",
    "\n",
    "AdaBoost 是训练 boost 分类器的一个特殊方法。一个 boost 分类器其形式为：\n",
    "\n",
    "$$\n",
    "F_T(x) = \\sum_{t=1}^{T} f_t(x)\n",
    "$$\n",
    "\n",
    "其中 $f_t$ 是弱学习器(weak learner), 返回样本的类别。\n",
    "\n",
    "每一个弱学习器(weak learner) 都生成一个假设函数 $h(x_i)$。在 t 迭代时，当前 t 迭代对应的弱分类器加上一个参数 $\\alpha_t$, 则训练误差为：\n",
    "\n",
    "$$\n",
    "E_t = \\sum_{i}E[F_{t - 1}(x_i) + \\alpha_t h(x_i)]\n",
    "$$\n",
    "\n",
    "$F_{t - 1}(x_i)$ 是目前为止已经生成的 boosted 分类器。\n",
    "\n",
    "#### 权重\n",
    "\n",
    "在训练的过程中，第一个样本都会被赋予一个权重 $w_{i, t}$, 该权重等于 $E(F_{t-1}(x_i))$。即\n",
    "\n",
    "$$\n",
    "w_{i, t} = E\\big(F_{t-1}(x_i)\\big)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 推导\n",
    "\n",
    "假设，有数据集 $\\{(x_1, y_1), \\dots, (x_N, y_N)\\}, y_i \\in \\{-1, 1\\}$, 和一组弱分类器 $\\{k_1, \\dots, k_L \\}, k_j(x_i) \\in \\{-1, 1\\}$.\n",
    "\n",
    "则在 $m - 1$ 迭代后，我们 boosted 分类器的线性组合形式为：\n",
    "\n",
    "$$\n",
    "C_{(m - 1)}(x_i) = \\alpha_1 k_1 (x_i) + \\cdots + \\alpha_{m-1} k_{m-1} (x_i)\n",
    "$$\n",
    "\n",
    "在 $m$ 次迭代时，我们得到一个更优的分类器。\n",
    "\n",
    "$$\n",
    "C_{m}(x_i) = C_{(m - 1)} + \\alpha_m k_m(x_i)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以问题在于：对于弱分类器 $k_m 怎样选择其参数 \\alpha_m $。\n",
    "\n",
    "我们使用[指数损失函数](https://en.wikipedia.org/wiki/Loss_functions_for_classification#Exponential_loss) 来定义分类器 $C_m$  的误差：\n",
    "\n",
    "$$\n",
    "E = \\sum_{i} e^{- y_i C_m(x_i)} = \\sum_{i = 1}^{N} e^{-y_iC_{m - 1}(x_i)} e^{-y_i \\alpha_m k_m(x_i)}\n",
    "$$\n",
    "\n",
    "令 $w_i^{(m)} =  e^{-y_iC_{m - 1}(x_i)}, m > 1. 当 m = 1 时， w_i^{(1)} = 1 $。则上式可以写为：\n",
    "\n",
    "$$\n",
    "E = \\sum_{i = 1}^{N} w_i^{(m)} e^{-y_i \\alpha_m k_m(x_i)}\n",
    "$$\n",
    "\n",
    "因为 $y_i k_m(x_i)$ 要么等于 $1$ 要么等于 $ -1 $, 所以上式又可以写为：\n",
    "\n",
    "\\begin{align}\n",
    "E &= \\sum_{i = 1}^{N} w_i^{(m)} e^{-y_i \\alpha_m k_m(x_i)} \n",
    "\\\\\n",
    "&= \\sum_{y_i = k_m(x_i)} w_i^{(m)} e ^{-\\alpha_i} + \\sum_{y_i \\ne k_m(x_i)} w_i^{(m)} e ^{\\alpha_i} \\\\\n",
    "&= \\sum_{i = 1}^{N} w_i^{(m)} e ^{-\\alpha_i} + \\sum_{y_i \\ne k_m(x_i)} w_i^{(m)} (e ^{\\alpha_i} - e ^{- \\alpha_i})\n",
    "\\end{align}\n",
    "\n",
    "可知上式右边最后一项才与 $k_m$ 有关。所我们提高 $k_m$ 的正确率，就可减小最后一项：$\\sum_{y_i \\ne k_m(x_i)} w_i^{(m)} $，假设 $\\alpha_m > 0$。 对损失误差函数求导其为 0 ，求出驻点就可以求得 $\\alpha_m$.\n",
    "\n",
    "即:\n",
    "\n",
    "$$\n",
    "\\frac{dE}{d\\alpha_m} = \\frac{d\\big( \\sum_{y_i = k_m(x_i)} w_i^{(m)} e ^{-\\alpha_i} + \\sum_{y_i \\ne k_m(x_i)} w_i^{(m)} e ^{\\alpha_i} \\big)}{d{\\alpha_m}}\n",
    "$$\n",
    "\n",
    "最终求得驻点：\n",
    "\n",
    "$$\n",
    "\\alpha_m = \\frac{1}{2} ln\\bigg( \\frac{\\sum_{y_i = k_m(x_i)} w_i^{(m)}}{\\sum_{y_i \\ne k_m(x_i)} w_i^{(m)}} \\bigg)\n",
    "$$\n",
    "\n",
    "求驻点的证明可以参考 wiki.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设 \n",
    "\n",
    "$$ \n",
    "\\epsilon_m = \\frac{\\sum_{y_i \\ne k_m(x_i)} w_i^{(m)}}{\\sum_{i=1}^{N} w_i^{(m)}} \n",
    "$$\n",
    "\n",
    "则：\n",
    "\n",
    "$$\n",
    "\\alpha_m = \\frac{1}{2}ln\\bigg(\\frac{1 - \\epsilon_m}{\\epsilon_m}\\bigg)\n",
    "$$\n",
    "\n",
    "我们 $k_m$ 弱分类器是已知的，求出了 $\\alpha_m$ 后就可以都完成训练了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 算法\n",
    "\n",
    "假设有：\n",
    "\n",
    "- 训练集: $\\{(x_1, y_1), \\dots, (x_n, y_n)\\}, y_i \\in \\{-1, + 1\\}$\n",
    "- 误差函数: $E(f(x), y, i) = e^{-y_i f(x_i)}$\n",
    "- 弱分类器: $h: x -> \\{-1, 1\\}$\n",
    "- 初始化权重: $w_{1, 1}, \\cdots, w_{n 1} = \\frac{1}{n}$\n",
    "\n",
    "则进行迭代: $1 \\dots T$\n",
    "\n",
    "- 生成弱分类器 $h_t(X, y, W)$\n",
    "    - 计算 $\\epsilon_t$\n",
    "    - 计算 $\\alpha_t = \\frac{1}{2} ln(\\frac{1 - \\epsilon_t}{\\epsilon_t})$\n",
    "- 添加集成分类器上\n",
    "    - $F_t(x) = F_{t - 1}(x) + \\alpha_t h_t (x)$\n",
    "- 更新权重\n",
    "    - $w_{i, t + 1} = w_{i, t} e ^{-y_i \\alpha_t h_t(x_i)}, \\forall i$\n",
    "    - 重新规一化 $w_{i, t + 1}$, 使得 $\\sum_{i}w_{i, t + 1} = 1$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 『休斯』效应\n",
    "\n",
    "在一些模型的应用中，特征维数的增加到某一临界点后，继续增加反而会导致分类器的性能变差。这种现象称之为 \"Hugues Effect\", 休斯效应或休斯现象。\n",
    "\n",
    "AdaBoost 是可以解决休斯效应， The curse of dimensionality 。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
