{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GBM- L1\n",
    "\n",
    "GBM - L2 对 outliers 很敏感。因为 Outliers 的 MSE 平方的权重会增大。如果我们不能消除 outliers 噪声的话，我们可以使用 L1, MAE 来作为优化方向。\n",
    "\n",
    "向着 MSE 优化，是通过对误差的大小(magnitude)来优化，而并没有考虑误差的方向(direction)。我们可通过 MAE 来找到误差优化的方向。\n",
    "\n",
    "[Heading in the right direction](http://explained.ai/gradient-boosting/L1-loss.html)\n",
    "\n",
    "### 使用符号向量\n",
    "\n",
    "我们可以使用符号向量 $sign(y_i - F_{m - 1}(x_i))$ 来作为方向。向量中的每个元素的取值为 {-1， 0， 1}, 这样无论 outliers 的大小，其方向都是在一个量级上的。\n",
    "\n",
    "L1 的决策树桩的叶子节点预测的值就是中位数(median) 而不是 L2 中的平均数 (mean) 了。而初始的 F0 也是取中位数而非均值了。因为中位数有最小的 L1 误差。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data():\n",
    "    return pd.read_csv('rent-l1.txt', delimiter = '\\t')\n",
    "\n",
    "def mae(vals):\n",
    "    return np.sum(np.abs(vals - np.median(vals))) / len(vals) # mae 除以元素个数，当 mae 相同时可以选择元素个数多的划分。\n",
    "\n",
    "def mse(vals):\n",
    "    return np.var(vals) # 即使用 sign vector, 也是计算 mse.\n",
    "\n",
    "class DecisionTree(object):\n",
    "    def __init__(self):\n",
    "        return\n",
    "    \n",
    "    def model(self, X, y, costErr = mae):\n",
    "        m, n = X.shape\n",
    "        min_mae = np.inf\n",
    "        feature, split_value, lhs, rhs = 0, 0, 0, 0\n",
    "        \n",
    "        for i in range(n):\n",
    "            xVals = X[:, i]\n",
    "            uniVals = list(set(xVals))\n",
    "            uniVals.sort()\n",
    "            candidates = [(uniVals[i] + uniVals[i + 1]) / 2 for i in range(len(uniVals) - 1)]\n",
    "            \n",
    "            for c in candidates:\n",
    "                mae = costErr(np.sign(y[xVals <= c])) + costErr(np.sign(y[xVals > c]))\n",
    "                print(c, y[xVals <= c], y[xVals > c], mae)\n",
    "                if mae < min_mae:\n",
    "                    min_mae, feature, split_value = mae, i, c\n",
    "                    lhs = np.median(y[xVals <= c])\n",
    "                    rhs = np.median(y[xVals > c])\n",
    "                    \n",
    "        self.lhs, self.rhs = lhs, rhs\n",
    "        self.split_feature, self.split_value = feature, split_value\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        m = X.shape[0]\n",
    "        y = np.ones(m) * self.lhs\n",
    "        y[X[:, self.split_feature] > self.split_value] = self.rhs\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoostL1(object):\n",
    "    def __init__(self):\n",
    "        self.trees = []\n",
    "        self.f0 = 0\n",
    "        return\n",
    "    \n",
    "    def model(self, X, y, alpha = 1, iters = 50):\n",
    "        self.iters = iters\n",
    "        self.alpha = alpha\n",
    "        m = X.shape[0]\n",
    "        self.f0 = np.median(y)\n",
    "        yHat = np.ones(m) * self.f0\n",
    "        \n",
    "        for i in range(iters):\n",
    "            residuals = y - yHat\n",
    "            print('residuals:', residuals)\n",
    "            tree = DecisionTree().model(X, residuals, mse)  # 用 MSE, MAE  效果一样，但正确的还是用 MAE\n",
    "            yHat = yHat + alpha * tree.predict(X)\n",
    "            self.trees.append(tree)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        m = X.shape[0]\n",
    "        y = np.ones(m) * self.f0\n",
    "        for i in range(self.iters):\n",
    "            y = y + self.alpha * self.trees[i].predict(X)\n",
    "        return y\n",
    "    \n",
    "    def printTrees(self):\n",
    "        print('F0:', self.f0)\n",
    "        for i in range(self.iters):\n",
    "            tree = self.trees[i]\n",
    "            print('Tree ', i)\n",
    "            print('\\t split feature:', tree.split_feature)\n",
    "            print('\\t split feature:', tree.split_value)\n",
    "            print('\\t left median', tree.lhs)\n",
    "            print('\\t right median', tree.rhs)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "residuals: [-120.  -80.    0.  170.  720.]\n",
      "775.0 [-120.] [-80.   0. 170. 720.] 0.6875\n",
      "825.0 [-120.  -80.] [  0. 170. 720.] 0.22222222222222224\n",
      "875.0 [-120.  -80.    0.] [170. 720.] 0.22222222222222224\n",
      "925.0 [-120.  -80.    0.  170.] [720.] 0.6875\n",
      "residuals: [ -20.   20. -170.    0.  550.]\n",
      "775.0 [-20.] [  20. -170.    0.  550.] 0.6875\n",
      "825.0 [-20.  20.] [-170.    0.  550.] 1.6666666666666665\n",
      "875.0 [ -20.   20. -170.] [  0. 550.] 1.1388888888888888\n",
      "925.0 [ -20.   20. -170.    0.] [550.] 0.6875\n",
      "residuals: [   0.   10. -180.  -10.  540.]\n",
      "775.0 [0.] [  10. -180.  -10.  540.] 1.0\n",
      "825.0 [ 0. 10.] [-180.  -10.  540.] 1.1388888888888888\n",
      "875.0 [   0.   10. -180.] [-10. 540.] 1.6666666666666665\n",
      "925.0 [   0.   10. -180.  -10.] [540.] 0.6875\n",
      "F0: 1280.0\n",
      "Tree  0\n",
      "\t split feature: 0\n",
      "\t split feature: 825.0\n",
      "\t left median -100.0\n",
      "\t right median 170.0\n",
      "Tree  1\n",
      "\t split feature: 0\n",
      "\t split feature: 775.0\n",
      "\t left median -20.0\n",
      "\t right median 10.0\n",
      "Tree  2\n",
      "\t split feature: 0\n",
      "\t split feature: 925.0\n",
      "\t left median -5.0\n",
      "\t right median 540.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1155., 1185., 1455., 1455., 2000.])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = data()\n",
    "X = df.values[:, :-1]\n",
    "y = df.values[:, -1]\n",
    "bstL1 = BoostL1().model(X, y, 1, 3)\n",
    "bstL1.printTrees()\n",
    "bstL1.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**刚开始是用 MAE 来作为基学习器回归树的划分方式，后来发现换成 MSE 也是一样的。**其实就是用 MAE 来计算的，因为在 [GBM algorithm to minimize L1 loss](http://explained.ai/gradient-boosting/L1-loss.html)解释中，作者说 scikit-learn 的实现 MAE 有点耗时就用 MSE 来代替了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "虽然 L1 用 sign 向量来作为训练目标，但是最终预测的结果还是残差而不是符号，这点要注意。L1-GBM, L2-GBM 预测的都是残差，前者预测的是残差的中位数，后者预测的是残差的平均数。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
